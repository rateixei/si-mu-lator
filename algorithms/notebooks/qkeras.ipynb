{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901d3553-d58b-419c-912d-12bd53159bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f9a2572-f655-4923-b2e3-f74b5a61c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52266077-045a-4434-a8a1-f8a8ba769f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f65aefc0-0b3a-469f-856c-75c7c194b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33376999-2512-4ccd-af23-6803f13055b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d527dccf-0778-44a8-a21c-3a7e0b23eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24c86e0a-2df9-4d04-b21d-d4ed8ba8f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e13fb9b-8126-4ff6-b193-899925edb1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8f882ea-d215-4e84-9e4e-c2e55ca6a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff12e85-891d-44ea-937f-8170c06ba070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datatools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a8fb08d-ffc9-4f7e-a4b8-48fe122ef1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462d0df7-4e94-4795-87c9-f2e42ed25d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d0b3fe-16d6-432b-be57-2c570e1b1ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    }
   ],
   "source": [
    "SIM=\"/gpfs/slac/atlas/fs1/d/rafaeltl/public/Muon/simulation/\"\n",
    "DATA_LOC=f\"{SIM}/stgc/atlas_nsw_pad_z0_stgc20Max1_bkgr_1_CovAngle_TRAIN/*.h5\"\n",
    "\n",
    "files=glob(DATA_LOC)\n",
    "\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4d2c9b6-f589-4022-893f-3d63fc47d6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ Reading data... ~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135/135 [01:15<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ Calculating occupancy information... ~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387000/387000 [00:12<00:00, 30922.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!\n",
      "I read 387000 events, of which 192000 have muon and 195000 do not\n",
      "!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data, dmat, Y, Y_mu, Y_hit, sig_keys = datatools.make_data_matrix(files, max_files=500, sort_by='z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd3a077-22ff-44b0-a5b8-e18d5415ffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trainingvariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce760987-52d4-491e-9723-1fd4e13b81ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~ Preparing padded matrix ~~\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 387000/387000 [00:26<00:00, 14365.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output data matrix shape: (387000, 20, 19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_pad = datatools.training_prep(dmat, sig_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80c27e66-c9c1-487b-abf2-728376447a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "linearized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "634f93a2-4641-4deb-9e25-e5632bd77ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prep = X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f603149-e986-4185-b3bb-26aa93a8f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_of_interest = np.zeros(X_prep.shape[2], dtype=bool)\n",
    "training_vars = trainingvariables.tvars\n",
    "for tv in training_vars:\n",
    "    vars_of_interest[sig_keys.index(tv)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6279e65-dd9f-47b0-bf84-2922554c42fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_prep[:,:,vars_of_interest]\n",
    "X_keys = np.array(sig_keys)[vars_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5960faa-4447-4bc8-86a9-595bd2e234ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loc = '../models/'\n",
    "model_name = \"MyTCN_10,3,1:10,3,1_30_CBNormTrue_DBNormFalse_ll1_ptype2_penXTrue_penATrue_bkgPenTrue_regBiasTrue_AvgPool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0478dec7-5560-40ca-a201-2ead7e25b8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = keras.models.load_model(model_loc+model_name,compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a626e6a3-2c2a-42e9-9666-04c0cd65ecb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_preds = keras_model.predict(X, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aaa87ff1-5689-45ce-ade9-1333c6026f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_yhat = sigmoid(keras_preds[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f325898a-a87b-4f79-9d02-fa4110555b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_fact = max(data['ev_mu_x'])\n",
    "mult_facta = max(data['ev_mu_theta'])\n",
    "\n",
    "keras_x_reg = keras_preds[:,1]*mult_fact\n",
    "keras_a_reg = keras_preds[:,2]*mult_facta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9952e053-145b-4afe-9e58-1c395969108a",
   "metadata": {},
   "source": [
    "## Transform model to qkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e4aa003-1cfc-46bd-9528-ff6b46aaf3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c896b5c0-e09b-41fe-acd8-6f3c0b5eeee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 20, 7)]           0         \n",
      "_________________________________________________________________\n",
      "C1D_0 (Conv1D)               (None, 18, 10)            220       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 18, 10)            40        \n",
      "_________________________________________________________________\n",
      "C1D_1 (Conv1D)               (None, 16, 10)            310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 10)            40        \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "F_dense_0 (Dense)            (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 1,033\n",
      "Trainable params: 993\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7375f49-b2d2-416c-ab86-2e33b94f5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dict = {\n",
    "    \"QActivation\": {\n",
    "        \"relu\": \"quantized_relu(4,0)\"\n",
    "    },\n",
    "    \"QConv1D\": {\n",
    "        \"kernel_quantizer\": \"quantized_bits(4,0,1)\",\n",
    "        \"bias_quantizer\": \"quantized_bits(4,0,1)\"\n",
    "    },\n",
    "    \"QDense\": {\n",
    "        \"kernel_quantizer\": \"quantized_bits(3,0,1)\",\n",
    "        \"bias_quantizer\": \"quantized_bits(3,0,1)\"\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b23095fd-b206-4408-9872-e4ffbeab301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model = qkeras.utils.model_quantize(keras_model, q_dict, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "550cf0bd-62fc-4c77-8da3-5994c32bd8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 20, 7)]           0         \n",
      "_________________________________________________________________\n",
      "C1D_0 (QConv1D)              (None, 18, 10)            220       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 18, 10)            40        \n",
      "_________________________________________________________________\n",
      "C1D_1 (QConv1D)              (None, 16, 10)            310       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 16, 10)            40        \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "F_dense_0 (QDense)           (None, 30)                330       \n",
      "_________________________________________________________________\n",
      "output (QDense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 1,033\n",
      "Trainable params: 993\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "q_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f89e0164-2eb5-49f1-b166-56928d9f7ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /rapids/src/qkeras/qkeras/estimate.py:345: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /rapids/src/qkeras/qkeras/estimate.py:345: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of operations in model:\n",
      "    C1D_0                         : 3780  (smult_4_8)\n",
      "    C1D_1                         : 4800  (smult_4_4)\n",
      "    F_dense_0                     : 300   (smult_3_4)\n",
      "    output                        : 90    (smult_3_4)\n",
      "\n",
      "Number of operation types in model:\n",
      "    smult_3_4                     : 390\n",
      "    smult_4_4                     : 4800\n",
      "    smult_4_8                     : 3780\n",
      "\n",
      "Weight profiling:\n",
      "    C1D_0_weights                  : 210   (4-bit unit)\n",
      "    C1D_0_bias                     : 10    (4-bit unit)\n",
      "    C1D_1_weights                  : 300   (4-bit unit)\n",
      "    C1D_1_bias                     : 10    (4-bit unit)\n",
      "    F_dense_0_weights              : 300   (3-bit unit)\n",
      "    F_dense_0_bias                 : 30    (3-bit unit)\n",
      "    output_weights                 : 90    (3-bit unit)\n",
      "    output_bias                    : 3     (3-bit unit)\n",
      "    ----------------------------------------\n",
      "    Total Bits                     :  3389\n",
      "\n",
      "Weight sparsity:\n",
      "... quantizing model\n",
      "  batch_normalization has not been quantized\n",
      "  batch_normalization_1 has not been quantized\n",
      "    C1D_0                          : 0.1500\n",
      "    C1D_1                          : 0.1484\n",
      "    F_dense_0                      : 0.2970\n",
      "    output                         : 0.2043\n",
      "    ----------------------------------------\n",
      "    Total Sparsity                 : 0.2057\n"
     ]
    }
   ],
   "source": [
    "qkeras.estimate.print_qstats(q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c001518c-cbe4-4bd4-aa68-36d2a81423c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_preds = q_model.predict(X, batch_size=1024)\n",
    "\n",
    "q_yhat = sigmoid(q_preds[:,0])\n",
    "\n",
    "q_x_reg = q_preds[:,1]*mult_fact\n",
    "q_a_reg = q_preds[:,2]*mult_facta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb98e15f-f030-4544-8cda-128e7cd57822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63640f3f-2fc9-4a70-a283-b0a3273e1226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f7be757-59a1-4664-b38a-1619fbc4b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure()\n",
    "plt.hist( (q_yhat - keras_yhat)/keras_yhat, histtype='step', range=(-5000,5000), bins=50 )\n",
    "plt.hist( (q_x_reg - keras_x_reg)/keras_x_reg, histtype='step', range=(-5000,5000), bins=50 )\n",
    "plt.hist( (q_a_reg - keras_a_reg)/keras_a_reg, histtype='step', range=(-5000,5000), bins=50 )\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c19600cd-7991-48c9-839d-8c0a2a76b074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlmodels as mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0d11d247-c77b-40af-a5eb-d16d8c432b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "combloss = mlm.class_and_regr_loss(1,\n",
    "                                   do_angle=1, \n",
    "                                   pen_type=2, pen_x=True, pen_a=True, bkg_pen_x=True, bkg_pen_a=True, linearized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdb299f8-a5d3-4b2a-9224-a96e00067731",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "42570ee9-4db4-464c-b22f-04447ac9fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.compile(loss=combloss, optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71d5a3ba-bcb6-4cd0-9188-5f8d6cfd6eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ev_mu_x = (data['ev_mu_x'])/mult_fact\n",
    "data_ev_mu_a = (data['ev_mu_theta'])/mult_facta\n",
    "\n",
    "X_train, Y_clas_train, Y_xreg_train, Y_areg_train = shuffle(X, Y_mu, data_ev_mu_x, data_ev_mu_a)\n",
    "\n",
    "Y_train = np.zeros( (Y_clas_train.shape[0], 3 ) ) \n",
    "Y_train[:,0] = Y_clas_train\n",
    "Y_train[:,1] = Y_xreg_train\n",
    "Y_train[:,2] = Y_areg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34437082-28b3-45e9-ac72-6f5087faa0ee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "18/18 - 5s - loss: 2.6240 - val_loss: 2.0439\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.04391, saving model to qkeras_weights.h5\n",
      "Epoch 2/3000\n",
      "18/18 - 1s - loss: 1.9522 - val_loss: 2.1927\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 2.04391\n",
      "Epoch 3/3000\n",
      "18/18 - 1s - loss: 1.7506 - val_loss: 2.0614\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.04391\n",
      "Epoch 4/3000\n",
      "18/18 - 1s - loss: 1.6790 - val_loss: 2.0140\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.04391 to 2.01398, saving model to qkeras_weights.h5\n",
      "Epoch 5/3000\n",
      "18/18 - 1s - loss: 1.5853 - val_loss: 1.8316\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.01398 to 1.83156, saving model to qkeras_weights.h5\n",
      "Epoch 6/3000\n",
      "18/18 - 1s - loss: 1.5206 - val_loss: 1.9702\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.83156\n",
      "Epoch 7/3000\n",
      "18/18 - 1s - loss: 1.5638 - val_loss: 1.6565\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.83156 to 1.65651, saving model to qkeras_weights.h5\n",
      "Epoch 8/3000\n",
      "18/18 - 1s - loss: 1.4583 - val_loss: 1.7886\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.65651\n",
      "Epoch 9/3000\n",
      "18/18 - 1s - loss: 1.3875 - val_loss: 1.5859\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.65651 to 1.58588, saving model to qkeras_weights.h5\n",
      "Epoch 10/3000\n",
      "18/18 - 1s - loss: 1.3386 - val_loss: 1.4325\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.58588 to 1.43255, saving model to qkeras_weights.h5\n",
      "Epoch 11/3000\n",
      "18/18 - 1s - loss: 1.2688 - val_loss: 1.5229\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.43255\n",
      "Epoch 12/3000\n",
      "18/18 - 1s - loss: 1.3220 - val_loss: 1.3711\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.43255 to 1.37106, saving model to qkeras_weights.h5\n",
      "Epoch 13/3000\n",
      "18/18 - 1s - loss: 1.2926 - val_loss: 1.2733\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.37106 to 1.27326, saving model to qkeras_weights.h5\n",
      "Epoch 14/3000\n",
      "18/18 - 1s - loss: 1.2554 - val_loss: 1.3155\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.27326\n",
      "Epoch 15/3000\n",
      "18/18 - 1s - loss: 1.2211 - val_loss: 1.3803\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.27326\n",
      "Epoch 16/3000\n",
      "18/18 - 1s - loss: 1.2311 - val_loss: 1.3077\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.27326\n",
      "Epoch 17/3000\n",
      "18/18 - 1s - loss: 1.5193 - val_loss: 1.2541\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.27326 to 1.25412, saving model to qkeras_weights.h5\n",
      "Epoch 18/3000\n",
      "18/18 - 1s - loss: 1.6998 - val_loss: 1.4700\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.25412\n",
      "Epoch 19/3000\n",
      "18/18 - 1s - loss: 1.5522 - val_loss: 1.1667\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.25412 to 1.16673, saving model to qkeras_weights.h5\n",
      "Epoch 20/3000\n",
      "18/18 - 1s - loss: 1.2767 - val_loss: 1.2529\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.16673\n",
      "Epoch 21/3000\n",
      "18/18 - 1s - loss: 1.1822 - val_loss: 1.1459\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.16673 to 1.14595, saving model to qkeras_weights.h5\n",
      "Epoch 22/3000\n",
      "18/18 - 1s - loss: 1.1329 - val_loss: 1.1191\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.14595 to 1.11906, saving model to qkeras_weights.h5\n",
      "Epoch 23/3000\n",
      "18/18 - 1s - loss: 1.1183 - val_loss: 1.1019\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.11906 to 1.10190, saving model to qkeras_weights.h5\n",
      "Epoch 24/3000\n",
      "18/18 - 1s - loss: 1.1729 - val_loss: 1.1120\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.10190\n",
      "Epoch 25/3000\n",
      "18/18 - 1s - loss: 1.1894 - val_loss: 1.3651\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.10190\n",
      "Epoch 26/3000\n",
      "18/18 - 1s - loss: 1.1544 - val_loss: 1.2164\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.10190\n",
      "Epoch 27/3000\n",
      "18/18 - 1s - loss: 1.2149 - val_loss: 1.1171\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.10190\n",
      "Epoch 28/3000\n",
      "18/18 - 1s - loss: 1.1087 - val_loss: 1.1026\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.10190\n",
      "Epoch 29/3000\n",
      "18/18 - 1s - loss: 1.1788 - val_loss: 1.1942\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.10190\n",
      "Epoch 30/3000\n",
      "18/18 - 1s - loss: 1.1750 - val_loss: 1.2017\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.10190\n",
      "Epoch 31/3000\n",
      "18/18 - 1s - loss: 1.1122 - val_loss: 1.0426\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.10190 to 1.04263, saving model to qkeras_weights.h5\n",
      "Epoch 32/3000\n",
      "18/18 - 1s - loss: 1.0512 - val_loss: 1.0257\n",
      "\n",
      "Epoch 00032: val_loss improved from 1.04263 to 1.02567, saving model to qkeras_weights.h5\n",
      "Epoch 33/3000\n",
      "18/18 - 1s - loss: 1.0718 - val_loss: 1.1528\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.02567\n",
      "Epoch 34/3000\n",
      "18/18 - 1s - loss: 1.1899 - val_loss: 1.1154\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.02567\n",
      "Epoch 35/3000\n",
      "18/18 - 1s - loss: 1.2545 - val_loss: 1.0728\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.02567\n",
      "Epoch 36/3000\n",
      "18/18 - 1s - loss: 1.0954 - val_loss: 1.1735\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.02567\n",
      "Epoch 37/3000\n",
      "18/18 - 1s - loss: 1.0970 - val_loss: 1.1219\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.02567\n",
      "Epoch 38/3000\n",
      "18/18 - 1s - loss: 1.1572 - val_loss: 1.2561\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.02567\n",
      "Epoch 39/3000\n",
      "18/18 - 1s - loss: 1.1698 - val_loss: 1.0805\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.02567\n",
      "Epoch 40/3000\n",
      "18/18 - 1s - loss: 1.0872 - val_loss: 1.3755\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.02567\n",
      "Epoch 41/3000\n",
      "18/18 - 1s - loss: 1.1930 - val_loss: 1.2009\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.02567\n",
      "Epoch 42/3000\n",
      "18/18 - 1s - loss: 1.1652 - val_loss: 1.0618\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.02567\n",
      "Epoch 43/3000\n",
      "18/18 - 1s - loss: 1.0845 - val_loss: 1.2863\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.02567\n",
      "Epoch 44/3000\n",
      "18/18 - 1s - loss: 1.1222 - val_loss: 1.1755\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.02567\n",
      "Epoch 45/3000\n",
      "18/18 - 1s - loss: 1.0721 - val_loss: 1.0330\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.02567\n",
      "Epoch 46/3000\n",
      "18/18 - 1s - loss: 1.0644 - val_loss: 1.0633\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.02567\n",
      "Epoch 47/3000\n",
      "18/18 - 1s - loss: 1.0814 - val_loss: 1.0437\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.02567\n",
      "Epoch 48/3000\n",
      "18/18 - 1s - loss: 1.0390 - val_loss: 1.2425\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.02567\n",
      "Epoch 49/3000\n",
      "18/18 - 1s - loss: 1.2148 - val_loss: 1.1074\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.02567\n",
      "Epoch 50/3000\n",
      "18/18 - 1s - loss: 1.0577 - val_loss: 1.1774\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.02567\n",
      "Epoch 51/3000\n",
      "18/18 - 1s - loss: 1.0783 - val_loss: 1.0505\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.02567\n",
      "Epoch 52/3000\n",
      "18/18 - 1s - loss: 1.1219 - val_loss: 1.0854\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.02567\n",
      "Epoch 53/3000\n",
      "18/18 - 1s - loss: 1.0909 - val_loss: 1.1341\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.02567\n",
      "Epoch 54/3000\n",
      "18/18 - 1s - loss: 1.1114 - val_loss: 1.4052\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.02567\n",
      "Epoch 55/3000\n",
      "18/18 - 1s - loss: 1.1030 - val_loss: 1.0734\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.02567\n",
      "Epoch 56/3000\n",
      "18/18 - 1s - loss: 1.0668 - val_loss: 1.0531\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.02567\n",
      "Epoch 57/3000\n",
      "18/18 - 1s - loss: 1.2806 - val_loss: 1.3901\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.02567\n",
      "Epoch 58/3000\n",
      "18/18 - 1s - loss: 1.3231 - val_loss: 1.0949\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.02567\n",
      "Epoch 59/3000\n",
      "18/18 - 1s - loss: 1.1549 - val_loss: 1.1158\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.02567\n",
      "Epoch 60/3000\n",
      "18/18 - 1s - loss: 1.1805 - val_loss: 1.2408\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.02567\n",
      "Epoch 61/3000\n",
      "18/18 - 1s - loss: 1.1182 - val_loss: 1.0875\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.02567\n",
      "Epoch 62/3000\n",
      "18/18 - 1s - loss: 1.3076 - val_loss: 1.2356\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.02567\n",
      "Epoch 63/3000\n",
      "18/18 - 1s - loss: 1.3916 - val_loss: 1.9445\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.02567\n",
      "Epoch 64/3000\n",
      "18/18 - 1s - loss: 1.1738 - val_loss: 1.0627\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.02567\n",
      "Epoch 65/3000\n",
      "18/18 - 1s - loss: 1.1101 - val_loss: 1.0985\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.02567\n",
      "Epoch 66/3000\n",
      "18/18 - 1s - loss: 1.3094 - val_loss: 1.0903\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.02567\n",
      "Epoch 67/3000\n",
      "18/18 - 1s - loss: 1.1230 - val_loss: 1.0929\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.02567\n",
      "Epoch 68/3000\n",
      "18/18 - 1s - loss: 1.0761 - val_loss: 1.1240\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.02567\n",
      "Epoch 69/3000\n",
      "18/18 - 1s - loss: 1.0985 - val_loss: 1.0883\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.02567\n",
      "Epoch 70/3000\n",
      "18/18 - 1s - loss: 1.0859 - val_loss: 1.1314\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.02567\n",
      "Epoch 71/3000\n",
      "18/18 - 1s - loss: 1.1825 - val_loss: 1.3855\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.02567\n",
      "Epoch 72/3000\n",
      "18/18 - 1s - loss: 1.1230 - val_loss: 1.0671\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.02567\n",
      "Epoch 73/3000\n",
      "18/18 - 1s - loss: 1.0427 - val_loss: 1.0410\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.02567\n",
      "Epoch 74/3000\n",
      "18/18 - 1s - loss: 1.1457 - val_loss: 1.6353\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.02567\n",
      "Epoch 75/3000\n",
      "18/18 - 1s - loss: 1.1084 - val_loss: 1.2499\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.02567\n",
      "Epoch 76/3000\n",
      "18/18 - 1s - loss: 1.0897 - val_loss: 1.1970\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.02567\n",
      "Epoch 77/3000\n",
      "18/18 - 1s - loss: 1.0551 - val_loss: 1.0437\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.02567\n",
      "Epoch 78/3000\n",
      "18/18 - 1s - loss: 1.0970 - val_loss: 1.4034\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.02567\n",
      "Epoch 79/3000\n",
      "18/18 - 1s - loss: 1.2264 - val_loss: 1.1183\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.02567\n",
      "Epoch 80/3000\n",
      "18/18 - 1s - loss: 1.1298 - val_loss: 1.2512\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.02567\n",
      "Epoch 81/3000\n",
      "18/18 - 1s - loss: 1.1869 - val_loss: 1.3240\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.02567\n",
      "Epoch 82/3000\n",
      "18/18 - 1s - loss: 1.2092 - val_loss: 1.0939\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.02567\n",
      "Epoch 83/3000\n",
      "18/18 - 1s - loss: 1.0918 - val_loss: 1.0645\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.02567\n",
      "Epoch 84/3000\n",
      "18/18 - 1s - loss: 1.0574 - val_loss: 1.2514\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.02567\n",
      "Epoch 85/3000\n",
      "18/18 - 1s - loss: 1.1043 - val_loss: 1.3964\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.02567\n",
      "Epoch 86/3000\n",
      "18/18 - 1s - loss: 1.1960 - val_loss: 1.0691\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.02567\n",
      "Epoch 87/3000\n",
      "18/18 - 1s - loss: 1.1228 - val_loss: 1.2637\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.02567\n",
      "Epoch 88/3000\n",
      "18/18 - 1s - loss: 1.1907 - val_loss: 1.6730\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.02567\n",
      "Epoch 89/3000\n",
      "18/18 - 1s - loss: 1.2168 - val_loss: 1.0923\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.02567\n",
      "Epoch 90/3000\n",
      "18/18 - 1s - loss: 1.0591 - val_loss: 1.2389\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.02567\n",
      "Epoch 91/3000\n",
      "18/18 - 1s - loss: 1.0593 - val_loss: 1.2215\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.02567\n",
      "Epoch 92/3000\n",
      "18/18 - 1s - loss: 1.0810 - val_loss: 1.1785\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.02567\n",
      "Epoch 93/3000\n",
      "18/18 - 1s - loss: 1.0837 - val_loss: 1.0497\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.02567\n",
      "Epoch 94/3000\n",
      "18/18 - 1s - loss: 1.0708 - val_loss: 1.4357\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.02567\n",
      "Epoch 95/3000\n",
      "18/18 - 1s - loss: 1.1210 - val_loss: 1.1385\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.02567\n",
      "Epoch 96/3000\n",
      "18/18 - 1s - loss: 1.0814 - val_loss: 1.0457\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.02567\n",
      "Epoch 97/3000\n",
      "18/18 - 1s - loss: 1.0446 - val_loss: 1.0542\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.02567\n",
      "Epoch 98/3000\n",
      "18/18 - 1s - loss: 1.0253 - val_loss: 1.0481\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.02567\n",
      "Epoch 99/3000\n",
      "18/18 - 1s - loss: 1.0433 - val_loss: 1.3825\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.02567\n",
      "Epoch 100/3000\n",
      "18/18 - 1s - loss: 1.2117 - val_loss: 1.0894\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.02567\n",
      "Epoch 101/3000\n",
      "18/18 - 1s - loss: 1.1232 - val_loss: 1.0364\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.02567\n",
      "Epoch 102/3000\n",
      "18/18 - 1s - loss: 1.0542 - val_loss: 1.8061\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.02567\n",
      "Epoch 103/3000\n",
      "18/18 - 1s - loss: 1.1676 - val_loss: 1.0546\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.02567\n",
      "Epoch 104/3000\n",
      "18/18 - 1s - loss: 1.1160 - val_loss: 1.1585\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.02567\n",
      "Epoch 105/3000\n",
      "18/18 - 1s - loss: 1.2492 - val_loss: 1.1670\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.02567\n",
      "Epoch 106/3000\n",
      "18/18 - 1s - loss: 1.1050 - val_loss: 1.1149\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.02567\n",
      "Epoch 107/3000\n",
      "18/18 - 1s - loss: 1.2070 - val_loss: 1.1308\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.02567\n",
      "Epoch 108/3000\n",
      "18/18 - 1s - loss: 1.0959 - val_loss: 1.0775\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.02567\n",
      "Epoch 109/3000\n",
      "18/18 - 1s - loss: 1.1386 - val_loss: 1.1981\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.02567\n",
      "Epoch 110/3000\n",
      "18/18 - 1s - loss: 1.2109 - val_loss: 1.0700\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.02567\n",
      "Epoch 111/3000\n",
      "18/18 - 1s - loss: 1.0750 - val_loss: 1.3981\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.02567\n",
      "Epoch 112/3000\n",
      "18/18 - 1s - loss: 1.0930 - val_loss: 1.2335\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.02567\n",
      "Epoch 113/3000\n",
      "18/18 - 1s - loss: 1.1071 - val_loss: 1.1159\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.02567\n",
      "Epoch 114/3000\n",
      "18/18 - 1s - loss: 1.1075 - val_loss: 1.0959\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.02567\n",
      "Epoch 115/3000\n",
      "18/18 - 1s - loss: 1.0522 - val_loss: 1.1277\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.02567\n",
      "Epoch 116/3000\n",
      "18/18 - 1s - loss: 1.0593 - val_loss: 1.0836\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.02567\n",
      "Epoch 117/3000\n",
      "18/18 - 1s - loss: 1.2019 - val_loss: 1.1295\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.02567\n",
      "Epoch 118/3000\n",
      "18/18 - 1s - loss: 1.0839 - val_loss: 1.2173\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.02567\n",
      "Epoch 119/3000\n",
      "18/18 - 1s - loss: 1.1553 - val_loss: 1.1049\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.02567\n",
      "Epoch 120/3000\n",
      "18/18 - 1s - loss: 1.1576 - val_loss: 1.1594\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.02567\n",
      "Epoch 121/3000\n",
      "18/18 - 1s - loss: 1.1309 - val_loss: 1.0678\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.02567\n",
      "Epoch 122/3000\n",
      "18/18 - 1s - loss: 1.0833 - val_loss: 1.0627\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.02567\n",
      "Epoch 123/3000\n",
      "18/18 - 1s - loss: 1.1348 - val_loss: 1.0855\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.02567\n",
      "Epoch 124/3000\n",
      "18/18 - 1s - loss: 1.2153 - val_loss: 1.3228\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.02567\n",
      "Epoch 125/3000\n",
      "18/18 - 1s - loss: 1.3240 - val_loss: 1.3812\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.02567\n",
      "Epoch 126/3000\n",
      "18/18 - 1s - loss: 1.1489 - val_loss: 1.4097\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.02567\n",
      "Epoch 127/3000\n",
      "18/18 - 1s - loss: 1.1390 - val_loss: 1.2960\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.02567\n",
      "Epoch 128/3000\n",
      "18/18 - 1s - loss: 1.1075 - val_loss: 1.1689\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.02567\n",
      "Epoch 129/3000\n",
      "18/18 - 1s - loss: 1.1217 - val_loss: 1.2440\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.02567\n",
      "Epoch 130/3000\n",
      "18/18 - 1s - loss: 1.1064 - val_loss: 1.1149\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.02567\n",
      "Epoch 131/3000\n",
      "18/18 - 1s - loss: 1.0991 - val_loss: 1.2876\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.02567\n",
      "Epoch 132/3000\n",
      "18/18 - 1s - loss: 1.1517 - val_loss: 1.0828\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.02567\n",
      "Epoch 133/3000\n",
      "18/18 - 1s - loss: 1.1338 - val_loss: 1.0895\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.02567\n",
      "Epoch 134/3000\n",
      "18/18 - 1s - loss: 1.0891 - val_loss: 1.0687\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.02567\n",
      "Epoch 135/3000\n",
      "18/18 - 1s - loss: 1.0840 - val_loss: 1.0622\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.02567\n",
      "Epoch 136/3000\n",
      "18/18 - 1s - loss: 1.0881 - val_loss: 1.0605\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.02567\n",
      "Epoch 137/3000\n",
      "18/18 - 1s - loss: 1.0562 - val_loss: 1.0500\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.02567\n",
      "Epoch 138/3000\n",
      "18/18 - 1s - loss: 1.0813 - val_loss: 1.2777\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.02567\n",
      "Epoch 139/3000\n",
      "18/18 - 1s - loss: 1.1785 - val_loss: 1.0703\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.02567\n",
      "Epoch 140/3000\n",
      "18/18 - 1s - loss: 1.0685 - val_loss: 1.0680\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.02567\n",
      "Epoch 141/3000\n",
      "18/18 - 1s - loss: 1.0977 - val_loss: 1.2134\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.02567\n",
      "Epoch 142/3000\n",
      "18/18 - 1s - loss: 1.0894 - val_loss: 1.0698\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.02567\n",
      "Epoch 143/3000\n",
      "18/18 - 1s - loss: 1.0673 - val_loss: 1.0723\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.02567\n",
      "Epoch 144/3000\n",
      "18/18 - 1s - loss: 1.0935 - val_loss: 1.0625\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.02567\n",
      "Epoch 145/3000\n",
      "18/18 - 1s - loss: 1.0580 - val_loss: 1.0627\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.02567\n",
      "Epoch 146/3000\n",
      "18/18 - 1s - loss: 1.0530 - val_loss: 1.0594\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.02567\n",
      "Epoch 147/3000\n",
      "18/18 - 1s - loss: 1.1122 - val_loss: 1.0595\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.02567\n",
      "Epoch 148/3000\n",
      "18/18 - 1s - loss: 1.1091 - val_loss: 1.0703\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.02567\n",
      "Epoch 149/3000\n",
      "18/18 - 1s - loss: 1.0563 - val_loss: 1.0578\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.02567\n",
      "Epoch 150/3000\n",
      "18/18 - 1s - loss: 1.0997 - val_loss: 1.0660\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.02567\n",
      "Epoch 151/3000\n",
      "18/18 - 1s - loss: 1.0784 - val_loss: 1.4727\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.02567\n",
      "Epoch 152/3000\n",
      "18/18 - 1s - loss: 1.1841 - val_loss: 1.1269\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.02567\n",
      "Epoch 153/3000\n",
      "18/18 - 1s - loss: 1.2094 - val_loss: 1.1728\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.02567\n",
      "Epoch 154/3000\n",
      "18/18 - 1s - loss: 1.1259 - val_loss: 1.1653\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.02567\n",
      "Epoch 155/3000\n",
      "18/18 - 1s - loss: 1.1141 - val_loss: 1.1302\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.02567\n",
      "Epoch 156/3000\n",
      "18/18 - 1s - loss: 1.0710 - val_loss: 1.3721\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 1.02567\n",
      "Epoch 157/3000\n",
      "18/18 - 1s - loss: 1.0833 - val_loss: 1.0688\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.02567\n",
      "Epoch 158/3000\n",
      "18/18 - 1s - loss: 1.2189 - val_loss: 1.1195\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.02567\n",
      "Epoch 159/3000\n",
      "18/18 - 1s - loss: 1.1104 - val_loss: 1.1093\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.02567\n",
      "Epoch 160/3000\n",
      "18/18 - 1s - loss: 1.1710 - val_loss: 1.2466\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.02567\n",
      "Epoch 161/3000\n",
      "18/18 - 1s - loss: 1.1404 - val_loss: 1.0853\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.02567\n",
      "Epoch 162/3000\n",
      "18/18 - 1s - loss: 1.0695 - val_loss: 1.0792\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.02567\n",
      "Epoch 163/3000\n",
      "18/18 - 1s - loss: 1.0759 - val_loss: 1.0646\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.02567\n",
      "Epoch 164/3000\n",
      "18/18 - 1s - loss: 1.1046 - val_loss: 1.0902\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.02567\n",
      "Epoch 165/3000\n",
      "18/18 - 1s - loss: 1.2682 - val_loss: 1.1179\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.02567\n",
      "Epoch 166/3000\n",
      "18/18 - 1s - loss: 1.0759 - val_loss: 1.0731\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.02567\n",
      "Epoch 167/3000\n",
      "18/18 - 1s - loss: 1.0738 - val_loss: 1.0848\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.02567\n",
      "Epoch 168/3000\n",
      "18/18 - 1s - loss: 1.0736 - val_loss: 1.0767\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.02567\n",
      "Epoch 169/3000\n",
      "18/18 - 1s - loss: 1.0766 - val_loss: 1.0759\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.02567\n",
      "Epoch 170/3000\n",
      "18/18 - 1s - loss: 1.0703 - val_loss: 1.2012\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.02567\n",
      "Epoch 171/3000\n",
      "18/18 - 1s - loss: 1.0709 - val_loss: 1.1392\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.02567\n",
      "Epoch 172/3000\n",
      "18/18 - 1s - loss: 1.0698 - val_loss: 1.1414\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.02567\n",
      "Epoch 173/3000\n",
      "18/18 - 1s - loss: 1.0791 - val_loss: 1.1152\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.02567\n",
      "Epoch 174/3000\n",
      "18/18 - 1s - loss: 1.1413 - val_loss: 1.2129\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.02567\n",
      "Epoch 175/3000\n",
      "18/18 - 1s - loss: 1.1331 - val_loss: 1.1220\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.02567\n",
      "Epoch 176/3000\n",
      "18/18 - 1s - loss: 1.1234 - val_loss: 1.1010\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.02567\n",
      "Epoch 177/3000\n",
      "18/18 - 1s - loss: 1.0862 - val_loss: 1.0879\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.02567\n",
      "Epoch 178/3000\n",
      "18/18 - 1s - loss: 1.0763 - val_loss: 1.0800\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.02567\n",
      "Epoch 179/3000\n",
      "18/18 - 1s - loss: 1.0822 - val_loss: 1.0819\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.02567\n",
      "Epoch 180/3000\n",
      "18/18 - 1s - loss: 1.0779 - val_loss: 1.0830\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.02567\n",
      "Epoch 181/3000\n",
      "18/18 - 1s - loss: 1.0738 - val_loss: 1.1293\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.02567\n",
      "Epoch 182/3000\n",
      "18/18 - 1s - loss: 1.0712 - val_loss: 1.0724\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.02567\n",
      "Epoch 183/3000\n",
      "18/18 - 1s - loss: 1.0700 - val_loss: 1.0720\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.02567\n",
      "Epoch 184/3000\n",
      "18/18 - 1s - loss: 1.0688 - val_loss: 1.0726\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.02567\n",
      "Epoch 185/3000\n",
      "18/18 - 1s - loss: 1.0655 - val_loss: 1.0709\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.02567\n",
      "Epoch 186/3000\n",
      "18/18 - 1s - loss: 1.0626 - val_loss: 1.0673\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.02567\n",
      "Epoch 187/3000\n",
      "18/18 - 1s - loss: 1.0656 - val_loss: 1.0752\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.02567\n",
      "Epoch 188/3000\n",
      "18/18 - 1s - loss: 1.1018 - val_loss: 1.0676\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.02567\n",
      "Epoch 189/3000\n",
      "18/18 - 1s - loss: 1.0672 - val_loss: 1.1445\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.02567\n",
      "Epoch 190/3000\n",
      "18/18 - 1s - loss: 1.1029 - val_loss: 1.1129\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.02567\n",
      "Epoch 191/3000\n",
      "18/18 - 1s - loss: 1.0646 - val_loss: 1.0645\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.02567\n",
      "Epoch 192/3000\n",
      "18/18 - 1s - loss: 1.0787 - val_loss: 1.0655\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.02567\n",
      "Epoch 193/3000\n",
      "18/18 - 1s - loss: 1.0575 - val_loss: 1.2420\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.02567\n",
      "Epoch 194/3000\n",
      "18/18 - 1s - loss: 1.0636 - val_loss: 1.0513\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.02567\n",
      "Epoch 195/3000\n",
      "18/18 - 1s - loss: 1.0873 - val_loss: 1.0616\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.02567\n",
      "Epoch 196/3000\n",
      "18/18 - 1s - loss: 1.0617 - val_loss: 1.0743\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.02567\n",
      "Epoch 197/3000\n",
      "18/18 - 1s - loss: 1.0556 - val_loss: 1.0558\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.02567\n",
      "Epoch 198/3000\n",
      "18/18 - 1s - loss: 1.0480 - val_loss: 1.7479\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.02567\n",
      "Epoch 199/3000\n",
      "18/18 - 1s - loss: 1.1702 - val_loss: 1.0462\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.02567\n",
      "Epoch 200/3000\n",
      "18/18 - 1s - loss: 1.0690 - val_loss: 1.0586\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.02567\n",
      "Epoch 201/3000\n",
      "18/18 - 1s - loss: 1.1424 - val_loss: 1.0494\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.02567\n",
      "Epoch 202/3000\n",
      "18/18 - 1s - loss: 1.0648 - val_loss: 1.0469\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.02567\n",
      "Epoch 203/3000\n",
      "18/18 - 1s - loss: 1.0827 - val_loss: 1.0352\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.02567\n",
      "Epoch 204/3000\n",
      "18/18 - 1s - loss: 1.1188 - val_loss: 1.2074\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.02567\n",
      "Epoch 205/3000\n",
      "18/18 - 1s - loss: 1.1191 - val_loss: 1.0298\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.02567\n",
      "Epoch 206/3000\n",
      "18/18 - 1s - loss: 1.1249 - val_loss: 1.0239\n",
      "\n",
      "Epoch 00206: val_loss improved from 1.02567 to 1.02395, saving model to qkeras_weights.h5\n",
      "Epoch 207/3000\n",
      "18/18 - 1s - loss: 1.0400 - val_loss: 1.5536\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.02395\n",
      "Epoch 208/3000\n",
      "18/18 - 1s - loss: 1.1249 - val_loss: 1.2252\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.02395\n",
      "Epoch 209/3000\n",
      "18/18 - 1s - loss: 1.0984 - val_loss: 1.0357\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.02395\n",
      "Epoch 210/3000\n",
      "18/18 - 1s - loss: 1.0904 - val_loss: 1.0692\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.02395\n",
      "Epoch 211/3000\n",
      "18/18 - 1s - loss: 1.0947 - val_loss: 1.0082\n",
      "\n",
      "Epoch 00211: val_loss improved from 1.02395 to 1.00822, saving model to qkeras_weights.h5\n",
      "Epoch 212/3000\n",
      "18/18 - 1s - loss: 1.0243 - val_loss: 1.1997\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.00822\n",
      "Epoch 213/3000\n",
      "18/18 - 1s - loss: 1.0832 - val_loss: 1.0180\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.00822\n",
      "Epoch 214/3000\n",
      "18/18 - 1s - loss: 1.1496 - val_loss: 1.2317\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.00822\n",
      "Epoch 215/3000\n",
      "18/18 - 1s - loss: 1.0656 - val_loss: 1.0023\n",
      "\n",
      "Epoch 00215: val_loss improved from 1.00822 to 1.00232, saving model to qkeras_weights.h5\n",
      "Epoch 216/3000\n",
      "18/18 - 1s - loss: 1.0114 - val_loss: 1.5245\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.00232\n",
      "Epoch 217/3000\n",
      "18/18 - 1s - loss: 1.1658 - val_loss: 1.0131\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.00232\n",
      "Epoch 218/3000\n",
      "18/18 - 1s - loss: 1.0493 - val_loss: 0.9978\n",
      "\n",
      "Epoch 00218: val_loss improved from 1.00232 to 0.99780, saving model to qkeras_weights.h5\n",
      "Epoch 219/3000\n",
      "18/18 - 1s - loss: 1.0191 - val_loss: 1.0008\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 0.99780\n",
      "Epoch 220/3000\n",
      "18/18 - 1s - loss: 1.1173 - val_loss: 1.1001\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 0.99780\n",
      "Epoch 221/3000\n",
      "18/18 - 1s - loss: 1.0287 - val_loss: 1.0548\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 0.99780\n",
      "Epoch 222/3000\n",
      "18/18 - 1s - loss: 1.1155 - val_loss: 1.2281\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 0.99780\n",
      "Epoch 223/3000\n",
      "18/18 - 1s - loss: 1.0362 - val_loss: 1.0020\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 0.99780\n",
      "Epoch 224/3000\n",
      "18/18 - 1s - loss: 1.0925 - val_loss: 1.7301\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 0.99780\n",
      "Epoch 225/3000\n",
      "18/18 - 1s - loss: 1.0996 - val_loss: 0.9962\n",
      "\n",
      "Epoch 00225: val_loss improved from 0.99780 to 0.99617, saving model to qkeras_weights.h5\n",
      "Epoch 226/3000\n",
      "18/18 - 1s - loss: 1.0387 - val_loss: 1.0253\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 0.99617\n",
      "Epoch 227/3000\n",
      "18/18 - 1s - loss: 1.1091 - val_loss: 1.0257\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 0.99617\n",
      "Epoch 228/3000\n",
      "18/18 - 1s - loss: 1.0030 - val_loss: 1.0219\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 0.99617\n",
      "Epoch 229/3000\n",
      "18/18 - 1s - loss: 1.0637 - val_loss: 1.0036\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 0.99617\n",
      "Epoch 230/3000\n",
      "18/18 - 1s - loss: 1.0162 - val_loss: 1.0069\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 0.99617\n",
      "Epoch 231/3000\n",
      "18/18 - 1s - loss: 1.1844 - val_loss: 1.0098\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 0.99617\n",
      "Epoch 232/3000\n",
      "18/18 - 1s - loss: 1.0188 - val_loss: 1.0257\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 0.99617\n",
      "Epoch 233/3000\n",
      "18/18 - 1s - loss: 1.0271 - val_loss: 1.0445\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 0.99617\n",
      "Epoch 234/3000\n",
      "18/18 - 1s - loss: 1.0703 - val_loss: 1.0097\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 0.99617\n",
      "Epoch 235/3000\n",
      "18/18 - 1s - loss: 1.0729 - val_loss: 1.0106\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 0.99617\n",
      "Epoch 236/3000\n",
      "18/18 - 1s - loss: 1.0992 - val_loss: 1.0086\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 0.99617\n",
      "Epoch 237/3000\n",
      "18/18 - 1s - loss: 1.0452 - val_loss: 1.5063\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 0.99617\n",
      "Epoch 238/3000\n",
      "18/18 - 1s - loss: 1.0387 - val_loss: 1.0146\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 0.99617\n",
      "Epoch 239/3000\n",
      "18/18 - 1s - loss: 1.0838 - val_loss: 1.0035\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 0.99617\n",
      "Epoch 240/3000\n",
      "18/18 - 1s - loss: 1.0660 - val_loss: 1.0150\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 0.99617\n",
      "Epoch 241/3000\n",
      "18/18 - 1s - loss: 1.1263 - val_loss: 1.1129\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 0.99617\n",
      "Epoch 242/3000\n",
      "18/18 - 1s - loss: 1.0241 - val_loss: 1.0096\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 0.99617\n",
      "Epoch 243/3000\n",
      "18/18 - 1s - loss: 1.0078 - val_loss: 1.0023\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 0.99617\n",
      "Epoch 244/3000\n",
      "18/18 - 1s - loss: 1.0446 - val_loss: 1.1231\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 0.99617\n",
      "Epoch 245/3000\n",
      "18/18 - 1s - loss: 1.0244 - val_loss: 1.0061\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 0.99617\n",
      "Epoch 246/3000\n",
      "18/18 - 1s - loss: 1.0679 - val_loss: 1.1695\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 0.99617\n",
      "Epoch 247/3000\n",
      "18/18 - 1s - loss: 1.1736 - val_loss: 1.0102\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 0.99617\n",
      "Epoch 248/3000\n",
      "18/18 - 1s - loss: 1.0079 - val_loss: 0.9979\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 0.99617\n",
      "Epoch 249/3000\n",
      "18/18 - 1s - loss: 1.0029 - val_loss: 0.9984\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 0.99617\n",
      "Epoch 250/3000\n",
      "18/18 - 1s - loss: 1.2033 - val_loss: 1.0901\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 0.99617\n",
      "Epoch 251/3000\n",
      "18/18 - 1s - loss: 1.0184 - val_loss: 1.1638\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 0.99617\n",
      "Epoch 252/3000\n",
      "18/18 - 1s - loss: 1.0098 - val_loss: 0.9931\n",
      "\n",
      "Epoch 00252: val_loss improved from 0.99617 to 0.99308, saving model to qkeras_weights.h5\n",
      "Epoch 253/3000\n",
      "18/18 - 1s - loss: 1.0679 - val_loss: 0.9937\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 0.99308\n",
      "Epoch 254/3000\n",
      "18/18 - 1s - loss: 1.0723 - val_loss: 1.2739\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 0.99308\n",
      "Epoch 255/3000\n",
      "18/18 - 1s - loss: 1.1471 - val_loss: 1.0061\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 0.99308\n",
      "Epoch 256/3000\n",
      "18/18 - 1s - loss: 1.0114 - val_loss: 1.0079\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 0.99308\n",
      "Epoch 257/3000\n",
      "18/18 - 1s - loss: 1.0062 - val_loss: 0.9914\n",
      "\n",
      "Epoch 00257: val_loss improved from 0.99308 to 0.99135, saving model to qkeras_weights.h5\n",
      "Epoch 258/3000\n",
      "18/18 - 1s - loss: 1.1117 - val_loss: 1.1072\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 0.99135\n",
      "Epoch 259/3000\n",
      "18/18 - 1s - loss: 1.0373 - val_loss: 1.1530\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 0.99135\n",
      "Epoch 260/3000\n",
      "18/18 - 1s - loss: 1.0579 - val_loss: 1.0685\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 0.99135\n",
      "Epoch 261/3000\n",
      "18/18 - 1s - loss: 1.0741 - val_loss: 1.2070\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 0.99135\n",
      "Epoch 262/3000\n",
      "18/18 - 1s - loss: 1.1083 - val_loss: 1.0256\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 0.99135\n",
      "Epoch 263/3000\n",
      "18/18 - 1s - loss: 1.0269 - val_loss: 1.0241\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 0.99135\n",
      "Epoch 264/3000\n",
      "18/18 - 1s - loss: 1.0463 - val_loss: 1.0194\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 0.99135\n",
      "Epoch 265/3000\n",
      "18/18 - 1s - loss: 1.2346 - val_loss: 1.0320\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 0.99135\n",
      "Epoch 266/3000\n",
      "18/18 - 1s - loss: 1.0667 - val_loss: 1.1336\n",
      "\n",
      "Epoch 00266: val_loss did not improve from 0.99135\n",
      "Epoch 267/3000\n",
      "18/18 - 1s - loss: 1.1997 - val_loss: 1.0570\n",
      "\n",
      "Epoch 00267: val_loss did not improve from 0.99135\n",
      "Epoch 268/3000\n",
      "18/18 - 1s - loss: 1.0368 - val_loss: 2.7058\n",
      "\n",
      "Epoch 00268: val_loss did not improve from 0.99135\n",
      "Epoch 269/3000\n",
      "18/18 - 1s - loss: 1.3776 - val_loss: 1.0142\n",
      "\n",
      "Epoch 00269: val_loss did not improve from 0.99135\n",
      "Epoch 270/3000\n",
      "18/18 - 1s - loss: 1.5945 - val_loss: 1.5672\n",
      "\n",
      "Epoch 00270: val_loss did not improve from 0.99135\n",
      "Epoch 271/3000\n",
      "18/18 - 1s - loss: 1.4477 - val_loss: 1.0270\n",
      "\n",
      "Epoch 00271: val_loss did not improve from 0.99135\n",
      "Epoch 272/3000\n",
      "18/18 - 1s - loss: 1.1454 - val_loss: 1.6202\n",
      "\n",
      "Epoch 00272: val_loss did not improve from 0.99135\n",
      "Epoch 273/3000\n",
      "18/18 - 1s - loss: 1.4134 - val_loss: 1.4351\n",
      "\n",
      "Epoch 00273: val_loss did not improve from 0.99135\n",
      "Epoch 274/3000\n",
      "18/18 - 1s - loss: 1.1026 - val_loss: 1.1389\n",
      "\n",
      "Epoch 00274: val_loss did not improve from 0.99135\n",
      "Epoch 275/3000\n",
      "18/18 - 1s - loss: 1.2039 - val_loss: 1.1446\n",
      "\n",
      "Epoch 00275: val_loss did not improve from 0.99135\n",
      "Epoch 276/3000\n",
      "18/18 - 1s - loss: 1.1871 - val_loss: 1.0197\n",
      "\n",
      "Epoch 00276: val_loss did not improve from 0.99135\n",
      "Epoch 277/3000\n",
      "18/18 - 1s - loss: 1.1122 - val_loss: 1.1077\n",
      "\n",
      "Epoch 00277: val_loss did not improve from 0.99135\n",
      "Epoch 278/3000\n",
      "18/18 - 1s - loss: 1.3272 - val_loss: 2.5849\n",
      "\n",
      "Epoch 00278: val_loss did not improve from 0.99135\n",
      "Epoch 279/3000\n",
      "18/18 - 1s - loss: 1.5135 - val_loss: 2.6043\n",
      "\n",
      "Epoch 00279: val_loss did not improve from 0.99135\n",
      "Epoch 280/3000\n",
      "18/18 - 1s - loss: 1.3204 - val_loss: 1.0054\n",
      "\n",
      "Epoch 00280: val_loss did not improve from 0.99135\n",
      "Epoch 281/3000\n",
      "18/18 - 1s - loss: 1.4772 - val_loss: 2.6960\n",
      "\n",
      "Epoch 00281: val_loss did not improve from 0.99135\n",
      "Epoch 282/3000\n",
      "18/18 - 1s - loss: 1.1840 - val_loss: 1.0321\n",
      "\n",
      "Epoch 00282: val_loss did not improve from 0.99135\n",
      "Epoch 283/3000\n",
      "18/18 - 1s - loss: 1.0162 - val_loss: 2.6839\n",
      "\n",
      "Epoch 00283: val_loss did not improve from 0.99135\n",
      "Epoch 284/3000\n",
      "18/18 - 1s - loss: 1.2699 - val_loss: 1.0167\n",
      "\n",
      "Epoch 00284: val_loss did not improve from 0.99135\n",
      "Epoch 285/3000\n",
      "18/18 - 1s - loss: 1.0028 - val_loss: 1.0242\n",
      "\n",
      "Epoch 00285: val_loss did not improve from 0.99135\n",
      "Epoch 286/3000\n",
      "18/18 - 1s - loss: 1.1274 - val_loss: 1.0207\n",
      "\n",
      "Epoch 00286: val_loss did not improve from 0.99135\n",
      "Epoch 287/3000\n",
      "18/18 - 1s - loss: 1.2738 - val_loss: 1.5172\n",
      "\n",
      "Epoch 00287: val_loss did not improve from 0.99135\n",
      "Epoch 288/3000\n",
      "18/18 - 1s - loss: 1.5465 - val_loss: 1.1693\n",
      "\n",
      "Epoch 00288: val_loss did not improve from 0.99135\n",
      "Epoch 289/3000\n",
      "18/18 - 1s - loss: 1.4215 - val_loss: 2.6799\n",
      "\n",
      "Epoch 00289: val_loss did not improve from 0.99135\n",
      "Epoch 290/3000\n",
      "18/18 - 1s - loss: 1.5757 - val_loss: 1.2849\n",
      "\n",
      "Epoch 00290: val_loss did not improve from 0.99135\n",
      "Epoch 291/3000\n",
      "18/18 - 1s - loss: 1.4864 - val_loss: 1.2438\n",
      "\n",
      "Epoch 00291: val_loss did not improve from 0.99135\n",
      "Epoch 292/3000\n",
      "18/18 - 1s - loss: 1.4872 - val_loss: 1.0230\n",
      "\n",
      "Epoch 00292: val_loss did not improve from 0.99135\n",
      "Epoch 293/3000\n",
      "18/18 - 1s - loss: 1.4879 - val_loss: 1.1405\n",
      "\n",
      "Epoch 00293: val_loss did not improve from 0.99135\n",
      "Epoch 294/3000\n",
      "18/18 - 1s - loss: 1.1979 - val_loss: 1.0047\n",
      "\n",
      "Epoch 00294: val_loss did not improve from 0.99135\n",
      "Epoch 295/3000\n",
      "18/18 - 1s - loss: 1.3053 - val_loss: 1.1102\n",
      "\n",
      "Epoch 00295: val_loss did not improve from 0.99135\n",
      "Epoch 296/3000\n",
      "18/18 - 1s - loss: 1.2788 - val_loss: 3.2219\n",
      "\n",
      "Epoch 00296: val_loss did not improve from 0.99135\n",
      "Epoch 297/3000\n",
      "18/18 - 1s - loss: 1.4516 - val_loss: 1.0092\n",
      "\n",
      "Epoch 00297: val_loss did not improve from 0.99135\n",
      "Epoch 298/3000\n",
      "18/18 - 1s - loss: 1.2019 - val_loss: 0.9980\n",
      "\n",
      "Epoch 00298: val_loss did not improve from 0.99135\n",
      "Epoch 299/3000\n",
      "18/18 - 1s - loss: 1.0711 - val_loss: 0.9877\n",
      "\n",
      "Epoch 00299: val_loss improved from 0.99135 to 0.98766, saving model to qkeras_weights.h5\n",
      "Epoch 300/3000\n",
      "18/18 - 1s - loss: 1.5492 - val_loss: 1.0907\n",
      "\n",
      "Epoch 00300: val_loss did not improve from 0.98766\n",
      "Epoch 301/3000\n",
      "18/18 - 1s - loss: 1.3043 - val_loss: 1.0921\n",
      "\n",
      "Epoch 00301: val_loss did not improve from 0.98766\n",
      "Epoch 302/3000\n",
      "18/18 - 1s - loss: 1.2300 - val_loss: 1.0948\n",
      "\n",
      "Epoch 00302: val_loss did not improve from 0.98766\n",
      "Epoch 303/3000\n",
      "18/18 - 1s - loss: 1.2738 - val_loss: 2.6762\n",
      "\n",
      "Epoch 00303: val_loss did not improve from 0.98766\n",
      "Epoch 304/3000\n",
      "18/18 - 1s - loss: 1.2096 - val_loss: 0.9847\n",
      "\n",
      "Epoch 00304: val_loss improved from 0.98766 to 0.98471, saving model to qkeras_weights.h5\n",
      "Epoch 305/3000\n",
      "18/18 - 1s - loss: 1.3125 - val_loss: 0.9938\n",
      "\n",
      "Epoch 00305: val_loss did not improve from 0.98471\n",
      "Epoch 306/3000\n",
      "18/18 - 1s - loss: 1.2994 - val_loss: 0.9786\n",
      "\n",
      "Epoch 00306: val_loss improved from 0.98471 to 0.97865, saving model to qkeras_weights.h5\n",
      "Epoch 307/3000\n",
      "18/18 - 1s - loss: 1.0437 - val_loss: 0.9825\n",
      "\n",
      "Epoch 00307: val_loss did not improve from 0.97865\n",
      "Epoch 308/3000\n",
      "18/18 - 1s - loss: 1.1199 - val_loss: 1.2570\n",
      "\n",
      "Epoch 00308: val_loss did not improve from 0.97865\n",
      "Epoch 309/3000\n",
      "18/18 - 1s - loss: 1.4692 - val_loss: 1.0092\n",
      "\n",
      "Epoch 00309: val_loss did not improve from 0.97865\n",
      "Epoch 310/3000\n",
      "18/18 - 1s - loss: 1.1425 - val_loss: 0.9936\n",
      "\n",
      "Epoch 00310: val_loss did not improve from 0.97865\n",
      "Epoch 311/3000\n",
      "18/18 - 1s - loss: 1.2904 - val_loss: 0.9842\n",
      "\n",
      "Epoch 00311: val_loss did not improve from 0.97865\n",
      "Epoch 312/3000\n",
      "18/18 - 1s - loss: 1.3057 - val_loss: 1.1146\n",
      "\n",
      "Epoch 00312: val_loss did not improve from 0.97865\n",
      "Epoch 313/3000\n",
      "18/18 - 1s - loss: 1.1488 - val_loss: 1.0105\n",
      "\n",
      "Epoch 00313: val_loss did not improve from 0.97865\n",
      "Epoch 314/3000\n",
      "18/18 - 1s - loss: 1.1375 - val_loss: 1.0547\n",
      "\n",
      "Epoch 00314: val_loss did not improve from 0.97865\n",
      "Epoch 315/3000\n",
      "18/18 - 1s - loss: 1.3408 - val_loss: 1.0812\n",
      "\n",
      "Epoch 00315: val_loss did not improve from 0.97865\n",
      "Epoch 316/3000\n",
      "18/18 - 1s - loss: 0.9939 - val_loss: 0.9902\n",
      "\n",
      "Epoch 00316: val_loss did not improve from 0.97865\n",
      "Epoch 317/3000\n",
      "18/18 - 1s - loss: 1.1139 - val_loss: 0.9958\n",
      "\n",
      "Epoch 00317: val_loss did not improve from 0.97865\n",
      "Epoch 318/3000\n",
      "18/18 - 1s - loss: 1.1749 - val_loss: 1.5386\n",
      "\n",
      "Epoch 00318: val_loss did not improve from 0.97865\n",
      "Epoch 319/3000\n",
      "18/18 - 1s - loss: 1.1365 - val_loss: 1.0163\n",
      "\n",
      "Epoch 00319: val_loss did not improve from 0.97865\n",
      "Epoch 320/3000\n",
      "18/18 - 1s - loss: 1.0100 - val_loss: 1.5327\n",
      "\n",
      "Epoch 00320: val_loss did not improve from 0.97865\n",
      "Epoch 321/3000\n",
      "18/18 - 1s - loss: 1.4384 - val_loss: 0.9874\n",
      "\n",
      "Epoch 00321: val_loss did not improve from 0.97865\n",
      "Epoch 322/3000\n",
      "18/18 - 1s - loss: 0.9812 - val_loss: 0.9842\n",
      "\n",
      "Epoch 00322: val_loss did not improve from 0.97865\n",
      "Epoch 323/3000\n",
      "18/18 - 1s - loss: 1.1214 - val_loss: 0.9808\n",
      "\n",
      "Epoch 00323: val_loss did not improve from 0.97865\n",
      "Epoch 324/3000\n",
      "18/18 - 1s - loss: 1.1112 - val_loss: 0.9905\n",
      "\n",
      "Epoch 00324: val_loss did not improve from 0.97865\n",
      "Epoch 325/3000\n",
      "18/18 - 1s - loss: 1.0522 - val_loss: 1.0795\n",
      "\n",
      "Epoch 00325: val_loss did not improve from 0.97865\n",
      "Epoch 326/3000\n",
      "18/18 - 1s - loss: 1.0927 - val_loss: 1.0732\n",
      "\n",
      "Epoch 00326: val_loss did not improve from 0.97865\n",
      "Epoch 327/3000\n",
      "18/18 - 1s - loss: 1.1696 - val_loss: 1.5931\n",
      "\n",
      "Epoch 00327: val_loss did not improve from 0.97865\n",
      "Epoch 328/3000\n",
      "18/18 - 1s - loss: 1.1688 - val_loss: 1.6868\n",
      "\n",
      "Epoch 00328: val_loss did not improve from 0.97865\n",
      "Epoch 329/3000\n",
      "18/18 - 1s - loss: 1.2056 - val_loss: 1.0280\n",
      "\n",
      "Epoch 00329: val_loss did not improve from 0.97865\n",
      "Epoch 330/3000\n",
      "18/18 - 1s - loss: 1.0132 - val_loss: 1.0364\n",
      "\n",
      "Epoch 00330: val_loss did not improve from 0.97865\n",
      "Epoch 331/3000\n",
      "18/18 - 1s - loss: 1.0571 - val_loss: 1.1607\n",
      "\n",
      "Epoch 00331: val_loss did not improve from 0.97865\n",
      "Epoch 332/3000\n",
      "18/18 - 1s - loss: 1.1609 - val_loss: 0.9843\n",
      "\n",
      "Epoch 00332: val_loss did not improve from 0.97865\n",
      "Epoch 333/3000\n",
      "18/18 - 1s - loss: 1.2850 - val_loss: 1.0017\n",
      "\n",
      "Epoch 00333: val_loss did not improve from 0.97865\n",
      "Epoch 334/3000\n",
      "18/18 - 1s - loss: 1.1111 - val_loss: 1.0118\n",
      "\n",
      "Epoch 00334: val_loss did not improve from 0.97865\n",
      "Epoch 335/3000\n",
      "18/18 - 1s - loss: 0.9899 - val_loss: 0.9951\n",
      "\n",
      "Epoch 00335: val_loss did not improve from 0.97865\n",
      "Epoch 336/3000\n",
      "18/18 - 1s - loss: 1.2413 - val_loss: 1.0011\n",
      "\n",
      "Epoch 00336: val_loss did not improve from 0.97865\n",
      "Epoch 337/3000\n",
      "18/18 - 1s - loss: 1.1803 - val_loss: 1.0562\n",
      "\n",
      "Epoch 00337: val_loss did not improve from 0.97865\n",
      "Epoch 338/3000\n",
      "18/18 - 1s - loss: 1.4805 - val_loss: 0.9861\n",
      "\n",
      "Epoch 00338: val_loss did not improve from 0.97865\n",
      "Epoch 339/3000\n",
      "18/18 - 1s - loss: 1.2082 - val_loss: 2.6623\n",
      "\n",
      "Epoch 00339: val_loss did not improve from 0.97865\n",
      "Epoch 340/3000\n",
      "18/18 - 1s - loss: 1.4284 - val_loss: 1.7887\n",
      "\n",
      "Epoch 00340: val_loss did not improve from 0.97865\n",
      "Epoch 341/3000\n",
      "18/18 - 1s - loss: 1.6036 - val_loss: 2.6256\n",
      "\n",
      "Epoch 00341: val_loss did not improve from 0.97865\n",
      "Epoch 342/3000\n",
      "18/18 - 1s - loss: 1.3674 - val_loss: 1.0962\n",
      "\n",
      "Epoch 00342: val_loss did not improve from 0.97865\n",
      "Epoch 343/3000\n",
      "18/18 - 1s - loss: 1.2866 - val_loss: 1.1520\n",
      "\n",
      "Epoch 00343: val_loss did not improve from 0.97865\n",
      "Epoch 344/3000\n",
      "18/18 - 1s - loss: 1.3495 - val_loss: 0.9956\n",
      "\n",
      "Epoch 00344: val_loss did not improve from 0.97865\n",
      "Epoch 345/3000\n",
      "18/18 - 1s - loss: 1.2752 - val_loss: 1.5292\n",
      "\n",
      "Epoch 00345: val_loss did not improve from 0.97865\n",
      "Epoch 346/3000\n",
      "18/18 - 1s - loss: 1.0802 - val_loss: 1.0108\n",
      "\n",
      "Epoch 00346: val_loss did not improve from 0.97865\n",
      "Epoch 347/3000\n",
      "18/18 - 1s - loss: 1.1817 - val_loss: 1.2641\n",
      "\n",
      "Epoch 00347: val_loss did not improve from 0.97865\n",
      "Epoch 348/3000\n",
      "18/18 - 1s - loss: 1.4442 - val_loss: 0.9786\n",
      "\n",
      "Epoch 00348: val_loss improved from 0.97865 to 0.97858, saving model to qkeras_weights.h5\n",
      "Epoch 349/3000\n",
      "18/18 - 1s - loss: 1.4185 - val_loss: 1.0020\n",
      "\n",
      "Epoch 00349: val_loss did not improve from 0.97858\n",
      "Epoch 350/3000\n",
      "18/18 - 1s - loss: 1.3185 - val_loss: 2.6811\n",
      "\n",
      "Epoch 00350: val_loss did not improve from 0.97858\n",
      "Epoch 351/3000\n",
      "18/18 - 1s - loss: 1.5129 - val_loss: 1.2641\n",
      "\n",
      "Epoch 00351: val_loss did not improve from 0.97858\n",
      "Epoch 352/3000\n",
      "18/18 - 1s - loss: 1.3710 - val_loss: 1.0301\n",
      "\n",
      "Epoch 00352: val_loss did not improve from 0.97858\n",
      "Epoch 353/3000\n",
      "18/18 - 1s - loss: 1.3443 - val_loss: 0.9827\n",
      "\n",
      "Epoch 00353: val_loss did not improve from 0.97858\n",
      "Epoch 354/3000\n",
      "18/18 - 1s - loss: 1.4005 - val_loss: 0.9926\n",
      "\n",
      "Epoch 00354: val_loss did not improve from 0.97858\n",
      "Epoch 355/3000\n",
      "18/18 - 1s - loss: 1.0240 - val_loss: 0.9900\n",
      "\n",
      "Epoch 00355: val_loss did not improve from 0.97858\n",
      "Epoch 356/3000\n",
      "18/18 - 1s - loss: 1.3482 - val_loss: 2.6707\n",
      "\n",
      "Epoch 00356: val_loss did not improve from 0.97858\n",
      "Epoch 357/3000\n",
      "18/18 - 1s - loss: 1.7423 - val_loss: 1.0417\n",
      "\n",
      "Epoch 00357: val_loss did not improve from 0.97858\n",
      "Epoch 358/3000\n",
      "18/18 - 1s - loss: 1.2413 - val_loss: 0.9825\n",
      "\n",
      "Epoch 00358: val_loss did not improve from 0.97858\n",
      "Epoch 359/3000\n",
      "18/18 - 1s - loss: 1.1027 - val_loss: 0.9870\n",
      "\n",
      "Epoch 00359: val_loss did not improve from 0.97858\n",
      "Epoch 360/3000\n",
      "18/18 - 1s - loss: 1.2651 - val_loss: 1.1174\n",
      "\n",
      "Epoch 00360: val_loss did not improve from 0.97858\n",
      "Epoch 361/3000\n",
      "18/18 - 1s - loss: 1.0101 - val_loss: 0.9983\n",
      "\n",
      "Epoch 00361: val_loss did not improve from 0.97858\n",
      "Epoch 362/3000\n",
      "18/18 - 1s - loss: 0.9812 - val_loss: 1.7287\n",
      "\n",
      "Epoch 00362: val_loss did not improve from 0.97858\n",
      "Epoch 363/3000\n",
      "18/18 - 1s - loss: 1.1520 - val_loss: 0.9992\n",
      "\n",
      "Epoch 00363: val_loss did not improve from 0.97858\n",
      "Epoch 364/3000\n",
      "18/18 - 1s - loss: 1.2273 - val_loss: 3.1636\n",
      "\n",
      "Epoch 00364: val_loss did not improve from 0.97858\n",
      "Epoch 365/3000\n",
      "18/18 - 1s - loss: 1.0870 - val_loss: 0.9887\n",
      "\n",
      "Epoch 00365: val_loss did not improve from 0.97858\n",
      "Epoch 366/3000\n",
      "18/18 - 1s - loss: 1.1078 - val_loss: 0.9869\n",
      "\n",
      "Epoch 00366: val_loss did not improve from 0.97858\n",
      "Epoch 367/3000\n",
      "18/18 - 1s - loss: 1.0482 - val_loss: 1.0074\n",
      "\n",
      "Epoch 00367: val_loss did not improve from 0.97858\n",
      "Epoch 368/3000\n",
      "18/18 - 1s - loss: 0.9836 - val_loss: 0.9813\n",
      "\n",
      "Epoch 00368: val_loss did not improve from 0.97858\n",
      "Epoch 369/3000\n",
      "18/18 - 1s - loss: 1.2726 - val_loss: 1.0186\n",
      "\n",
      "Epoch 00369: val_loss did not improve from 0.97858\n",
      "Epoch 370/3000\n",
      "18/18 - 1s - loss: 0.9815 - val_loss: 1.0025\n",
      "\n",
      "Epoch 00370: val_loss did not improve from 0.97858\n",
      "Epoch 371/3000\n",
      "18/18 - 1s - loss: 1.1793 - val_loss: 0.9858\n",
      "\n",
      "Epoch 00371: val_loss did not improve from 0.97858\n",
      "Epoch 372/3000\n",
      "18/18 - 1s - loss: 1.0589 - val_loss: 0.9877\n",
      "\n",
      "Epoch 00372: val_loss did not improve from 0.97858\n",
      "Epoch 373/3000\n",
      "18/18 - 1s - loss: 1.1690 - val_loss: 0.9967\n",
      "\n",
      "Epoch 00373: val_loss did not improve from 0.97858\n",
      "Epoch 374/3000\n",
      "18/18 - 1s - loss: 1.0597 - val_loss: 1.0284\n",
      "\n",
      "Epoch 00374: val_loss did not improve from 0.97858\n",
      "Epoch 375/3000\n",
      "18/18 - 1s - loss: 1.4724 - val_loss: 1.1475\n",
      "\n",
      "Epoch 00375: val_loss did not improve from 0.97858\n",
      "Epoch 376/3000\n",
      "18/18 - 1s - loss: 1.2027 - val_loss: 1.0546\n",
      "\n",
      "Epoch 00376: val_loss did not improve from 0.97858\n",
      "Epoch 377/3000\n",
      "18/18 - 1s - loss: 1.3722 - val_loss: 1.1657\n",
      "\n",
      "Epoch 00377: val_loss did not improve from 0.97858\n",
      "Epoch 378/3000\n",
      "18/18 - 1s - loss: 1.6748 - val_loss: 1.1799\n",
      "\n",
      "Epoch 00378: val_loss did not improve from 0.97858\n",
      "Epoch 379/3000\n",
      "18/18 - 1s - loss: 1.4920 - val_loss: 2.6418\n",
      "\n",
      "Epoch 00379: val_loss did not improve from 0.97858\n",
      "Epoch 380/3000\n",
      "18/18 - 1s - loss: 1.3922 - val_loss: 0.9924\n",
      "\n",
      "Epoch 00380: val_loss did not improve from 0.97858\n",
      "Epoch 381/3000\n",
      "18/18 - 1s - loss: 1.4334 - val_loss: 2.4940\n",
      "\n",
      "Epoch 00381: val_loss did not improve from 0.97858\n",
      "Epoch 382/3000\n",
      "18/18 - 1s - loss: 1.6502 - val_loss: 1.7568\n",
      "\n",
      "Epoch 00382: val_loss did not improve from 0.97858\n",
      "Epoch 383/3000\n",
      "18/18 - 1s - loss: 1.4000 - val_loss: 1.0009\n",
      "\n",
      "Epoch 00383: val_loss did not improve from 0.97858\n",
      "Epoch 384/3000\n",
      "18/18 - 1s - loss: 1.4202 - val_loss: 1.4257\n",
      "\n",
      "Epoch 00384: val_loss did not improve from 0.97858\n",
      "Epoch 385/3000\n",
      "18/18 - 1s - loss: 1.1133 - val_loss: 1.5642\n",
      "\n",
      "Epoch 00385: val_loss did not improve from 0.97858\n",
      "Epoch 386/3000\n",
      "18/18 - 1s - loss: 1.0806 - val_loss: 1.0407\n",
      "\n",
      "Epoch 00386: val_loss did not improve from 0.97858\n",
      "Epoch 387/3000\n",
      "18/18 - 1s - loss: 1.0587 - val_loss: 1.1359\n",
      "\n",
      "Epoch 00387: val_loss did not improve from 0.97858\n",
      "Epoch 388/3000\n",
      "18/18 - 1s - loss: 1.0134 - val_loss: 1.1192\n",
      "\n",
      "Epoch 00388: val_loss did not improve from 0.97858\n",
      "Epoch 389/3000\n",
      "18/18 - 1s - loss: 1.0880 - val_loss: 1.0044\n",
      "\n",
      "Epoch 00389: val_loss did not improve from 0.97858\n",
      "Epoch 390/3000\n",
      "18/18 - 1s - loss: 1.1266 - val_loss: 0.9968\n",
      "\n",
      "Epoch 00390: val_loss did not improve from 0.97858\n",
      "Epoch 391/3000\n",
      "18/18 - 1s - loss: 1.1449 - val_loss: 1.0031\n",
      "\n",
      "Epoch 00391: val_loss did not improve from 0.97858\n",
      "Epoch 392/3000\n",
      "18/18 - 1s - loss: 1.0591 - val_loss: 1.2251\n",
      "\n",
      "Epoch 00392: val_loss did not improve from 0.97858\n",
      "Epoch 393/3000\n",
      "18/18 - 1s - loss: 1.2299 - val_loss: 0.9932\n",
      "\n",
      "Epoch 00393: val_loss did not improve from 0.97858\n",
      "Epoch 394/3000\n",
      "18/18 - 1s - loss: 1.0491 - val_loss: 1.1859\n",
      "\n",
      "Epoch 00394: val_loss did not improve from 0.97858\n",
      "Epoch 395/3000\n",
      "18/18 - 1s - loss: 1.1119 - val_loss: 1.2133\n",
      "\n",
      "Epoch 00395: val_loss did not improve from 0.97858\n",
      "Epoch 396/3000\n",
      "18/18 - 1s - loss: 1.1783 - val_loss: 1.0093\n",
      "\n",
      "Epoch 00396: val_loss did not improve from 0.97858\n",
      "Epoch 397/3000\n",
      "18/18 - 1s - loss: 1.0451 - val_loss: 1.1945\n",
      "\n",
      "Epoch 00397: val_loss did not improve from 0.97858\n",
      "Epoch 398/3000\n",
      "18/18 - 1s - loss: 1.1520 - val_loss: 1.0972\n",
      "\n",
      "Epoch 00398: val_loss did not improve from 0.97858\n",
      "Epoch 399/3000\n",
      "18/18 - 1s - loss: 1.2757 - val_loss: 1.0995\n",
      "\n",
      "Epoch 00399: val_loss did not improve from 0.97858\n",
      "Epoch 400/3000\n",
      "18/18 - 1s - loss: 1.0138 - val_loss: 1.0743\n",
      "\n",
      "Epoch 00400: val_loss did not improve from 0.97858\n",
      "Epoch 401/3000\n",
      "18/18 - 1s - loss: 1.0127 - val_loss: 1.0221\n",
      "\n",
      "Epoch 00401: val_loss did not improve from 0.97858\n",
      "Epoch 402/3000\n",
      "18/18 - 1s - loss: 1.1532 - val_loss: 1.0686\n",
      "\n",
      "Epoch 00402: val_loss did not improve from 0.97858\n",
      "Epoch 403/3000\n",
      "18/18 - 1s - loss: 1.0670 - val_loss: 1.0918\n",
      "\n",
      "Epoch 00403: val_loss did not improve from 0.97858\n",
      "Epoch 404/3000\n",
      "18/18 - 1s - loss: 1.3659 - val_loss: 1.0347\n",
      "\n",
      "Epoch 00404: val_loss did not improve from 0.97858\n",
      "Epoch 405/3000\n",
      "18/18 - 1s - loss: 1.2721 - val_loss: 1.0292\n",
      "\n",
      "Epoch 00405: val_loss did not improve from 0.97858\n",
      "Epoch 406/3000\n",
      "18/18 - 1s - loss: 1.2540 - val_loss: 1.0171\n",
      "\n",
      "Epoch 00406: val_loss did not improve from 0.97858\n",
      "Epoch 407/3000\n",
      "18/18 - 1s - loss: 1.0225 - val_loss: 1.1432\n",
      "\n",
      "Epoch 00407: val_loss did not improve from 0.97858\n",
      "Epoch 408/3000\n",
      "18/18 - 1s - loss: 1.0067 - val_loss: 0.9944\n",
      "\n",
      "Epoch 00408: val_loss did not improve from 0.97858\n",
      "Epoch 409/3000\n",
      "18/18 - 1s - loss: 1.3975 - val_loss: 1.0867\n",
      "\n",
      "Epoch 00409: val_loss did not improve from 0.97858\n",
      "Epoch 410/3000\n",
      "18/18 - 1s - loss: 1.2069 - val_loss: 1.1565\n",
      "\n",
      "Epoch 00410: val_loss did not improve from 0.97858\n",
      "Epoch 411/3000\n",
      "18/18 - 1s - loss: 1.0791 - val_loss: 0.9956\n",
      "\n",
      "Epoch 00411: val_loss did not improve from 0.97858\n",
      "Epoch 412/3000\n",
      "18/18 - 1s - loss: 1.2046 - val_loss: 1.0318\n",
      "\n",
      "Epoch 00412: val_loss did not improve from 0.97858\n",
      "Epoch 413/3000\n",
      "18/18 - 1s - loss: 1.1601 - val_loss: 1.1803\n",
      "\n",
      "Epoch 00413: val_loss did not improve from 0.97858\n",
      "Epoch 414/3000\n",
      "18/18 - 1s - loss: 1.3029 - val_loss: 1.0097\n",
      "\n",
      "Epoch 00414: val_loss did not improve from 0.97858\n",
      "Epoch 415/3000\n",
      "18/18 - 1s - loss: 1.2721 - val_loss: 1.0057\n",
      "\n",
      "Epoch 00415: val_loss did not improve from 0.97858\n",
      "Epoch 416/3000\n",
      "18/18 - 1s - loss: 1.0037 - val_loss: 1.3168\n",
      "\n",
      "Epoch 00416: val_loss did not improve from 0.97858\n",
      "Epoch 417/3000\n",
      "18/18 - 1s - loss: 1.1528 - val_loss: 2.4899\n",
      "\n",
      "Epoch 00417: val_loss did not improve from 0.97858\n",
      "Epoch 418/3000\n",
      "18/18 - 1s - loss: 1.1932 - val_loss: 1.0601\n",
      "\n",
      "Epoch 00418: val_loss did not improve from 0.97858\n",
      "Epoch 419/3000\n",
      "18/18 - 1s - loss: 1.0025 - val_loss: 2.4850\n",
      "\n",
      "Epoch 00419: val_loss did not improve from 0.97858\n",
      "Epoch 420/3000\n",
      "18/18 - 1s - loss: 1.2890 - val_loss: 1.1153\n",
      "\n",
      "Epoch 00420: val_loss did not improve from 0.97858\n",
      "Epoch 421/3000\n",
      "18/18 - 1s - loss: 1.0304 - val_loss: 1.3426\n",
      "\n",
      "Epoch 00421: val_loss did not improve from 0.97858\n",
      "Epoch 422/3000\n",
      "18/18 - 1s - loss: 1.0098 - val_loss: 1.4749\n",
      "\n",
      "Epoch 00422: val_loss did not improve from 0.97858\n",
      "Epoch 423/3000\n",
      "18/18 - 1s - loss: 1.1880 - val_loss: 1.1955\n",
      "\n",
      "Epoch 00423: val_loss did not improve from 0.97858\n",
      "Epoch 424/3000\n",
      "18/18 - 1s - loss: 1.1885 - val_loss: 0.9869\n",
      "\n",
      "Epoch 00424: val_loss did not improve from 0.97858\n",
      "Epoch 425/3000\n",
      "18/18 - 1s - loss: 1.0068 - val_loss: 0.9680\n",
      "\n",
      "Epoch 00425: val_loss improved from 0.97858 to 0.96803, saving model to qkeras_weights.h5\n",
      "Epoch 426/3000\n",
      "18/18 - 1s - loss: 1.2712 - val_loss: 1.0027\n",
      "\n",
      "Epoch 00426: val_loss did not improve from 0.96803\n",
      "Epoch 427/3000\n",
      "18/18 - 1s - loss: 1.1191 - val_loss: 0.9713\n",
      "\n",
      "Epoch 00427: val_loss did not improve from 0.96803\n",
      "Epoch 428/3000\n",
      "18/18 - 1s - loss: 1.0989 - val_loss: 1.4262\n",
      "\n",
      "Epoch 00428: val_loss did not improve from 0.96803\n",
      "Epoch 429/3000\n",
      "18/18 - 1s - loss: 1.1195 - val_loss: 1.0191\n",
      "\n",
      "Epoch 00429: val_loss did not improve from 0.96803\n",
      "Epoch 430/3000\n",
      "18/18 - 1s - loss: 1.2180 - val_loss: 0.9897\n",
      "\n",
      "Epoch 00430: val_loss did not improve from 0.96803\n",
      "Epoch 431/3000\n",
      "18/18 - 1s - loss: 1.1367 - val_loss: 1.1583\n",
      "\n",
      "Epoch 00431: val_loss did not improve from 0.96803\n",
      "Epoch 432/3000\n",
      "18/18 - 1s - loss: 1.0437 - val_loss: 2.6562\n",
      "\n",
      "Epoch 00432: val_loss did not improve from 0.96803\n",
      "Epoch 433/3000\n",
      "18/18 - 1s - loss: 1.0756 - val_loss: 1.2239\n",
      "\n",
      "Epoch 00433: val_loss did not improve from 0.96803\n",
      "Epoch 434/3000\n",
      "18/18 - 1s - loss: 1.2129 - val_loss: 1.1342\n",
      "\n",
      "Epoch 00434: val_loss did not improve from 0.96803\n",
      "Epoch 435/3000\n",
      "18/18 - 1s - loss: 1.1792 - val_loss: 1.0764\n",
      "\n",
      "Epoch 00435: val_loss did not improve from 0.96803\n",
      "Epoch 436/3000\n",
      "18/18 - 1s - loss: 1.0135 - val_loss: 1.0190\n",
      "\n",
      "Epoch 00436: val_loss did not improve from 0.96803\n",
      "Epoch 437/3000\n",
      "18/18 - 1s - loss: 1.3201 - val_loss: 1.0049\n",
      "\n",
      "Epoch 00437: val_loss did not improve from 0.96803\n",
      "Epoch 438/3000\n",
      "18/18 - 1s - loss: 1.0225 - val_loss: 1.0519\n",
      "\n",
      "Epoch 00438: val_loss did not improve from 0.96803\n",
      "Epoch 439/3000\n",
      "18/18 - 1s - loss: 1.0747 - val_loss: 1.0619\n",
      "\n",
      "Epoch 00439: val_loss did not improve from 0.96803\n",
      "Epoch 440/3000\n",
      "18/18 - 1s - loss: 1.1163 - val_loss: 1.0340\n",
      "\n",
      "Epoch 00440: val_loss did not improve from 0.96803\n",
      "Epoch 441/3000\n",
      "18/18 - 1s - loss: 1.0551 - val_loss: 1.3371\n",
      "\n",
      "Epoch 00441: val_loss did not improve from 0.96803\n",
      "Epoch 442/3000\n",
      "18/18 - 1s - loss: 0.9943 - val_loss: 1.1833\n",
      "\n",
      "Epoch 00442: val_loss did not improve from 0.96803\n",
      "Epoch 443/3000\n",
      "18/18 - 1s - loss: 1.1963 - val_loss: 1.2929\n",
      "\n",
      "Epoch 00443: val_loss did not improve from 0.96803\n",
      "Epoch 444/3000\n",
      "18/18 - 1s - loss: 1.0369 - val_loss: 1.0859\n",
      "\n",
      "Epoch 00444: val_loss did not improve from 0.96803\n",
      "Epoch 445/3000\n",
      "18/18 - 1s - loss: 1.0400 - val_loss: 1.0322\n",
      "\n",
      "Epoch 00445: val_loss did not improve from 0.96803\n",
      "Epoch 446/3000\n",
      "18/18 - 1s - loss: 1.1104 - val_loss: 1.1283\n",
      "\n",
      "Epoch 00446: val_loss did not improve from 0.96803\n",
      "Epoch 447/3000\n",
      "18/18 - 1s - loss: 1.1808 - val_loss: 1.4552\n",
      "\n",
      "Epoch 00447: val_loss did not improve from 0.96803\n",
      "Epoch 448/3000\n",
      "18/18 - 1s - loss: 1.1215 - val_loss: 1.4232\n",
      "\n",
      "Epoch 00448: val_loss did not improve from 0.96803\n",
      "Epoch 449/3000\n",
      "18/18 - 1s - loss: 1.4852 - val_loss: 1.9487\n",
      "\n",
      "Epoch 00449: val_loss did not improve from 0.96803\n",
      "Epoch 450/3000\n",
      "18/18 - 1s - loss: 1.1956 - val_loss: 1.1146\n",
      "\n",
      "Epoch 00450: val_loss did not improve from 0.96803\n",
      "Epoch 451/3000\n",
      "18/18 - 1s - loss: 1.2694 - val_loss: 1.4707\n",
      "\n",
      "Epoch 00451: val_loss did not improve from 0.96803\n",
      "Epoch 452/3000\n",
      "18/18 - 1s - loss: 1.0516 - val_loss: 1.0460\n",
      "\n",
      "Epoch 00452: val_loss did not improve from 0.96803\n",
      "Epoch 453/3000\n",
      "18/18 - 1s - loss: 1.3374 - val_loss: 1.4240\n",
      "\n",
      "Epoch 00453: val_loss did not improve from 0.96803\n",
      "Epoch 454/3000\n",
      "18/18 - 1s - loss: 1.0748 - val_loss: 1.1316\n",
      "\n",
      "Epoch 00454: val_loss did not improve from 0.96803\n",
      "Epoch 455/3000\n",
      "18/18 - 1s - loss: 1.0771 - val_loss: 1.6443\n",
      "\n",
      "Epoch 00455: val_loss did not improve from 0.96803\n",
      "Epoch 456/3000\n",
      "18/18 - 1s - loss: 1.4640 - val_loss: 1.1405\n",
      "\n",
      "Epoch 00456: val_loss did not improve from 0.96803\n",
      "Epoch 457/3000\n",
      "18/18 - 1s - loss: 1.1298 - val_loss: 1.0611\n",
      "\n",
      "Epoch 00457: val_loss did not improve from 0.96803\n",
      "Epoch 458/3000\n",
      "18/18 - 1s - loss: 1.1982 - val_loss: 1.0641\n",
      "\n",
      "Epoch 00458: val_loss did not improve from 0.96803\n",
      "Epoch 459/3000\n",
      "18/18 - 1s - loss: 1.1164 - val_loss: 1.0494\n",
      "\n",
      "Epoch 00459: val_loss did not improve from 0.96803\n",
      "Epoch 460/3000\n",
      "18/18 - 1s - loss: 1.0452 - val_loss: 1.0543\n",
      "\n",
      "Epoch 00460: val_loss did not improve from 0.96803\n",
      "Epoch 461/3000\n",
      "18/18 - 1s - loss: 0.9967 - val_loss: 1.0271\n",
      "\n",
      "Epoch 00461: val_loss did not improve from 0.96803\n",
      "Epoch 462/3000\n",
      "18/18 - 1s - loss: 1.0038 - val_loss: 1.0061\n",
      "\n",
      "Epoch 00462: val_loss did not improve from 0.96803\n",
      "Epoch 463/3000\n",
      "18/18 - 1s - loss: 1.1494 - val_loss: 1.0555\n",
      "\n",
      "Epoch 00463: val_loss did not improve from 0.96803\n",
      "Epoch 464/3000\n",
      "18/18 - 1s - loss: 1.0265 - val_loss: 1.7277\n",
      "\n",
      "Epoch 00464: val_loss did not improve from 0.96803\n",
      "Epoch 465/3000\n",
      "18/18 - 1s - loss: 1.1125 - val_loss: 1.0001\n",
      "\n",
      "Epoch 00465: val_loss did not improve from 0.96803\n",
      "Epoch 466/3000\n",
      "18/18 - 1s - loss: 1.0944 - val_loss: 1.0116\n",
      "\n",
      "Epoch 00466: val_loss did not improve from 0.96803\n",
      "Epoch 467/3000\n",
      "18/18 - 1s - loss: 0.9864 - val_loss: 0.9790\n",
      "\n",
      "Epoch 00467: val_loss did not improve from 0.96803\n",
      "Epoch 468/3000\n",
      "18/18 - 1s - loss: 1.0207 - val_loss: 1.4982\n",
      "\n",
      "Epoch 00468: val_loss did not improve from 0.96803\n",
      "Epoch 469/3000\n",
      "18/18 - 1s - loss: 1.0258 - val_loss: 0.9840\n",
      "\n",
      "Epoch 00469: val_loss did not improve from 0.96803\n",
      "Epoch 470/3000\n",
      "18/18 - 1s - loss: 1.1106 - val_loss: 1.0038\n",
      "\n",
      "Epoch 00470: val_loss did not improve from 0.96803\n",
      "Epoch 471/3000\n",
      "18/18 - 1s - loss: 1.0641 - val_loss: 0.9660\n",
      "\n",
      "Epoch 00471: val_loss improved from 0.96803 to 0.96595, saving model to qkeras_weights.h5\n",
      "Epoch 472/3000\n",
      "18/18 - 1s - loss: 1.0108 - val_loss: 0.9782\n",
      "\n",
      "Epoch 00472: val_loss did not improve from 0.96595\n",
      "Epoch 473/3000\n",
      "18/18 - 1s - loss: 1.1560 - val_loss: 1.0127\n",
      "\n",
      "Epoch 00473: val_loss did not improve from 0.96595\n",
      "Epoch 474/3000\n",
      "18/18 - 1s - loss: 0.9767 - val_loss: 0.9646\n",
      "\n",
      "Epoch 00474: val_loss improved from 0.96595 to 0.96457, saving model to qkeras_weights.h5\n",
      "Epoch 475/3000\n",
      "18/18 - 1s - loss: 1.0181 - val_loss: 1.0347\n",
      "\n",
      "Epoch 00475: val_loss did not improve from 0.96457\n",
      "Epoch 476/3000\n",
      "18/18 - 1s - loss: 1.0385 - val_loss: 0.9686\n",
      "\n",
      "Epoch 00476: val_loss did not improve from 0.96457\n",
      "Epoch 477/3000\n",
      "18/18 - 1s - loss: 1.3892 - val_loss: 1.1393\n",
      "\n",
      "Epoch 00477: val_loss did not improve from 0.96457\n",
      "Epoch 478/3000\n",
      "18/18 - 1s - loss: 1.2495 - val_loss: 1.6032\n",
      "\n",
      "Epoch 00478: val_loss did not improve from 0.96457\n",
      "Epoch 479/3000\n",
      "18/18 - 1s - loss: 1.1212 - val_loss: 1.0056\n",
      "\n",
      "Epoch 00479: val_loss did not improve from 0.96457\n",
      "Epoch 480/3000\n",
      "18/18 - 1s - loss: 1.0048 - val_loss: 2.0908\n",
      "\n",
      "Epoch 00480: val_loss did not improve from 0.96457\n",
      "Epoch 481/3000\n",
      "18/18 - 1s - loss: 1.3262 - val_loss: 1.0156\n",
      "\n",
      "Epoch 00481: val_loss did not improve from 0.96457\n",
      "Epoch 482/3000\n",
      "18/18 - 1s - loss: 1.0941 - val_loss: 1.1618\n",
      "\n",
      "Epoch 00482: val_loss did not improve from 0.96457\n",
      "Epoch 483/3000\n",
      "18/18 - 1s - loss: 1.2605 - val_loss: 1.2918\n",
      "\n",
      "Epoch 00483: val_loss did not improve from 0.96457\n",
      "Epoch 484/3000\n",
      "18/18 - 1s - loss: 1.0491 - val_loss: 1.3990\n",
      "\n",
      "Epoch 00484: val_loss did not improve from 0.96457\n",
      "Epoch 485/3000\n",
      "18/18 - 1s - loss: 1.0349 - val_loss: 0.9590\n",
      "\n",
      "Epoch 00485: val_loss improved from 0.96457 to 0.95904, saving model to qkeras_weights.h5\n",
      "Epoch 486/3000\n",
      "18/18 - 1s - loss: 1.0616 - val_loss: 0.9686\n",
      "\n",
      "Epoch 00486: val_loss did not improve from 0.95904\n",
      "Epoch 487/3000\n",
      "18/18 - 1s - loss: 1.0521 - val_loss: 0.9781\n",
      "\n",
      "Epoch 00487: val_loss did not improve from 0.95904\n",
      "Epoch 488/3000\n",
      "18/18 - 1s - loss: 1.0554 - val_loss: 1.1192\n",
      "\n",
      "Epoch 00488: val_loss did not improve from 0.95904\n",
      "Epoch 489/3000\n",
      "18/18 - 1s - loss: 0.9937 - val_loss: 1.0027\n",
      "\n",
      "Epoch 00489: val_loss did not improve from 0.95904\n",
      "Epoch 490/3000\n",
      "18/18 - 1s - loss: 1.2457 - val_loss: 1.1859\n",
      "\n",
      "Epoch 00490: val_loss did not improve from 0.95904\n",
      "Epoch 491/3000\n",
      "18/18 - 1s - loss: 1.0529 - val_loss: 1.0023\n",
      "\n",
      "Epoch 00491: val_loss did not improve from 0.95904\n",
      "Epoch 492/3000\n",
      "18/18 - 1s - loss: 1.0433 - val_loss: 0.9756\n",
      "\n",
      "Epoch 00492: val_loss did not improve from 0.95904\n",
      "Epoch 493/3000\n",
      "18/18 - 1s - loss: 1.0353 - val_loss: 1.1516\n",
      "\n",
      "Epoch 00493: val_loss did not improve from 0.95904\n",
      "Epoch 494/3000\n",
      "18/18 - 1s - loss: 1.2371 - val_loss: 1.0021\n",
      "\n",
      "Epoch 00494: val_loss did not improve from 0.95904\n",
      "Epoch 495/3000\n",
      "18/18 - 1s - loss: 1.2237 - val_loss: 0.9647\n",
      "\n",
      "Epoch 00495: val_loss did not improve from 0.95904\n",
      "Epoch 496/3000\n",
      "18/18 - 1s - loss: 1.1737 - val_loss: 2.6795\n",
      "\n",
      "Epoch 00496: val_loss did not improve from 0.95904\n",
      "Epoch 497/3000\n",
      "18/18 - 1s - loss: 1.2246 - val_loss: 0.9723\n",
      "\n",
      "Epoch 00497: val_loss did not improve from 0.95904\n",
      "Epoch 498/3000\n",
      "18/18 - 1s - loss: 0.9571 - val_loss: 0.9663\n",
      "\n",
      "Epoch 00498: val_loss did not improve from 0.95904\n",
      "Epoch 499/3000\n",
      "18/18 - 1s - loss: 1.2148 - val_loss: 0.9751\n",
      "\n",
      "Epoch 00499: val_loss did not improve from 0.95904\n",
      "Epoch 500/3000\n",
      "18/18 - 1s - loss: 0.9916 - val_loss: 0.9938\n",
      "\n",
      "Epoch 00500: val_loss did not improve from 0.95904\n",
      "Epoch 501/3000\n",
      "18/18 - 1s - loss: 1.2150 - val_loss: 1.0407\n",
      "\n",
      "Epoch 00501: val_loss did not improve from 0.95904\n",
      "Epoch 502/3000\n",
      "18/18 - 1s - loss: 1.0630 - val_loss: 1.0508\n",
      "\n",
      "Epoch 00502: val_loss did not improve from 0.95904\n",
      "Epoch 503/3000\n",
      "18/18 - 1s - loss: 1.1316 - val_loss: 1.0482\n",
      "\n",
      "Epoch 00503: val_loss did not improve from 0.95904\n",
      "Epoch 504/3000\n",
      "18/18 - 1s - loss: 1.2340 - val_loss: 1.0692\n",
      "\n",
      "Epoch 00504: val_loss did not improve from 0.95904\n",
      "Epoch 505/3000\n",
      "18/18 - 1s - loss: 1.4720 - val_loss: 0.9751\n",
      "\n",
      "Epoch 00505: val_loss did not improve from 0.95904\n",
      "Epoch 506/3000\n",
      "18/18 - 1s - loss: 1.2420 - val_loss: 0.9608\n",
      "\n",
      "Epoch 00506: val_loss did not improve from 0.95904\n",
      "Epoch 507/3000\n",
      "18/18 - 1s - loss: 1.0916 - val_loss: 0.9664\n",
      "\n",
      "Epoch 00507: val_loss did not improve from 0.95904\n",
      "Epoch 508/3000\n",
      "18/18 - 1s - loss: 0.9878 - val_loss: 0.9590\n",
      "\n",
      "Epoch 00508: val_loss improved from 0.95904 to 0.95896, saving model to qkeras_weights.h5\n",
      "Epoch 509/3000\n",
      "18/18 - 1s - loss: 1.1439 - val_loss: 0.9837\n",
      "\n",
      "Epoch 00509: val_loss did not improve from 0.95896\n",
      "Epoch 510/3000\n",
      "18/18 - 1s - loss: 0.9763 - val_loss: 1.0035\n",
      "\n",
      "Epoch 00510: val_loss did not improve from 0.95896\n",
      "Epoch 511/3000\n",
      "18/18 - 1s - loss: 1.0016 - val_loss: 1.4807\n",
      "\n",
      "Epoch 00511: val_loss did not improve from 0.95896\n",
      "Epoch 512/3000\n",
      "18/18 - 1s - loss: 1.0091 - val_loss: 0.9938\n",
      "\n",
      "Epoch 00512: val_loss did not improve from 0.95896\n",
      "Epoch 513/3000\n",
      "18/18 - 1s - loss: 0.9830 - val_loss: 1.5363\n",
      "\n",
      "Epoch 00513: val_loss did not improve from 0.95896\n",
      "Epoch 514/3000\n",
      "18/18 - 1s - loss: 1.2271 - val_loss: 0.9657\n",
      "\n",
      "Epoch 00514: val_loss did not improve from 0.95896\n",
      "Epoch 515/3000\n",
      "18/18 - 1s - loss: 1.0892 - val_loss: 0.9970\n",
      "\n",
      "Epoch 00515: val_loss did not improve from 0.95896\n",
      "Epoch 516/3000\n",
      "18/18 - 1s - loss: 1.3200 - val_loss: 1.1168\n",
      "\n",
      "Epoch 00516: val_loss did not improve from 0.95896\n",
      "Epoch 517/3000\n",
      "18/18 - 1s - loss: 1.0938 - val_loss: 1.0248\n",
      "\n",
      "Epoch 00517: val_loss did not improve from 0.95896\n",
      "Epoch 518/3000\n",
      "18/18 - 1s - loss: 0.9878 - val_loss: 0.9849\n",
      "\n",
      "Epoch 00518: val_loss did not improve from 0.95896\n",
      "Epoch 519/3000\n",
      "18/18 - 1s - loss: 1.1561 - val_loss: 0.9725\n",
      "\n",
      "Epoch 00519: val_loss did not improve from 0.95896\n",
      "Epoch 520/3000\n",
      "18/18 - 1s - loss: 1.0914 - val_loss: 1.1391\n",
      "\n",
      "Epoch 00520: val_loss did not improve from 0.95896\n",
      "Epoch 521/3000\n",
      "18/18 - 1s - loss: 1.1915 - val_loss: 0.9606\n",
      "\n",
      "Epoch 00521: val_loss did not improve from 0.95896\n",
      "Epoch 522/3000\n",
      "18/18 - 1s - loss: 0.9851 - val_loss: 1.4382\n",
      "\n",
      "Epoch 00522: val_loss did not improve from 0.95896\n",
      "Epoch 523/3000\n",
      "18/18 - 1s - loss: 1.2793 - val_loss: 1.6596\n",
      "\n",
      "Epoch 00523: val_loss did not improve from 0.95896\n",
      "Epoch 524/3000\n",
      "18/18 - 1s - loss: 1.0349 - val_loss: 1.1116\n",
      "\n",
      "Epoch 00524: val_loss did not improve from 0.95896\n",
      "Epoch 525/3000\n",
      "18/18 - 1s - loss: 1.1218 - val_loss: 0.9637\n",
      "\n",
      "Epoch 00525: val_loss did not improve from 0.95896\n",
      "Epoch 526/3000\n",
      "18/18 - 1s - loss: 1.0222 - val_loss: 0.9465\n",
      "\n",
      "Epoch 00526: val_loss improved from 0.95896 to 0.94649, saving model to qkeras_weights.h5\n",
      "Epoch 527/3000\n",
      "18/18 - 1s - loss: 1.3597 - val_loss: 1.0177\n",
      "\n",
      "Epoch 00527: val_loss did not improve from 0.94649\n",
      "Epoch 528/3000\n",
      "18/18 - 1s - loss: 1.0080 - val_loss: 0.9664\n",
      "\n",
      "Epoch 00528: val_loss did not improve from 0.94649\n",
      "Epoch 529/3000\n",
      "18/18 - 1s - loss: 1.0267 - val_loss: 1.4497\n",
      "\n",
      "Epoch 00529: val_loss did not improve from 0.94649\n",
      "Epoch 530/3000\n",
      "18/18 - 1s - loss: 1.1823 - val_loss: 1.4003\n",
      "\n",
      "Epoch 00530: val_loss did not improve from 0.94649\n",
      "Epoch 531/3000\n",
      "18/18 - 1s - loss: 1.2830 - val_loss: 1.6656\n",
      "\n",
      "Epoch 00531: val_loss did not improve from 0.94649\n",
      "Epoch 532/3000\n",
      "18/18 - 1s - loss: 1.2562 - val_loss: 0.9880\n",
      "\n",
      "Epoch 00532: val_loss did not improve from 0.94649\n",
      "Epoch 533/3000\n",
      "18/18 - 1s - loss: 1.2250 - val_loss: 1.1300\n",
      "\n",
      "Epoch 00533: val_loss did not improve from 0.94649\n",
      "Epoch 534/3000\n",
      "18/18 - 1s - loss: 1.2073 - val_loss: 0.9773\n",
      "\n",
      "Epoch 00534: val_loss did not improve from 0.94649\n",
      "Epoch 535/3000\n",
      "18/18 - 1s - loss: 1.4374 - val_loss: 0.9632\n",
      "\n",
      "Epoch 00535: val_loss did not improve from 0.94649\n",
      "Epoch 536/3000\n",
      "18/18 - 1s - loss: 1.3391 - val_loss: 0.9770\n",
      "\n",
      "Epoch 00536: val_loss did not improve from 0.94649\n",
      "Epoch 537/3000\n",
      "18/18 - 1s - loss: 0.9429 - val_loss: 0.9410\n",
      "\n",
      "Epoch 00537: val_loss improved from 0.94649 to 0.94101, saving model to qkeras_weights.h5\n",
      "Epoch 538/3000\n",
      "18/18 - 1s - loss: 1.0351 - val_loss: 0.9506\n",
      "\n",
      "Epoch 00538: val_loss did not improve from 0.94101\n",
      "Epoch 539/3000\n",
      "18/18 - 1s - loss: 1.1290 - val_loss: 1.0109\n",
      "\n",
      "Epoch 00539: val_loss did not improve from 0.94101\n",
      "Epoch 540/3000\n",
      "18/18 - 1s - loss: 0.9971 - val_loss: 1.0045\n",
      "\n",
      "Epoch 00540: val_loss did not improve from 0.94101\n",
      "Epoch 541/3000\n",
      "18/18 - 1s - loss: 0.9693 - val_loss: 1.0370\n",
      "\n",
      "Epoch 00541: val_loss did not improve from 0.94101\n",
      "Epoch 542/3000\n",
      "18/18 - 1s - loss: 1.0676 - val_loss: 1.0308\n",
      "\n",
      "Epoch 00542: val_loss did not improve from 0.94101\n",
      "Epoch 543/3000\n",
      "18/18 - 1s - loss: 1.2723 - val_loss: 1.3337\n",
      "\n",
      "Epoch 00543: val_loss did not improve from 0.94101\n",
      "Epoch 544/3000\n",
      "18/18 - 1s - loss: 1.1597 - val_loss: 2.7036\n",
      "\n",
      "Epoch 00544: val_loss did not improve from 0.94101\n",
      "Epoch 545/3000\n",
      "18/18 - 1s - loss: 1.1440 - val_loss: 1.1368\n",
      "\n",
      "Epoch 00545: val_loss did not improve from 0.94101\n",
      "Epoch 546/3000\n",
      "18/18 - 1s - loss: 1.4966 - val_loss: 0.9760\n",
      "\n",
      "Epoch 00546: val_loss did not improve from 0.94101\n",
      "Epoch 547/3000\n",
      "18/18 - 1s - loss: 1.2932 - val_loss: 0.9674\n",
      "\n",
      "Epoch 00547: val_loss did not improve from 0.94101\n",
      "Epoch 548/3000\n",
      "18/18 - 1s - loss: 0.9673 - val_loss: 0.9582\n",
      "\n",
      "Epoch 00548: val_loss did not improve from 0.94101\n",
      "Epoch 549/3000\n",
      "18/18 - 1s - loss: 0.9379 - val_loss: 1.6629\n",
      "\n",
      "Epoch 00549: val_loss did not improve from 0.94101\n",
      "Epoch 550/3000\n",
      "18/18 - 1s - loss: 1.1689 - val_loss: 1.0459\n",
      "\n",
      "Epoch 00550: val_loss did not improve from 0.94101\n",
      "Epoch 551/3000\n",
      "18/18 - 1s - loss: 0.9987 - val_loss: 1.6508\n",
      "\n",
      "Epoch 00551: val_loss did not improve from 0.94101\n",
      "Epoch 552/3000\n",
      "18/18 - 1s - loss: 1.0359 - val_loss: 0.9675\n",
      "\n",
      "Epoch 00552: val_loss did not improve from 0.94101\n",
      "Epoch 553/3000\n",
      "18/18 - 1s - loss: 1.0465 - val_loss: 1.0482\n",
      "\n",
      "Epoch 00553: val_loss did not improve from 0.94101\n",
      "Epoch 554/3000\n",
      "18/18 - 1s - loss: 1.0952 - val_loss: 1.0082\n",
      "\n",
      "Epoch 00554: val_loss did not improve from 0.94101\n",
      "Epoch 555/3000\n",
      "18/18 - 1s - loss: 0.9873 - val_loss: 1.5317\n",
      "\n",
      "Epoch 00555: val_loss did not improve from 0.94101\n",
      "Epoch 556/3000\n",
      "18/18 - 1s - loss: 0.9950 - val_loss: 1.5515\n",
      "\n",
      "Epoch 00556: val_loss did not improve from 0.94101\n",
      "Epoch 557/3000\n",
      "18/18 - 1s - loss: 1.1974 - val_loss: 1.0349\n",
      "\n",
      "Epoch 00557: val_loss did not improve from 0.94101\n",
      "Epoch 558/3000\n",
      "18/18 - 1s - loss: 1.1925 - val_loss: 0.9995\n",
      "\n",
      "Epoch 00558: val_loss did not improve from 0.94101\n",
      "Epoch 559/3000\n",
      "18/18 - 1s - loss: 0.9884 - val_loss: 0.9837\n",
      "\n",
      "Epoch 00559: val_loss did not improve from 0.94101\n",
      "Epoch 560/3000\n",
      "18/18 - 1s - loss: 1.0100 - val_loss: 1.0116\n",
      "\n",
      "Epoch 00560: val_loss did not improve from 0.94101\n",
      "Epoch 561/3000\n",
      "18/18 - 1s - loss: 1.0908 - val_loss: 1.0166\n",
      "\n",
      "Epoch 00561: val_loss did not improve from 0.94101\n",
      "Epoch 562/3000\n",
      "18/18 - 1s - loss: 1.0127 - val_loss: 1.0219\n",
      "\n",
      "Epoch 00562: val_loss did not improve from 0.94101\n",
      "Epoch 563/3000\n",
      "18/18 - 1s - loss: 1.0560 - val_loss: 0.9882\n",
      "\n",
      "Epoch 00563: val_loss did not improve from 0.94101\n",
      "Epoch 564/3000\n",
      "18/18 - 1s - loss: 1.0087 - val_loss: 0.9879\n",
      "\n",
      "Epoch 00564: val_loss did not improve from 0.94101\n",
      "Epoch 565/3000\n",
      "18/18 - 1s - loss: 1.0712 - val_loss: 1.0166\n",
      "\n",
      "Epoch 00565: val_loss did not improve from 0.94101\n",
      "Epoch 566/3000\n",
      "18/18 - 1s - loss: 0.9771 - val_loss: 0.9740\n",
      "\n",
      "Epoch 00566: val_loss did not improve from 0.94101\n",
      "Epoch 567/3000\n",
      "18/18 - 1s - loss: 0.9948 - val_loss: 0.9788\n",
      "\n",
      "Epoch 00567: val_loss did not improve from 0.94101\n",
      "Epoch 568/3000\n",
      "18/18 - 1s - loss: 1.1150 - val_loss: 1.3827\n",
      "\n",
      "Epoch 00568: val_loss did not improve from 0.94101\n",
      "Epoch 569/3000\n",
      "18/18 - 1s - loss: 1.0538 - val_loss: 1.0227\n",
      "\n",
      "Epoch 00569: val_loss did not improve from 0.94101\n",
      "Epoch 570/3000\n",
      "18/18 - 1s - loss: 0.9920 - val_loss: 0.9869\n",
      "\n",
      "Epoch 00570: val_loss did not improve from 0.94101\n",
      "Epoch 571/3000\n",
      "18/18 - 1s - loss: 1.0693 - val_loss: 0.9841\n",
      "\n",
      "Epoch 00571: val_loss did not improve from 0.94101\n",
      "Epoch 572/3000\n",
      "18/18 - 1s - loss: 1.0506 - val_loss: 1.2500\n",
      "\n",
      "Epoch 00572: val_loss did not improve from 0.94101\n",
      "Epoch 573/3000\n",
      "18/18 - 1s - loss: 1.0023 - val_loss: 1.0124\n",
      "\n",
      "Epoch 00573: val_loss did not improve from 0.94101\n",
      "Epoch 574/3000\n",
      "18/18 - 1s - loss: 1.0184 - val_loss: 0.9697\n",
      "\n",
      "Epoch 00574: val_loss did not improve from 0.94101\n",
      "Epoch 575/3000\n",
      "18/18 - 1s - loss: 1.1564 - val_loss: 1.0452\n",
      "\n",
      "Epoch 00575: val_loss did not improve from 0.94101\n",
      "Epoch 576/3000\n",
      "18/18 - 1s - loss: 0.9892 - val_loss: 0.9649\n",
      "\n",
      "Epoch 00576: val_loss did not improve from 0.94101\n",
      "Epoch 577/3000\n",
      "18/18 - 1s - loss: 1.0913 - val_loss: 1.0069\n",
      "\n",
      "Epoch 00577: val_loss did not improve from 0.94101\n",
      "Epoch 578/3000\n",
      "18/18 - 1s - loss: 0.9740 - val_loss: 0.9686\n",
      "\n",
      "Epoch 00578: val_loss did not improve from 0.94101\n",
      "Epoch 579/3000\n",
      "18/18 - 1s - loss: 1.0216 - val_loss: 1.0123\n",
      "\n",
      "Epoch 00579: val_loss did not improve from 0.94101\n",
      "Epoch 580/3000\n",
      "18/18 - 1s - loss: 0.9989 - val_loss: 1.0145\n",
      "\n",
      "Epoch 00580: val_loss did not improve from 0.94101\n",
      "Epoch 581/3000\n",
      "18/18 - 1s - loss: 1.0155 - val_loss: 1.0246\n",
      "\n",
      "Epoch 00581: val_loss did not improve from 0.94101\n",
      "Epoch 582/3000\n",
      "18/18 - 1s - loss: 1.1177 - val_loss: 1.0503\n",
      "\n",
      "Epoch 00582: val_loss did not improve from 0.94101\n",
      "Epoch 583/3000\n",
      "18/18 - 1s - loss: 1.0130 - val_loss: 1.0356\n",
      "\n",
      "Epoch 00583: val_loss did not improve from 0.94101\n",
      "Epoch 584/3000\n",
      "18/18 - 1s - loss: 1.0473 - val_loss: 1.1546\n",
      "\n",
      "Epoch 00584: val_loss did not improve from 0.94101\n",
      "Epoch 585/3000\n",
      "18/18 - 1s - loss: 0.9950 - val_loss: 0.9772\n",
      "\n",
      "Epoch 00585: val_loss did not improve from 0.94101\n",
      "Epoch 586/3000\n",
      "18/18 - 1s - loss: 1.0866 - val_loss: 1.0492\n",
      "\n",
      "Epoch 00586: val_loss did not improve from 0.94101\n",
      "Epoch 587/3000\n",
      "18/18 - 1s - loss: 0.9989 - val_loss: 1.0138\n",
      "\n",
      "Epoch 00587: val_loss did not improve from 0.94101\n",
      "Epoch 588/3000\n",
      "18/18 - 1s - loss: 1.1338 - val_loss: 1.0262\n",
      "\n",
      "Epoch 00588: val_loss did not improve from 0.94101\n",
      "Epoch 589/3000\n",
      "18/18 - 1s - loss: 0.9798 - val_loss: 0.9942\n",
      "\n",
      "Epoch 00589: val_loss did not improve from 0.94101\n",
      "Epoch 590/3000\n",
      "18/18 - 1s - loss: 0.9913 - val_loss: 1.6554\n",
      "\n",
      "Epoch 00590: val_loss did not improve from 0.94101\n",
      "Epoch 591/3000\n",
      "18/18 - 1s - loss: 1.0573 - val_loss: 1.3255\n",
      "\n",
      "Epoch 00591: val_loss did not improve from 0.94101\n",
      "Epoch 592/3000\n",
      "18/18 - 1s - loss: 1.1077 - val_loss: 1.2908\n",
      "\n",
      "Epoch 00592: val_loss did not improve from 0.94101\n",
      "Epoch 593/3000\n",
      "18/18 - 1s - loss: 1.0200 - val_loss: 1.1869\n",
      "\n",
      "Epoch 00593: val_loss did not improve from 0.94101\n",
      "Epoch 594/3000\n",
      "18/18 - 1s - loss: 1.0427 - val_loss: 1.2992\n",
      "\n",
      "Epoch 00594: val_loss did not improve from 0.94101\n",
      "Epoch 595/3000\n",
      "18/18 - 1s - loss: 1.0555 - val_loss: 1.0356\n",
      "\n",
      "Epoch 00595: val_loss did not improve from 0.94101\n",
      "Epoch 596/3000\n",
      "18/18 - 1s - loss: 1.1324 - val_loss: 1.0595\n",
      "\n",
      "Epoch 00596: val_loss did not improve from 0.94101\n",
      "Epoch 597/3000\n",
      "18/18 - 1s - loss: 1.0137 - val_loss: 1.0291\n",
      "\n",
      "Epoch 00597: val_loss did not improve from 0.94101\n",
      "Epoch 598/3000\n",
      "18/18 - 1s - loss: 0.9964 - val_loss: 1.0400\n",
      "\n",
      "Epoch 00598: val_loss did not improve from 0.94101\n",
      "Epoch 599/3000\n",
      "18/18 - 1s - loss: 1.0053 - val_loss: 1.0357\n",
      "\n",
      "Epoch 00599: val_loss did not improve from 0.94101\n",
      "Epoch 600/3000\n",
      "18/18 - 1s - loss: 0.9811 - val_loss: 0.9920\n",
      "\n",
      "Epoch 00600: val_loss did not improve from 0.94101\n",
      "Epoch 601/3000\n",
      "18/18 - 1s - loss: 1.0136 - val_loss: 0.9707\n",
      "\n",
      "Epoch 00601: val_loss did not improve from 0.94101\n",
      "Epoch 602/3000\n",
      "18/18 - 1s - loss: 1.2037 - val_loss: 1.0030\n",
      "\n",
      "Epoch 00602: val_loss did not improve from 0.94101\n",
      "Epoch 603/3000\n",
      "18/18 - 1s - loss: 0.9732 - val_loss: 0.9604\n",
      "\n",
      "Epoch 00603: val_loss did not improve from 0.94101\n",
      "Epoch 604/3000\n",
      "18/18 - 1s - loss: 1.0228 - val_loss: 1.0355\n",
      "\n",
      "Epoch 00604: val_loss did not improve from 0.94101\n",
      "Epoch 605/3000\n",
      "18/18 - 1s - loss: 0.9704 - val_loss: 1.0297\n",
      "\n",
      "Epoch 00605: val_loss did not improve from 0.94101\n",
      "Epoch 606/3000\n",
      "18/18 - 1s - loss: 0.9948 - val_loss: 1.5121\n",
      "\n",
      "Epoch 00606: val_loss did not improve from 0.94101\n",
      "Epoch 607/3000\n",
      "18/18 - 1s - loss: 1.1206 - val_loss: 1.0131\n",
      "\n",
      "Epoch 00607: val_loss did not improve from 0.94101\n",
      "Epoch 608/3000\n",
      "18/18 - 1s - loss: 0.9660 - val_loss: 1.4296\n",
      "\n",
      "Epoch 00608: val_loss did not improve from 0.94101\n",
      "Epoch 609/3000\n",
      "18/18 - 1s - loss: 1.0226 - val_loss: 1.5076\n",
      "\n",
      "Epoch 00609: val_loss did not improve from 0.94101\n",
      "Epoch 610/3000\n",
      "18/18 - 1s - loss: 1.1024 - val_loss: 1.3407\n",
      "\n",
      "Epoch 00610: val_loss did not improve from 0.94101\n",
      "Epoch 611/3000\n",
      "18/18 - 1s - loss: 1.0590 - val_loss: 1.1367\n",
      "\n",
      "Epoch 00611: val_loss did not improve from 0.94101\n",
      "Epoch 612/3000\n",
      "18/18 - 1s - loss: 0.9669 - val_loss: 1.3405\n",
      "\n",
      "Epoch 00612: val_loss did not improve from 0.94101\n",
      "Epoch 613/3000\n",
      "18/18 - 1s - loss: 1.1252 - val_loss: 1.0884\n",
      "\n",
      "Epoch 00613: val_loss did not improve from 0.94101\n",
      "Epoch 614/3000\n",
      "18/18 - 1s - loss: 1.0590 - val_loss: 1.1855\n",
      "\n",
      "Epoch 00614: val_loss did not improve from 0.94101\n",
      "Epoch 615/3000\n",
      "18/18 - 1s - loss: 0.9532 - val_loss: 0.9749\n",
      "\n",
      "Epoch 00615: val_loss did not improve from 0.94101\n",
      "Epoch 616/3000\n",
      "18/18 - 1s - loss: 1.0071 - val_loss: 1.1091\n",
      "\n",
      "Epoch 00616: val_loss did not improve from 0.94101\n",
      "Epoch 617/3000\n",
      "18/18 - 1s - loss: 1.0603 - val_loss: 1.2689\n",
      "\n",
      "Epoch 00617: val_loss did not improve from 0.94101\n",
      "Epoch 618/3000\n",
      "18/18 - 1s - loss: 1.0154 - val_loss: 0.9943\n",
      "\n",
      "Epoch 00618: val_loss did not improve from 0.94101\n",
      "Epoch 619/3000\n",
      "18/18 - 1s - loss: 1.1213 - val_loss: 1.0223\n",
      "\n",
      "Epoch 00619: val_loss did not improve from 0.94101\n",
      "Epoch 620/3000\n",
      "18/18 - 1s - loss: 1.0086 - val_loss: 0.9853\n",
      "\n",
      "Epoch 00620: val_loss did not improve from 0.94101\n",
      "Epoch 621/3000\n",
      "18/18 - 1s - loss: 1.0945 - val_loss: 0.9889\n",
      "\n",
      "Epoch 00621: val_loss did not improve from 0.94101\n",
      "Epoch 622/3000\n",
      "18/18 - 1s - loss: 1.2923 - val_loss: 1.2239\n",
      "\n",
      "Epoch 00622: val_loss did not improve from 0.94101\n",
      "Epoch 623/3000\n",
      "18/18 - 1s - loss: 1.3573 - val_loss: 0.9749\n",
      "\n",
      "Epoch 00623: val_loss did not improve from 0.94101\n",
      "Epoch 624/3000\n",
      "18/18 - 1s - loss: 0.9562 - val_loss: 1.0868\n",
      "\n",
      "Epoch 00624: val_loss did not improve from 0.94101\n",
      "Epoch 625/3000\n",
      "18/18 - 1s - loss: 1.0671 - val_loss: 1.2604\n",
      "\n",
      "Epoch 00625: val_loss did not improve from 0.94101\n",
      "Epoch 626/3000\n",
      "18/18 - 1s - loss: 1.0713 - val_loss: 0.9708\n",
      "\n",
      "Epoch 00626: val_loss did not improve from 0.94101\n",
      "Epoch 627/3000\n",
      "18/18 - 1s - loss: 1.0044 - val_loss: 1.5473\n",
      "\n",
      "Epoch 00627: val_loss did not improve from 0.94101\n",
      "Epoch 628/3000\n",
      "18/18 - 1s - loss: 1.0660 - val_loss: 0.9627\n",
      "\n",
      "Epoch 00628: val_loss did not improve from 0.94101\n",
      "Epoch 629/3000\n",
      "18/18 - 1s - loss: 0.9562 - val_loss: 1.0548\n",
      "\n",
      "Epoch 00629: val_loss did not improve from 0.94101\n",
      "Epoch 630/3000\n",
      "18/18 - 1s - loss: 1.1736 - val_loss: 2.6180\n",
      "\n",
      "Epoch 00630: val_loss did not improve from 0.94101\n",
      "Epoch 631/3000\n",
      "18/18 - 1s - loss: 1.1943 - val_loss: 1.5754\n",
      "\n",
      "Epoch 00631: val_loss did not improve from 0.94101\n",
      "Epoch 632/3000\n",
      "18/18 - 1s - loss: 1.0077 - val_loss: 1.0532\n",
      "\n",
      "Epoch 00632: val_loss did not improve from 0.94101\n",
      "Epoch 633/3000\n",
      "18/18 - 1s - loss: 1.1034 - val_loss: 2.9794\n",
      "\n",
      "Epoch 00633: val_loss did not improve from 0.94101\n",
      "Epoch 634/3000\n",
      "18/18 - 1s - loss: 1.1700 - val_loss: 0.9495\n",
      "\n",
      "Epoch 00634: val_loss did not improve from 0.94101\n",
      "Epoch 635/3000\n",
      "18/18 - 1s - loss: 1.0628 - val_loss: 0.9530\n",
      "\n",
      "Epoch 00635: val_loss did not improve from 0.94101\n",
      "Epoch 636/3000\n",
      "18/18 - 1s - loss: 1.1204 - val_loss: 0.9827\n",
      "\n",
      "Epoch 00636: val_loss did not improve from 0.94101\n",
      "Epoch 637/3000\n",
      "18/18 - 1s - loss: 1.2655 - val_loss: 1.0239\n",
      "\n",
      "Epoch 00637: val_loss did not improve from 0.94101\n",
      "Epoch 638/3000\n",
      "18/18 - 1s - loss: 1.0255 - val_loss: 1.0993\n",
      "\n",
      "Epoch 00638: val_loss did not improve from 0.94101\n",
      "Epoch 639/3000\n",
      "18/18 - 1s - loss: 1.3618 - val_loss: 1.4067\n",
      "\n",
      "Epoch 00639: val_loss did not improve from 0.94101\n",
      "Epoch 640/3000\n",
      "18/18 - 1s - loss: 1.0475 - val_loss: 1.3778\n",
      "\n",
      "Epoch 00640: val_loss did not improve from 0.94101\n",
      "Epoch 641/3000\n",
      "18/18 - 1s - loss: 0.9923 - val_loss: 1.6699\n",
      "\n",
      "Epoch 00641: val_loss did not improve from 0.94101\n",
      "Epoch 642/3000\n",
      "18/18 - 1s - loss: 1.0797 - val_loss: 1.0331\n",
      "\n",
      "Epoch 00642: val_loss did not improve from 0.94101\n",
      "Epoch 643/3000\n",
      "18/18 - 1s - loss: 1.2140 - val_loss: 1.0092\n",
      "\n",
      "Epoch 00643: val_loss did not improve from 0.94101\n",
      "Epoch 644/3000\n",
      "18/18 - 1s - loss: 1.1187 - val_loss: 1.0925\n",
      "\n",
      "Epoch 00644: val_loss did not improve from 0.94101\n",
      "Epoch 645/3000\n",
      "18/18 - 1s - loss: 1.2999 - val_loss: 1.2575\n",
      "\n",
      "Epoch 00645: val_loss did not improve from 0.94101\n",
      "Epoch 646/3000\n",
      "18/18 - 1s - loss: 0.9845 - val_loss: 0.9566\n",
      "\n",
      "Epoch 00646: val_loss did not improve from 0.94101\n",
      "Epoch 647/3000\n",
      "18/18 - 1s - loss: 1.0165 - val_loss: 0.9595\n",
      "\n",
      "Epoch 00647: val_loss did not improve from 0.94101\n",
      "Epoch 648/3000\n",
      "18/18 - 1s - loss: 0.9700 - val_loss: 0.9485\n",
      "\n",
      "Epoch 00648: val_loss did not improve from 0.94101\n",
      "Epoch 649/3000\n",
      "18/18 - 1s - loss: 1.4237 - val_loss: 1.1725\n",
      "\n",
      "Epoch 00649: val_loss did not improve from 0.94101\n",
      "Epoch 650/3000\n",
      "18/18 - 1s - loss: 0.9845 - val_loss: 1.0016\n",
      "\n",
      "Epoch 00650: val_loss did not improve from 0.94101\n",
      "Epoch 651/3000\n",
      "18/18 - 1s - loss: 1.0394 - val_loss: 0.9731\n",
      "\n",
      "Epoch 00651: val_loss did not improve from 0.94101\n",
      "Epoch 652/3000\n",
      "18/18 - 1s - loss: 1.1169 - val_loss: 1.0572\n",
      "\n",
      "Epoch 00652: val_loss did not improve from 0.94101\n",
      "Epoch 653/3000\n",
      "18/18 - 1s - loss: 1.0507 - val_loss: 1.2152\n",
      "\n",
      "Epoch 00653: val_loss did not improve from 0.94101\n",
      "Epoch 654/3000\n",
      "18/18 - 1s - loss: 1.0618 - val_loss: 1.0462\n",
      "\n",
      "Epoch 00654: val_loss did not improve from 0.94101\n",
      "Epoch 655/3000\n",
      "18/18 - 1s - loss: 1.4196 - val_loss: 1.1343\n",
      "\n",
      "Epoch 00655: val_loss did not improve from 0.94101\n",
      "Epoch 656/3000\n",
      "18/18 - 1s - loss: 0.9864 - val_loss: 0.9538\n",
      "\n",
      "Epoch 00656: val_loss did not improve from 0.94101\n",
      "Epoch 657/3000\n",
      "18/18 - 1s - loss: 1.0178 - val_loss: 0.9470\n",
      "\n",
      "Epoch 00657: val_loss did not improve from 0.94101\n",
      "Epoch 658/3000\n",
      "18/18 - 1s - loss: 1.0851 - val_loss: 0.9603\n",
      "\n",
      "Epoch 00658: val_loss did not improve from 0.94101\n",
      "Epoch 659/3000\n",
      "18/18 - 1s - loss: 1.1894 - val_loss: 0.9451\n",
      "\n",
      "Epoch 00659: val_loss did not improve from 0.94101\n",
      "Epoch 660/3000\n",
      "18/18 - 1s - loss: 1.0337 - val_loss: 2.0431\n",
      "\n",
      "Epoch 00660: val_loss did not improve from 0.94101\n",
      "Epoch 661/3000\n",
      "18/18 - 1s - loss: 1.1228 - val_loss: 1.0365\n",
      "\n",
      "Epoch 00661: val_loss did not improve from 0.94101\n",
      "Epoch 662/3000\n",
      "18/18 - 1s - loss: 1.2131 - val_loss: 1.5072\n",
      "\n",
      "Epoch 00662: val_loss did not improve from 0.94101\n",
      "Epoch 663/3000\n",
      "18/18 - 1s - loss: 1.1313 - val_loss: 2.7846\n",
      "\n",
      "Epoch 00663: val_loss did not improve from 0.94101\n",
      "Epoch 664/3000\n",
      "18/18 - 1s - loss: 1.1279 - val_loss: 1.5562\n",
      "\n",
      "Epoch 00664: val_loss did not improve from 0.94101\n",
      "Epoch 665/3000\n",
      "18/18 - 1s - loss: 1.6441 - val_loss: 1.4740\n",
      "\n",
      "Epoch 00665: val_loss did not improve from 0.94101\n",
      "Epoch 666/3000\n",
      "18/18 - 1s - loss: 1.4924 - val_loss: 1.2404\n",
      "\n",
      "Epoch 00666: val_loss did not improve from 0.94101\n",
      "Epoch 667/3000\n",
      "18/18 - 1s - loss: 1.4419 - val_loss: 1.0123\n",
      "\n",
      "Epoch 00667: val_loss did not improve from 0.94101\n",
      "Epoch 668/3000\n",
      "18/18 - 1s - loss: 1.1683 - val_loss: 0.9683\n",
      "\n",
      "Epoch 00668: val_loss did not improve from 0.94101\n",
      "Epoch 669/3000\n",
      "18/18 - 1s - loss: 0.9909 - val_loss: 0.9976\n",
      "\n",
      "Epoch 00669: val_loss did not improve from 0.94101\n",
      "Epoch 670/3000\n",
      "18/18 - 1s - loss: 1.1069 - val_loss: 0.9568\n",
      "\n",
      "Epoch 00670: val_loss did not improve from 0.94101\n",
      "Epoch 671/3000\n",
      "18/18 - 1s - loss: 1.0798 - val_loss: 0.9835\n",
      "\n",
      "Epoch 00671: val_loss did not improve from 0.94101\n",
      "Epoch 672/3000\n",
      "18/18 - 1s - loss: 1.0711 - val_loss: 2.7294\n",
      "\n",
      "Epoch 00672: val_loss did not improve from 0.94101\n",
      "Epoch 673/3000\n",
      "18/18 - 1s - loss: 1.4934 - val_loss: 1.0483\n",
      "\n",
      "Epoch 00673: val_loss did not improve from 0.94101\n",
      "Epoch 674/3000\n",
      "18/18 - 1s - loss: 1.1246 - val_loss: 0.9566\n",
      "\n",
      "Epoch 00674: val_loss did not improve from 0.94101\n",
      "Epoch 675/3000\n",
      "18/18 - 1s - loss: 0.9636 - val_loss: 0.9644\n",
      "\n",
      "Epoch 00675: val_loss did not improve from 0.94101\n",
      "Epoch 676/3000\n",
      "18/18 - 1s - loss: 0.9602 - val_loss: 3.1773\n",
      "\n",
      "Epoch 00676: val_loss did not improve from 0.94101\n",
      "Epoch 677/3000\n",
      "18/18 - 1s - loss: 1.1402 - val_loss: 0.9717\n",
      "\n",
      "Epoch 00677: val_loss did not improve from 0.94101\n",
      "Epoch 678/3000\n",
      "18/18 - 1s - loss: 1.2293 - val_loss: 0.9830\n",
      "\n",
      "Epoch 00678: val_loss did not improve from 0.94101\n",
      "Epoch 679/3000\n",
      "18/18 - 1s - loss: 1.1592 - val_loss: 0.9828\n",
      "\n",
      "Epoch 00679: val_loss did not improve from 0.94101\n",
      "Epoch 680/3000\n",
      "18/18 - 1s - loss: 1.2709 - val_loss: 0.9854\n",
      "\n",
      "Epoch 00680: val_loss did not improve from 0.94101\n",
      "Epoch 681/3000\n",
      "18/18 - 1s - loss: 1.0363 - val_loss: 0.9705\n",
      "\n",
      "Epoch 00681: val_loss did not improve from 0.94101\n",
      "Epoch 682/3000\n",
      "18/18 - 1s - loss: 1.0932 - val_loss: 1.0712\n",
      "\n",
      "Epoch 00682: val_loss did not improve from 0.94101\n",
      "Epoch 683/3000\n",
      "18/18 - 1s - loss: 1.0248 - val_loss: 0.9895\n",
      "\n",
      "Epoch 00683: val_loss did not improve from 0.94101\n",
      "Epoch 684/3000\n",
      "18/18 - 1s - loss: 1.0541 - val_loss: 1.2958\n",
      "\n",
      "Epoch 00684: val_loss did not improve from 0.94101\n",
      "Epoch 685/3000\n",
      "18/18 - 1s - loss: 1.1008 - val_loss: 2.2674\n",
      "\n",
      "Epoch 00685: val_loss did not improve from 0.94101\n",
      "Epoch 686/3000\n",
      "18/18 - 1s - loss: 1.1196 - val_loss: 1.3890\n",
      "\n",
      "Epoch 00686: val_loss did not improve from 0.94101\n",
      "Epoch 687/3000\n",
      "18/18 - 1s - loss: 1.1279 - val_loss: 1.3648\n",
      "\n",
      "Epoch 00687: val_loss did not improve from 0.94101\n",
      "Epoch 688/3000\n",
      "18/18 - 1s - loss: 1.2683 - val_loss: 1.4332\n",
      "\n",
      "Epoch 00688: val_loss did not improve from 0.94101\n",
      "Epoch 689/3000\n",
      "18/18 - 1s - loss: 1.0780 - val_loss: 1.0312\n",
      "\n",
      "Epoch 00689: val_loss did not improve from 0.94101\n",
      "Epoch 690/3000\n",
      "18/18 - 1s - loss: 1.1485 - val_loss: 2.7113\n",
      "\n",
      "Epoch 00690: val_loss did not improve from 0.94101\n",
      "Epoch 691/3000\n",
      "18/18 - 1s - loss: 1.3768 - val_loss: 1.0422\n",
      "\n",
      "Epoch 00691: val_loss did not improve from 0.94101\n",
      "Epoch 692/3000\n",
      "18/18 - 1s - loss: 1.1850 - val_loss: 1.7037\n",
      "\n",
      "Epoch 00692: val_loss did not improve from 0.94101\n",
      "Epoch 693/3000\n",
      "18/18 - 1s - loss: 1.0632 - val_loss: 0.9964\n",
      "\n",
      "Epoch 00693: val_loss did not improve from 0.94101\n",
      "Epoch 694/3000\n",
      "18/18 - 1s - loss: 1.0583 - val_loss: 1.0020\n",
      "\n",
      "Epoch 00694: val_loss did not improve from 0.94101\n",
      "Epoch 695/3000\n",
      "18/18 - 1s - loss: 1.1798 - val_loss: 1.2106\n",
      "\n",
      "Epoch 00695: val_loss did not improve from 0.94101\n",
      "Epoch 696/3000\n",
      "18/18 - 1s - loss: 1.1638 - val_loss: 1.0134\n",
      "\n",
      "Epoch 00696: val_loss did not improve from 0.94101\n",
      "Epoch 697/3000\n",
      "18/18 - 1s - loss: 1.0985 - val_loss: 0.9693\n",
      "\n",
      "Epoch 00697: val_loss did not improve from 0.94101\n",
      "Epoch 698/3000\n",
      "18/18 - 1s - loss: 0.9703 - val_loss: 1.3121\n",
      "\n",
      "Epoch 00698: val_loss did not improve from 0.94101\n",
      "Epoch 699/3000\n",
      "18/18 - 1s - loss: 1.0635 - val_loss: 0.9762\n",
      "\n",
      "Epoch 00699: val_loss did not improve from 0.94101\n",
      "Epoch 700/3000\n",
      "18/18 - 1s - loss: 0.9615 - val_loss: 1.3571\n",
      "\n",
      "Epoch 00700: val_loss did not improve from 0.94101\n",
      "Epoch 701/3000\n",
      "18/18 - 1s - loss: 1.2501 - val_loss: 0.9960\n",
      "\n",
      "Epoch 00701: val_loss did not improve from 0.94101\n",
      "Epoch 702/3000\n",
      "18/18 - 1s - loss: 0.9687 - val_loss: 1.3491\n",
      "\n",
      "Epoch 00702: val_loss did not improve from 0.94101\n",
      "Epoch 703/3000\n",
      "18/18 - 1s - loss: 1.0534 - val_loss: 1.0274\n",
      "\n",
      "Epoch 00703: val_loss did not improve from 0.94101\n",
      "Epoch 704/3000\n",
      "18/18 - 1s - loss: 0.9853 - val_loss: 0.9791\n",
      "\n",
      "Epoch 00704: val_loss did not improve from 0.94101\n",
      "Epoch 705/3000\n",
      "18/18 - 1s - loss: 1.0436 - val_loss: 0.9671\n",
      "\n",
      "Epoch 00705: val_loss did not improve from 0.94101\n",
      "Epoch 706/3000\n",
      "18/18 - 1s - loss: 1.0495 - val_loss: 1.0469\n",
      "\n",
      "Epoch 00706: val_loss did not improve from 0.94101\n",
      "Epoch 707/3000\n",
      "18/18 - 1s - loss: 1.1436 - val_loss: 1.4005\n",
      "\n",
      "Epoch 00707: val_loss did not improve from 0.94101\n",
      "Epoch 708/3000\n",
      "18/18 - 1s - loss: 1.0583 - val_loss: 1.3402\n",
      "\n",
      "Epoch 00708: val_loss did not improve from 0.94101\n",
      "Epoch 709/3000\n",
      "18/18 - 1s - loss: 0.9765 - val_loss: 1.1717\n",
      "\n",
      "Epoch 00709: val_loss did not improve from 0.94101\n",
      "Epoch 710/3000\n",
      "18/18 - 1s - loss: 1.0751 - val_loss: 1.1377\n",
      "\n",
      "Epoch 00710: val_loss did not improve from 0.94101\n",
      "Epoch 711/3000\n",
      "18/18 - 1s - loss: 1.1063 - val_loss: 1.0216\n",
      "\n",
      "Epoch 00711: val_loss did not improve from 0.94101\n",
      "Epoch 712/3000\n",
      "18/18 - 1s - loss: 1.0141 - val_loss: 0.9659\n",
      "\n",
      "Epoch 00712: val_loss did not improve from 0.94101\n",
      "Epoch 713/3000\n",
      "18/18 - 1s - loss: 1.0154 - val_loss: 1.5450\n",
      "\n",
      "Epoch 00713: val_loss did not improve from 0.94101\n",
      "Epoch 714/3000\n",
      "18/18 - 1s - loss: 1.1869 - val_loss: 1.0025\n",
      "\n",
      "Epoch 00714: val_loss did not improve from 0.94101\n",
      "Epoch 715/3000\n",
      "18/18 - 1s - loss: 0.9574 - val_loss: 1.0125\n",
      "\n",
      "Epoch 00715: val_loss did not improve from 0.94101\n",
      "Epoch 716/3000\n",
      "18/18 - 1s - loss: 1.0577 - val_loss: 1.0221\n",
      "\n",
      "Epoch 00716: val_loss did not improve from 0.94101\n",
      "Epoch 717/3000\n",
      "18/18 - 1s - loss: 1.0520 - val_loss: 0.9820\n",
      "\n",
      "Epoch 00717: val_loss did not improve from 0.94101\n",
      "Epoch 718/3000\n",
      "18/18 - 1s - loss: 1.0828 - val_loss: 0.9686\n",
      "\n",
      "Epoch 00718: val_loss did not improve from 0.94101\n",
      "Epoch 719/3000\n",
      "18/18 - 1s - loss: 0.9706 - val_loss: 1.0825\n",
      "\n",
      "Epoch 00719: val_loss did not improve from 0.94101\n",
      "Epoch 720/3000\n",
      "18/18 - 1s - loss: 1.0922 - val_loss: 1.2583\n",
      "\n",
      "Epoch 00720: val_loss did not improve from 0.94101\n",
      "Epoch 721/3000\n",
      "18/18 - 1s - loss: 1.0267 - val_loss: 1.0147\n",
      "\n",
      "Epoch 00721: val_loss did not improve from 0.94101\n",
      "Epoch 722/3000\n",
      "18/18 - 1s - loss: 0.9611 - val_loss: 1.0000\n",
      "\n",
      "Epoch 00722: val_loss did not improve from 0.94101\n",
      "Epoch 723/3000\n",
      "18/18 - 1s - loss: 1.0918 - val_loss: 1.0452\n",
      "\n",
      "Epoch 00723: val_loss did not improve from 0.94101\n",
      "Epoch 724/3000\n",
      "18/18 - 1s - loss: 0.9991 - val_loss: 1.2249\n",
      "\n",
      "Epoch 00724: val_loss did not improve from 0.94101\n",
      "Epoch 725/3000\n",
      "18/18 - 1s - loss: 1.0561 - val_loss: 1.0921\n",
      "\n",
      "Epoch 00725: val_loss did not improve from 0.94101\n",
      "Epoch 726/3000\n",
      "18/18 - 1s - loss: 1.0381 - val_loss: 0.9561\n",
      "\n",
      "Epoch 00726: val_loss did not improve from 0.94101\n",
      "Epoch 727/3000\n",
      "18/18 - 1s - loss: 1.0642 - val_loss: 1.0813\n",
      "\n",
      "Epoch 00727: val_loss did not improve from 0.94101\n",
      "Epoch 728/3000\n",
      "18/18 - 1s - loss: 0.9694 - val_loss: 1.3238\n",
      "\n",
      "Epoch 00728: val_loss did not improve from 0.94101\n",
      "Epoch 729/3000\n",
      "18/18 - 1s - loss: 1.0710 - val_loss: 0.9698\n",
      "\n",
      "Epoch 00729: val_loss did not improve from 0.94101\n",
      "Epoch 730/3000\n",
      "18/18 - 1s - loss: 1.1070 - val_loss: 1.0312\n",
      "\n",
      "Epoch 00730: val_loss did not improve from 0.94101\n",
      "Epoch 731/3000\n",
      "18/18 - 1s - loss: 1.0124 - val_loss: 2.2064\n",
      "\n",
      "Epoch 00731: val_loss did not improve from 0.94101\n",
      "Epoch 732/3000\n",
      "18/18 - 1s - loss: 1.2701 - val_loss: 1.0636\n",
      "\n",
      "Epoch 00732: val_loss did not improve from 0.94101\n",
      "Epoch 733/3000\n",
      "18/18 - 1s - loss: 1.0325 - val_loss: 0.9590\n",
      "\n",
      "Epoch 00733: val_loss did not improve from 0.94101\n",
      "Epoch 734/3000\n",
      "18/18 - 1s - loss: 1.1094 - val_loss: 0.9987\n",
      "\n",
      "Epoch 00734: val_loss did not improve from 0.94101\n",
      "Epoch 735/3000\n",
      "18/18 - 1s - loss: 1.0697 - val_loss: 1.0018\n",
      "\n",
      "Epoch 00735: val_loss did not improve from 0.94101\n",
      "Epoch 736/3000\n",
      "18/18 - 1s - loss: 1.0648 - val_loss: 1.2940\n",
      "\n",
      "Epoch 00736: val_loss did not improve from 0.94101\n",
      "Epoch 737/3000\n",
      "18/18 - 1s - loss: 1.0562 - val_loss: 0.9383\n",
      "\n",
      "Epoch 00737: val_loss improved from 0.94101 to 0.93826, saving model to qkeras_weights.h5\n",
      "Epoch 738/3000\n",
      "18/18 - 1s - loss: 1.0805 - val_loss: 0.9455\n",
      "\n",
      "Epoch 00738: val_loss did not improve from 0.93826\n",
      "Epoch 739/3000\n",
      "18/18 - 1s - loss: 0.9693 - val_loss: 1.3021\n",
      "\n",
      "Epoch 00739: val_loss did not improve from 0.93826\n",
      "Epoch 740/3000\n",
      "18/18 - 1s - loss: 1.0526 - val_loss: 0.9750\n",
      "\n",
      "Epoch 00740: val_loss did not improve from 0.93826\n",
      "Epoch 741/3000\n",
      "18/18 - 1s - loss: 0.9581 - val_loss: 0.9486\n",
      "\n",
      "Epoch 00741: val_loss did not improve from 0.93826\n",
      "Epoch 742/3000\n",
      "18/18 - 1s - loss: 1.0778 - val_loss: 0.9559\n",
      "\n",
      "Epoch 00742: val_loss did not improve from 0.93826\n",
      "Epoch 743/3000\n",
      "18/18 - 1s - loss: 0.9639 - val_loss: 1.5624\n",
      "\n",
      "Epoch 00743: val_loss did not improve from 0.93826\n",
      "Epoch 744/3000\n",
      "18/18 - 1s - loss: 1.0359 - val_loss: 0.9718\n",
      "\n",
      "Epoch 00744: val_loss did not improve from 0.93826\n",
      "Epoch 745/3000\n",
      "18/18 - 1s - loss: 1.1664 - val_loss: 1.0497\n",
      "\n",
      "Epoch 00745: val_loss did not improve from 0.93826\n",
      "Epoch 746/3000\n",
      "18/18 - 1s - loss: 1.1310 - val_loss: 1.2195\n",
      "\n",
      "Epoch 00746: val_loss did not improve from 0.93826\n",
      "Epoch 747/3000\n",
      "18/18 - 1s - loss: 0.9909 - val_loss: 1.0499\n",
      "\n",
      "Epoch 00747: val_loss did not improve from 0.93826\n",
      "Epoch 748/3000\n",
      "18/18 - 1s - loss: 1.0395 - val_loss: 1.2492\n",
      "\n",
      "Epoch 00748: val_loss did not improve from 0.93826\n",
      "Epoch 749/3000\n",
      "18/18 - 1s - loss: 0.9726 - val_loss: 0.9753\n",
      "\n",
      "Epoch 00749: val_loss did not improve from 0.93826\n",
      "Epoch 750/3000\n",
      "18/18 - 1s - loss: 0.9579 - val_loss: 1.0826\n",
      "\n",
      "Epoch 00750: val_loss did not improve from 0.93826\n",
      "Epoch 751/3000\n",
      "18/18 - 1s - loss: 1.0615 - val_loss: 1.0056\n",
      "\n",
      "Epoch 00751: val_loss did not improve from 0.93826\n",
      "Epoch 752/3000\n",
      "18/18 - 1s - loss: 1.0537 - val_loss: 0.9963\n",
      "\n",
      "Epoch 00752: val_loss did not improve from 0.93826\n",
      "Epoch 753/3000\n",
      "18/18 - 1s - loss: 0.9455 - val_loss: 0.9666\n",
      "\n",
      "Epoch 00753: val_loss did not improve from 0.93826\n",
      "Epoch 754/3000\n",
      "18/18 - 1s - loss: 1.0139 - val_loss: 0.9759\n",
      "\n",
      "Epoch 00754: val_loss did not improve from 0.93826\n",
      "Epoch 755/3000\n",
      "18/18 - 1s - loss: 1.0954 - val_loss: 0.9784\n",
      "\n",
      "Epoch 00755: val_loss did not improve from 0.93826\n",
      "Epoch 756/3000\n",
      "18/18 - 1s - loss: 1.0731 - val_loss: 1.2266\n",
      "\n",
      "Epoch 00756: val_loss did not improve from 0.93826\n",
      "Epoch 757/3000\n",
      "18/18 - 1s - loss: 1.0475 - val_loss: 0.9945\n",
      "\n",
      "Epoch 00757: val_loss did not improve from 0.93826\n",
      "Epoch 758/3000\n",
      "18/18 - 1s - loss: 0.9746 - val_loss: 1.4591\n",
      "\n",
      "Epoch 00758: val_loss did not improve from 0.93826\n",
      "Epoch 759/3000\n",
      "18/18 - 1s - loss: 1.0133 - val_loss: 1.6026\n",
      "\n",
      "Epoch 00759: val_loss did not improve from 0.93826\n",
      "Epoch 760/3000\n",
      "18/18 - 1s - loss: 0.9943 - val_loss: 1.3095\n",
      "\n",
      "Epoch 00760: val_loss did not improve from 0.93826\n",
      "Epoch 761/3000\n",
      "18/18 - 1s - loss: 1.1381 - val_loss: 0.9895\n",
      "\n",
      "Epoch 00761: val_loss did not improve from 0.93826\n",
      "Epoch 762/3000\n",
      "18/18 - 1s - loss: 1.0662 - val_loss: 0.9669\n",
      "\n",
      "Epoch 00762: val_loss did not improve from 0.93826\n",
      "Epoch 763/3000\n",
      "18/18 - 1s - loss: 1.0682 - val_loss: 1.0858\n",
      "\n",
      "Epoch 00763: val_loss did not improve from 0.93826\n",
      "Epoch 764/3000\n",
      "18/18 - 1s - loss: 0.9809 - val_loss: 0.9678\n",
      "\n",
      "Epoch 00764: val_loss did not improve from 0.93826\n",
      "Epoch 765/3000\n",
      "18/18 - 1s - loss: 0.9588 - val_loss: 1.3901\n",
      "\n",
      "Epoch 00765: val_loss did not improve from 0.93826\n",
      "Epoch 766/3000\n",
      "18/18 - 1s - loss: 1.1046 - val_loss: 1.6373\n",
      "\n",
      "Epoch 00766: val_loss did not improve from 0.93826\n",
      "Epoch 767/3000\n",
      "18/18 - 1s - loss: 1.0605 - val_loss: 1.2525\n",
      "\n",
      "Epoch 00767: val_loss did not improve from 0.93826\n",
      "Epoch 768/3000\n",
      "18/18 - 1s - loss: 0.9843 - val_loss: 1.0523\n",
      "\n",
      "Epoch 00768: val_loss did not improve from 0.93826\n",
      "Epoch 769/3000\n",
      "18/18 - 1s - loss: 1.1558 - val_loss: 1.1177\n",
      "\n",
      "Epoch 00769: val_loss did not improve from 0.93826\n",
      "Epoch 770/3000\n",
      "18/18 - 1s - loss: 1.0130 - val_loss: 1.0609\n",
      "\n",
      "Epoch 00770: val_loss did not improve from 0.93826\n",
      "Epoch 771/3000\n",
      "18/18 - 1s - loss: 0.9888 - val_loss: 1.0147\n",
      "\n",
      "Epoch 00771: val_loss did not improve from 0.93826\n",
      "Epoch 772/3000\n",
      "18/18 - 1s - loss: 0.9618 - val_loss: 1.3754\n",
      "\n",
      "Epoch 00772: val_loss did not improve from 0.93826\n",
      "Epoch 773/3000\n",
      "18/18 - 1s - loss: 1.1919 - val_loss: 0.9767\n",
      "\n",
      "Epoch 00773: val_loss did not improve from 0.93826\n",
      "Epoch 774/3000\n",
      "18/18 - 1s - loss: 0.9673 - val_loss: 1.0266\n",
      "\n",
      "Epoch 00774: val_loss did not improve from 0.93826\n",
      "Epoch 775/3000\n",
      "18/18 - 1s - loss: 1.0715 - val_loss: 1.4275\n",
      "\n",
      "Epoch 00775: val_loss did not improve from 0.93826\n",
      "Epoch 776/3000\n",
      "18/18 - 1s - loss: 0.9578 - val_loss: 0.9910\n",
      "\n",
      "Epoch 00776: val_loss did not improve from 0.93826\n",
      "Epoch 777/3000\n",
      "18/18 - 1s - loss: 0.9458 - val_loss: 0.9465\n",
      "\n",
      "Epoch 00777: val_loss did not improve from 0.93826\n",
      "Epoch 778/3000\n",
      "18/18 - 1s - loss: 1.1190 - val_loss: 1.6522\n",
      "\n",
      "Epoch 00778: val_loss did not improve from 0.93826\n",
      "Epoch 779/3000\n",
      "18/18 - 1s - loss: 1.0086 - val_loss: 2.7990\n",
      "\n",
      "Epoch 00779: val_loss did not improve from 0.93826\n",
      "Epoch 780/3000\n",
      "18/18 - 1s - loss: 1.1371 - val_loss: 1.4763\n",
      "\n",
      "Epoch 00780: val_loss did not improve from 0.93826\n",
      "Epoch 781/3000\n",
      "18/18 - 1s - loss: 1.0225 - val_loss: 1.0109\n",
      "\n",
      "Epoch 00781: val_loss did not improve from 0.93826\n",
      "Epoch 782/3000\n",
      "18/18 - 1s - loss: 0.9754 - val_loss: 1.8928\n",
      "\n",
      "Epoch 00782: val_loss did not improve from 0.93826\n",
      "Epoch 783/3000\n",
      "18/18 - 1s - loss: 1.1079 - val_loss: 1.6391\n",
      "\n",
      "Epoch 00783: val_loss did not improve from 0.93826\n",
      "Epoch 784/3000\n",
      "18/18 - 1s - loss: 0.9887 - val_loss: 0.9466\n",
      "\n",
      "Epoch 00784: val_loss did not improve from 0.93826\n",
      "Epoch 785/3000\n",
      "18/18 - 1s - loss: 1.0884 - val_loss: 0.9609\n",
      "\n",
      "Epoch 00785: val_loss did not improve from 0.93826\n",
      "Epoch 786/3000\n",
      "18/18 - 1s - loss: 1.1363 - val_loss: 1.3531\n",
      "\n",
      "Epoch 00786: val_loss did not improve from 0.93826\n",
      "Epoch 787/3000\n",
      "18/18 - 1s - loss: 1.0060 - val_loss: 1.0680\n",
      "\n",
      "Epoch 00787: val_loss did not improve from 0.93826\n",
      "Epoch 788/3000\n",
      "18/18 - 1s - loss: 1.2468 - val_loss: 1.7781\n",
      "\n",
      "Epoch 00788: val_loss did not improve from 0.93826\n",
      "Epoch 789/3000\n",
      "18/18 - 1s - loss: 1.2356 - val_loss: 3.9483\n",
      "\n",
      "Epoch 00789: val_loss did not improve from 0.93826\n",
      "Epoch 790/3000\n",
      "18/18 - 1s - loss: 1.5828 - val_loss: 1.2226\n",
      "\n",
      "Epoch 00790: val_loss did not improve from 0.93826\n",
      "Epoch 791/3000\n",
      "18/18 - 1s - loss: 1.1441 - val_loss: 1.1254\n",
      "\n",
      "Epoch 00791: val_loss did not improve from 0.93826\n",
      "Epoch 792/3000\n",
      "18/18 - 1s - loss: 1.0631 - val_loss: 1.0962\n",
      "\n",
      "Epoch 00792: val_loss did not improve from 0.93826\n",
      "Epoch 793/3000\n",
      "18/18 - 1s - loss: 1.0430 - val_loss: 2.0806\n",
      "\n",
      "Epoch 00793: val_loss did not improve from 0.93826\n",
      "Epoch 794/3000\n",
      "18/18 - 1s - loss: 1.5822 - val_loss: 2.0661\n",
      "\n",
      "Epoch 00794: val_loss did not improve from 0.93826\n",
      "Epoch 795/3000\n",
      "18/18 - 1s - loss: 1.6351 - val_loss: 2.7261\n",
      "\n",
      "Epoch 00795: val_loss did not improve from 0.93826\n",
      "Epoch 796/3000\n",
      "18/18 - 1s - loss: 1.6464 - val_loss: 2.5305\n",
      "\n",
      "Epoch 00796: val_loss did not improve from 0.93826\n",
      "Epoch 797/3000\n",
      "18/18 - 1s - loss: 1.4894 - val_loss: 2.4885\n",
      "\n",
      "Epoch 00797: val_loss did not improve from 0.93826\n",
      "Epoch 798/3000\n",
      "18/18 - 1s - loss: 1.6452 - val_loss: 1.1258\n",
      "\n",
      "Epoch 00798: val_loss did not improve from 0.93826\n",
      "Epoch 799/3000\n",
      "18/18 - 1s - loss: 1.5473 - val_loss: 2.6570\n",
      "\n",
      "Epoch 00799: val_loss did not improve from 0.93826\n",
      "Epoch 800/3000\n",
      "18/18 - 1s - loss: 1.6272 - val_loss: 1.3060\n",
      "\n",
      "Epoch 00800: val_loss did not improve from 0.93826\n",
      "Epoch 801/3000\n",
      "18/18 - 1s - loss: 1.8099 - val_loss: 1.6908\n",
      "\n",
      "Epoch 00801: val_loss did not improve from 0.93826\n",
      "Epoch 802/3000\n",
      "18/18 - 1s - loss: 1.6373 - val_loss: 3.0895\n",
      "\n",
      "Epoch 00802: val_loss did not improve from 0.93826\n",
      "Epoch 803/3000\n",
      "18/18 - 1s - loss: 1.8201 - val_loss: 6.7193\n",
      "\n",
      "Epoch 00803: val_loss did not improve from 0.93826\n",
      "Epoch 804/3000\n",
      "18/18 - 1s - loss: 1.4136 - val_loss: 1.4000\n",
      "\n",
      "Epoch 00804: val_loss did not improve from 0.93826\n",
      "Epoch 805/3000\n",
      "18/18 - 1s - loss: 1.3925 - val_loss: 1.3622\n",
      "\n",
      "Epoch 00805: val_loss did not improve from 0.93826\n",
      "Epoch 806/3000\n",
      "18/18 - 1s - loss: 1.1830 - val_loss: 1.6926\n",
      "\n",
      "Epoch 00806: val_loss did not improve from 0.93826\n",
      "Epoch 807/3000\n",
      "18/18 - 1s - loss: 1.3512 - val_loss: 1.1014\n",
      "\n",
      "Epoch 00807: val_loss did not improve from 0.93826\n",
      "Epoch 808/3000\n",
      "18/18 - 1s - loss: 1.1777 - val_loss: 1.0705\n",
      "\n",
      "Epoch 00808: val_loss did not improve from 0.93826\n",
      "Epoch 809/3000\n",
      "18/18 - 1s - loss: 1.0727 - val_loss: 1.0196\n",
      "\n",
      "Epoch 00809: val_loss did not improve from 0.93826\n",
      "Epoch 810/3000\n",
      "18/18 - 1s - loss: 1.0048 - val_loss: 1.5234\n",
      "\n",
      "Epoch 00810: val_loss did not improve from 0.93826\n",
      "Epoch 811/3000\n",
      "18/18 - 1s - loss: 1.1128 - val_loss: 1.0380\n",
      "\n",
      "Epoch 00811: val_loss did not improve from 0.93826\n",
      "Epoch 812/3000\n",
      "18/18 - 1s - loss: 0.9944 - val_loss: 0.9929\n",
      "\n",
      "Epoch 00812: val_loss did not improve from 0.93826\n",
      "Epoch 813/3000\n",
      "18/18 - 1s - loss: 1.0936 - val_loss: 0.9858\n",
      "\n",
      "Epoch 00813: val_loss did not improve from 0.93826\n",
      "Epoch 814/3000\n",
      "18/18 - 1s - loss: 1.0249 - val_loss: 1.0326\n",
      "\n",
      "Epoch 00814: val_loss did not improve from 0.93826\n",
      "Epoch 815/3000\n",
      "18/18 - 1s - loss: 1.0727 - val_loss: 0.9812\n",
      "\n",
      "Epoch 00815: val_loss did not improve from 0.93826\n",
      "Epoch 816/3000\n",
      "18/18 - 1s - loss: 0.9815 - val_loss: 1.4585\n",
      "\n",
      "Epoch 00816: val_loss did not improve from 0.93826\n",
      "Epoch 817/3000\n",
      "18/18 - 1s - loss: 1.0070 - val_loss: 1.0065\n",
      "\n",
      "Epoch 00817: val_loss did not improve from 0.93826\n",
      "Epoch 818/3000\n",
      "18/18 - 1s - loss: 1.0894 - val_loss: 1.0270\n",
      "\n",
      "Epoch 00818: val_loss did not improve from 0.93826\n",
      "Epoch 819/3000\n",
      "18/18 - 1s - loss: 1.0728 - val_loss: 0.9884\n",
      "\n",
      "Epoch 00819: val_loss did not improve from 0.93826\n",
      "Epoch 820/3000\n",
      "18/18 - 1s - loss: 1.0596 - val_loss: 0.9601\n",
      "\n",
      "Epoch 00820: val_loss did not improve from 0.93826\n",
      "Epoch 821/3000\n",
      "18/18 - 1s - loss: 0.9553 - val_loss: 0.9518\n",
      "\n",
      "Epoch 00821: val_loss did not improve from 0.93826\n",
      "Epoch 822/3000\n",
      "18/18 - 1s - loss: 1.0588 - val_loss: 0.9573\n",
      "\n",
      "Epoch 00822: val_loss did not improve from 0.93826\n",
      "Epoch 823/3000\n",
      "18/18 - 1s - loss: 1.1785 - val_loss: 1.1012\n",
      "\n",
      "Epoch 00823: val_loss did not improve from 0.93826\n",
      "Epoch 824/3000\n",
      "18/18 - 1s - loss: 1.0118 - val_loss: 1.0622\n",
      "\n",
      "Epoch 00824: val_loss did not improve from 0.93826\n",
      "Epoch 825/3000\n",
      "18/18 - 1s - loss: 0.9813 - val_loss: 1.0032\n",
      "\n",
      "Epoch 00825: val_loss did not improve from 0.93826\n",
      "Epoch 826/3000\n",
      "18/18 - 2s - loss: 1.0816 - val_loss: 1.0274\n",
      "\n",
      "Epoch 00826: val_loss did not improve from 0.93826\n",
      "Epoch 827/3000\n",
      "18/18 - 1s - loss: 0.9828 - val_loss: 1.0271\n",
      "\n",
      "Epoch 00827: val_loss did not improve from 0.93826\n",
      "Epoch 828/3000\n",
      "18/18 - 1s - loss: 1.1761 - val_loss: 1.5443\n",
      "\n",
      "Epoch 00828: val_loss did not improve from 0.93826\n",
      "Epoch 829/3000\n",
      "18/18 - 1s - loss: 1.0870 - val_loss: 1.3693\n",
      "\n",
      "Epoch 00829: val_loss did not improve from 0.93826\n",
      "Epoch 830/3000\n",
      "18/18 - 1s - loss: 1.1432 - val_loss: 1.6708\n",
      "\n",
      "Epoch 00830: val_loss did not improve from 0.93826\n",
      "Epoch 831/3000\n",
      "18/18 - 1s - loss: 1.0500 - val_loss: 1.4446\n",
      "\n",
      "Epoch 00831: val_loss did not improve from 0.93826\n",
      "Epoch 832/3000\n",
      "18/18 - 1s - loss: 1.0320 - val_loss: 1.4287\n",
      "\n",
      "Epoch 00832: val_loss did not improve from 0.93826\n",
      "Epoch 833/3000\n",
      "18/18 - 1s - loss: 1.0209 - val_loss: 1.0986\n",
      "\n",
      "Epoch 00833: val_loss did not improve from 0.93826\n",
      "Epoch 834/3000\n",
      "18/18 - 1s - loss: 1.0103 - val_loss: 1.0239\n",
      "\n",
      "Epoch 00834: val_loss did not improve from 0.93826\n",
      "Epoch 835/3000\n",
      "18/18 - 1s - loss: 1.1078 - val_loss: 1.0334\n",
      "\n",
      "Epoch 00835: val_loss did not improve from 0.93826\n",
      "Epoch 836/3000\n",
      "18/18 - 1s - loss: 1.0369 - val_loss: 1.0862\n",
      "\n",
      "Epoch 00836: val_loss did not improve from 0.93826\n",
      "Epoch 837/3000\n",
      "18/18 - 1s - loss: 1.0312 - val_loss: 1.0327\n",
      "\n",
      "Epoch 00837: val_loss did not improve from 0.93826\n",
      "Epoch 838/3000\n",
      "18/18 - 1s - loss: 0.9931 - val_loss: 1.0240\n",
      "\n",
      "Epoch 00838: val_loss did not improve from 0.93826\n",
      "Epoch 839/3000\n",
      "18/18 - 1s - loss: 1.0406 - val_loss: 1.1996\n",
      "\n",
      "Epoch 00839: val_loss did not improve from 0.93826\n",
      "Epoch 840/3000\n",
      "18/18 - 1s - loss: 1.1526 - val_loss: 0.9863\n",
      "\n",
      "Epoch 00840: val_loss did not improve from 0.93826\n",
      "Epoch 841/3000\n",
      "18/18 - 1s - loss: 0.9963 - val_loss: 0.9881\n",
      "\n",
      "Epoch 00841: val_loss did not improve from 0.93826\n",
      "Epoch 842/3000\n",
      "18/18 - 1s - loss: 0.9851 - val_loss: 1.0062\n",
      "\n",
      "Epoch 00842: val_loss did not improve from 0.93826\n",
      "Epoch 843/3000\n",
      "18/18 - 1s - loss: 1.1781 - val_loss: 0.9995\n",
      "\n",
      "Epoch 00843: val_loss did not improve from 0.93826\n",
      "Epoch 844/3000\n",
      "18/18 - 1s - loss: 0.9823 - val_loss: 1.4047\n",
      "\n",
      "Epoch 00844: val_loss did not improve from 0.93826\n",
      "Epoch 845/3000\n",
      "18/18 - 1s - loss: 0.9880 - val_loss: 1.3670\n",
      "\n",
      "Epoch 00845: val_loss did not improve from 0.93826\n",
      "Epoch 846/3000\n",
      "18/18 - 1s - loss: 1.0161 - val_loss: 1.0763\n",
      "\n",
      "Epoch 00846: val_loss did not improve from 0.93826\n",
      "Epoch 847/3000\n",
      "18/18 - 1s - loss: 1.1277 - val_loss: 0.9848\n",
      "\n",
      "Epoch 00847: val_loss did not improve from 0.93826\n",
      "Epoch 848/3000\n",
      "18/18 - 1s - loss: 0.9745 - val_loss: 0.9611\n",
      "\n",
      "Epoch 00848: val_loss did not improve from 0.93826\n",
      "Epoch 849/3000\n",
      "18/18 - 1s - loss: 0.9837 - val_loss: 0.9953\n",
      "\n",
      "Epoch 00849: val_loss did not improve from 0.93826\n",
      "Epoch 850/3000\n",
      "18/18 - 1s - loss: 1.0057 - val_loss: 0.9790\n",
      "\n",
      "Epoch 00850: val_loss did not improve from 0.93826\n",
      "Epoch 851/3000\n",
      "18/18 - 1s - loss: 1.0580 - val_loss: 0.9600\n",
      "\n",
      "Epoch 00851: val_loss did not improve from 0.93826\n",
      "Epoch 852/3000\n",
      "18/18 - 1s - loss: 0.9484 - val_loss: 1.4938\n",
      "\n",
      "Epoch 00852: val_loss did not improve from 0.93826\n",
      "Epoch 853/3000\n",
      "18/18 - 1s - loss: 1.0482 - val_loss: 0.9950\n",
      "\n",
      "Epoch 00853: val_loss did not improve from 0.93826\n",
      "Epoch 854/3000\n",
      "18/18 - 1s - loss: 1.0359 - val_loss: 1.1822\n",
      "\n",
      "Epoch 00854: val_loss did not improve from 0.93826\n",
      "Epoch 855/3000\n",
      "18/18 - 1s - loss: 1.0219 - val_loss: 0.9639\n",
      "\n",
      "Epoch 00855: val_loss did not improve from 0.93826\n",
      "Epoch 856/3000\n",
      "18/18 - 1s - loss: 0.9598 - val_loss: 1.3179\n",
      "\n",
      "Epoch 00856: val_loss did not improve from 0.93826\n",
      "Epoch 857/3000\n",
      "18/18 - 1s - loss: 1.0488 - val_loss: 1.2783\n",
      "\n",
      "Epoch 00857: val_loss did not improve from 0.93826\n",
      "Epoch 858/3000\n",
      "18/18 - 1s - loss: 1.0398 - val_loss: 1.0274\n",
      "\n",
      "Epoch 00858: val_loss did not improve from 0.93826\n",
      "Epoch 859/3000\n",
      "18/18 - 1s - loss: 1.0027 - val_loss: 1.0134\n",
      "\n",
      "Epoch 00859: val_loss did not improve from 0.93826\n",
      "Epoch 860/3000\n",
      "18/18 - 1s - loss: 1.0526 - val_loss: 0.9869\n",
      "\n",
      "Epoch 00860: val_loss did not improve from 0.93826\n",
      "Epoch 861/3000\n",
      "18/18 - 1s - loss: 1.0271 - val_loss: 1.0698\n",
      "\n",
      "Epoch 00861: val_loss did not improve from 0.93826\n",
      "Epoch 862/3000\n",
      "18/18 - 1s - loss: 1.0568 - val_loss: 1.0007\n",
      "\n",
      "Epoch 00862: val_loss did not improve from 0.93826\n",
      "Epoch 863/3000\n",
      "18/18 - 1s - loss: 0.9594 - val_loss: 1.5235\n",
      "\n",
      "Epoch 00863: val_loss did not improve from 0.93826\n",
      "Epoch 864/3000\n",
      "18/18 - 1s - loss: 1.0398 - val_loss: 0.9894\n",
      "\n",
      "Epoch 00864: val_loss did not improve from 0.93826\n",
      "Epoch 865/3000\n",
      "18/18 - 1s - loss: 0.9821 - val_loss: 3.2373\n",
      "\n",
      "Epoch 00865: val_loss did not improve from 0.93826\n",
      "Epoch 866/3000\n",
      "18/18 - 1s - loss: 1.2602 - val_loss: 1.5307\n",
      "\n",
      "Epoch 00866: val_loss did not improve from 0.93826\n",
      "Epoch 867/3000\n",
      "18/18 - 1s - loss: 1.0229 - val_loss: 0.9950\n",
      "\n",
      "Epoch 00867: val_loss did not improve from 0.93826\n",
      "Epoch 868/3000\n",
      "18/18 - 1s - loss: 1.0657 - val_loss: 0.9983\n",
      "\n",
      "Epoch 00868: val_loss did not improve from 0.93826\n",
      "Epoch 869/3000\n",
      "18/18 - 1s - loss: 1.1336 - val_loss: 0.9593\n",
      "\n",
      "Epoch 00869: val_loss did not improve from 0.93826\n",
      "Epoch 870/3000\n",
      "18/18 - 1s - loss: 1.0273 - val_loss: 1.3752\n",
      "\n",
      "Epoch 00870: val_loss did not improve from 0.93826\n",
      "Epoch 871/3000\n",
      "18/18 - 1s - loss: 1.1476 - val_loss: 0.9608\n",
      "\n",
      "Epoch 00871: val_loss did not improve from 0.93826\n",
      "Epoch 872/3000\n",
      "18/18 - 1s - loss: 1.2367 - val_loss: 0.9537\n",
      "\n",
      "Epoch 00872: val_loss did not improve from 0.93826\n",
      "Epoch 873/3000\n",
      "18/18 - 1s - loss: 0.9693 - val_loss: 0.9684\n",
      "\n",
      "Epoch 00873: val_loss did not improve from 0.93826\n",
      "Epoch 874/3000\n",
      "18/18 - 1s - loss: 1.0854 - val_loss: 0.9782\n",
      "\n",
      "Epoch 00874: val_loss did not improve from 0.93826\n",
      "Epoch 875/3000\n",
      "18/18 - 1s - loss: 1.0160 - val_loss: 0.9835\n",
      "\n",
      "Epoch 00875: val_loss did not improve from 0.93826\n",
      "Epoch 876/3000\n",
      "18/18 - 1s - loss: 0.9666 - val_loss: 0.9637\n",
      "\n",
      "Epoch 00876: val_loss did not improve from 0.93826\n",
      "Epoch 877/3000\n",
      "18/18 - 1s - loss: 1.1462 - val_loss: 1.0673\n",
      "\n",
      "Epoch 00877: val_loss did not improve from 0.93826\n",
      "Epoch 878/3000\n",
      "18/18 - 1s - loss: 1.2275 - val_loss: 1.0058\n",
      "\n",
      "Epoch 00878: val_loss did not improve from 0.93826\n",
      "Epoch 879/3000\n",
      "18/18 - 1s - loss: 0.9858 - val_loss: 0.9907\n",
      "\n",
      "Epoch 00879: val_loss did not improve from 0.93826\n",
      "Epoch 880/3000\n",
      "18/18 - 1s - loss: 0.9503 - val_loss: 0.9999\n",
      "\n",
      "Epoch 00880: val_loss did not improve from 0.93826\n",
      "Epoch 881/3000\n",
      "18/18 - 1s - loss: 1.0369 - val_loss: 1.0340\n",
      "\n",
      "Epoch 00881: val_loss did not improve from 0.93826\n",
      "Epoch 882/3000\n",
      "18/18 - 1s - loss: 1.0838 - val_loss: 1.4243\n",
      "\n",
      "Epoch 00882: val_loss did not improve from 0.93826\n",
      "Epoch 883/3000\n",
      "18/18 - 1s - loss: 0.9567 - val_loss: 1.3190\n",
      "\n",
      "Epoch 00883: val_loss did not improve from 0.93826\n",
      "Epoch 884/3000\n",
      "18/18 - 1s - loss: 1.0965 - val_loss: 1.3739\n",
      "\n",
      "Epoch 00884: val_loss did not improve from 0.93826\n",
      "Epoch 885/3000\n",
      "18/18 - 1s - loss: 0.9911 - val_loss: 1.2091\n",
      "\n",
      "Epoch 00885: val_loss did not improve from 0.93826\n",
      "Epoch 886/3000\n",
      "18/18 - 1s - loss: 1.0937 - val_loss: 0.9678\n",
      "\n",
      "Epoch 00886: val_loss did not improve from 0.93826\n",
      "Epoch 887/3000\n",
      "18/18 - 1s - loss: 1.0388 - val_loss: 1.2310\n",
      "\n",
      "Epoch 00887: val_loss did not improve from 0.93826\n",
      "Epoch 888/3000\n",
      "18/18 - 1s - loss: 1.5169 - val_loss: 1.0677\n",
      "\n",
      "Epoch 00888: val_loss did not improve from 0.93826\n",
      "Epoch 889/3000\n",
      "18/18 - 1s - loss: 0.9842 - val_loss: 1.0114\n",
      "\n",
      "Epoch 00889: val_loss did not improve from 0.93826\n",
      "Epoch 890/3000\n",
      "18/18 - 1s - loss: 0.9806 - val_loss: 0.9522\n",
      "\n",
      "Epoch 00890: val_loss did not improve from 0.93826\n",
      "Epoch 891/3000\n",
      "18/18 - 1s - loss: 1.0367 - val_loss: 1.0130\n",
      "\n",
      "Epoch 00891: val_loss did not improve from 0.93826\n",
      "Epoch 892/3000\n",
      "18/18 - 1s - loss: 0.9683 - val_loss: 0.9605\n",
      "\n",
      "Epoch 00892: val_loss did not improve from 0.93826\n",
      "Epoch 893/3000\n",
      "18/18 - 1s - loss: 1.0854 - val_loss: 1.0578\n",
      "\n",
      "Epoch 00893: val_loss did not improve from 0.93826\n",
      "Epoch 894/3000\n",
      "18/18 - 1s - loss: 0.9764 - val_loss: 0.9698\n",
      "\n",
      "Epoch 00894: val_loss did not improve from 0.93826\n",
      "Epoch 895/3000\n",
      "18/18 - 1s - loss: 1.0992 - val_loss: 0.9910\n",
      "\n",
      "Epoch 00895: val_loss did not improve from 0.93826\n",
      "Epoch 896/3000\n",
      "18/18 - 1s - loss: 1.0856 - val_loss: 1.0117\n",
      "\n",
      "Epoch 00896: val_loss did not improve from 0.93826\n",
      "Epoch 897/3000\n",
      "18/18 - 1s - loss: 0.9530 - val_loss: 2.0482\n",
      "\n",
      "Epoch 00897: val_loss did not improve from 0.93826\n",
      "Epoch 898/3000\n",
      "18/18 - 1s - loss: 1.1196 - val_loss: 1.0069\n",
      "\n",
      "Epoch 00898: val_loss did not improve from 0.93826\n",
      "Epoch 899/3000\n",
      "18/18 - 1s - loss: 1.0657 - val_loss: 0.9692\n",
      "\n",
      "Epoch 00899: val_loss did not improve from 0.93826\n",
      "Epoch 900/3000\n",
      "18/18 - 1s - loss: 1.0332 - val_loss: 1.2675\n",
      "\n",
      "Epoch 00900: val_loss did not improve from 0.93826\n",
      "Epoch 901/3000\n",
      "18/18 - 1s - loss: 1.0691 - val_loss: 0.9644\n",
      "\n",
      "Epoch 00901: val_loss did not improve from 0.93826\n",
      "Epoch 902/3000\n",
      "18/18 - 1s - loss: 1.0255 - val_loss: 0.9407\n",
      "\n",
      "Epoch 00902: val_loss did not improve from 0.93826\n",
      "Epoch 903/3000\n",
      "18/18 - 1s - loss: 0.9712 - val_loss: 0.9430\n",
      "\n",
      "Epoch 00903: val_loss did not improve from 0.93826\n",
      "Epoch 904/3000\n",
      "18/18 - 1s - loss: 1.0309 - val_loss: 1.4503\n",
      "\n",
      "Epoch 00904: val_loss did not improve from 0.93826\n",
      "Epoch 905/3000\n",
      "18/18 - 1s - loss: 1.0483 - val_loss: 1.0816\n",
      "\n",
      "Epoch 00905: val_loss did not improve from 0.93826\n",
      "Epoch 906/3000\n",
      "18/18 - 1s - loss: 1.0759 - val_loss: 1.0529\n",
      "\n",
      "Epoch 00906: val_loss did not improve from 0.93826\n",
      "Epoch 907/3000\n",
      "18/18 - 1s - loss: 1.0391 - val_loss: 1.6825\n",
      "\n",
      "Epoch 00907: val_loss did not improve from 0.93826\n",
      "Epoch 908/3000\n",
      "18/18 - 1s - loss: 1.0156 - val_loss: 1.0384\n",
      "\n",
      "Epoch 00908: val_loss did not improve from 0.93826\n",
      "Epoch 909/3000\n",
      "18/18 - 1s - loss: 1.1468 - val_loss: 1.1597\n",
      "\n",
      "Epoch 00909: val_loss did not improve from 0.93826\n",
      "Epoch 910/3000\n",
      "18/18 - 1s - loss: 1.0488 - val_loss: 1.0922\n",
      "\n",
      "Epoch 00910: val_loss did not improve from 0.93826\n",
      "Epoch 911/3000\n",
      "18/18 - 1s - loss: 0.9992 - val_loss: 1.2485\n",
      "\n",
      "Epoch 00911: val_loss did not improve from 0.93826\n",
      "Epoch 912/3000\n",
      "18/18 - 1s - loss: 1.0705 - val_loss: 1.2912\n",
      "\n",
      "Epoch 00912: val_loss did not improve from 0.93826\n",
      "Epoch 913/3000\n",
      "18/18 - 1s - loss: 0.9795 - val_loss: 0.9569\n",
      "\n",
      "Epoch 00913: val_loss did not improve from 0.93826\n",
      "Epoch 914/3000\n",
      "18/18 - 1s - loss: 1.1811 - val_loss: 1.0101\n",
      "\n",
      "Epoch 00914: val_loss did not improve from 0.93826\n",
      "Epoch 915/3000\n",
      "18/18 - 1s - loss: 0.9728 - val_loss: 1.1670\n",
      "\n",
      "Epoch 00915: val_loss did not improve from 0.93826\n",
      "Epoch 916/3000\n",
      "18/18 - 1s - loss: 0.9513 - val_loss: 1.4868\n",
      "\n",
      "Epoch 00916: val_loss did not improve from 0.93826\n",
      "Epoch 917/3000\n",
      "18/18 - 1s - loss: 1.0760 - val_loss: 1.2959\n",
      "\n",
      "Epoch 00917: val_loss did not improve from 0.93826\n",
      "Epoch 918/3000\n",
      "18/18 - 1s - loss: 0.9871 - val_loss: 0.9893\n",
      "\n",
      "Epoch 00918: val_loss did not improve from 0.93826\n",
      "Epoch 919/3000\n",
      "18/18 - 1s - loss: 1.1912 - val_loss: 1.0545\n",
      "\n",
      "Epoch 00919: val_loss did not improve from 0.93826\n",
      "Epoch 920/3000\n",
      "18/18 - 1s - loss: 0.9747 - val_loss: 0.9549\n",
      "\n",
      "Epoch 00920: val_loss did not improve from 0.93826\n",
      "Epoch 921/3000\n",
      "18/18 - 1s - loss: 1.0453 - val_loss: 0.9523\n",
      "\n",
      "Epoch 00921: val_loss did not improve from 0.93826\n",
      "Epoch 922/3000\n",
      "18/18 - 1s - loss: 1.0023 - val_loss: 0.9576\n",
      "\n",
      "Epoch 00922: val_loss did not improve from 0.93826\n",
      "Epoch 923/3000\n",
      "18/18 - 1s - loss: 1.0687 - val_loss: 0.9562\n",
      "\n",
      "Epoch 00923: val_loss did not improve from 0.93826\n",
      "Epoch 924/3000\n",
      "18/18 - 1s - loss: 1.1334 - val_loss: 0.9503\n",
      "\n",
      "Epoch 00924: val_loss did not improve from 0.93826\n",
      "Epoch 925/3000\n",
      "18/18 - 1s - loss: 1.0884 - val_loss: 0.9806\n",
      "\n",
      "Epoch 00925: val_loss did not improve from 0.93826\n",
      "Epoch 926/3000\n",
      "18/18 - 1s - loss: 0.9882 - val_loss: 1.6247\n",
      "\n",
      "Epoch 00926: val_loss did not improve from 0.93826\n",
      "Epoch 927/3000\n",
      "18/18 - 1s - loss: 1.1554 - val_loss: 0.9825\n",
      "\n",
      "Epoch 00927: val_loss did not improve from 0.93826\n",
      "Epoch 928/3000\n",
      "18/18 - 1s - loss: 1.0831 - val_loss: 0.9646\n",
      "\n",
      "Epoch 00928: val_loss did not improve from 0.93826\n",
      "Epoch 929/3000\n",
      "18/18 - 1s - loss: 0.9615 - val_loss: 0.9499\n",
      "\n",
      "Epoch 00929: val_loss did not improve from 0.93826\n",
      "Epoch 930/3000\n",
      "18/18 - 1s - loss: 1.0196 - val_loss: 0.9483\n",
      "\n",
      "Epoch 00930: val_loss did not improve from 0.93826\n",
      "Epoch 931/3000\n",
      "18/18 - 1s - loss: 0.9412 - val_loss: 0.9629\n",
      "\n",
      "Epoch 00931: val_loss did not improve from 0.93826\n",
      "Epoch 932/3000\n",
      "18/18 - 1s - loss: 1.0578 - val_loss: 0.9747\n",
      "\n",
      "Epoch 00932: val_loss did not improve from 0.93826\n",
      "Epoch 933/3000\n",
      "18/18 - 1s - loss: 1.1324 - val_loss: 0.9466\n",
      "\n",
      "Epoch 00933: val_loss did not improve from 0.93826\n",
      "Epoch 934/3000\n",
      "18/18 - 1s - loss: 1.0591 - val_loss: 1.1662\n",
      "\n",
      "Epoch 00934: val_loss did not improve from 0.93826\n",
      "Epoch 935/3000\n",
      "18/18 - 1s - loss: 0.9823 - val_loss: 1.2826\n",
      "\n",
      "Epoch 00935: val_loss did not improve from 0.93826\n",
      "Epoch 936/3000\n",
      "18/18 - 1s - loss: 1.0326 - val_loss: 0.9842\n",
      "\n",
      "Epoch 00936: val_loss did not improve from 0.93826\n",
      "Epoch 937/3000\n",
      "18/18 - 1s - loss: 0.9773 - val_loss: 0.9504\n",
      "\n",
      "Epoch 00937: val_loss did not improve from 0.93826\n",
      "Epoch 938/3000\n",
      "18/18 - 1s - loss: 1.0250 - val_loss: 0.9306\n",
      "\n",
      "Epoch 00938: val_loss improved from 0.93826 to 0.93061, saving model to qkeras_weights.h5\n",
      "Epoch 939/3000\n",
      "18/18 - 1s - loss: 1.0316 - val_loss: 0.9893\n",
      "\n",
      "Epoch 00939: val_loss did not improve from 0.93061\n",
      "Epoch 940/3000\n",
      "18/18 - 1s - loss: 0.9319 - val_loss: 1.2495\n",
      "\n",
      "Epoch 00940: val_loss did not improve from 0.93061\n",
      "Epoch 941/3000\n",
      "18/18 - 1s - loss: 1.0566 - val_loss: 1.2189\n",
      "\n",
      "Epoch 00941: val_loss did not improve from 0.93061\n",
      "Epoch 942/3000\n",
      "18/18 - 1s - loss: 1.2851 - val_loss: 0.9850\n",
      "\n",
      "Epoch 00942: val_loss did not improve from 0.93061\n",
      "Epoch 943/3000\n",
      "18/18 - 1s - loss: 1.1419 - val_loss: 1.2205\n",
      "\n",
      "Epoch 00943: val_loss did not improve from 0.93061\n",
      "Epoch 944/3000\n",
      "18/18 - 1s - loss: 1.0285 - val_loss: 1.0930\n",
      "\n",
      "Epoch 00944: val_loss did not improve from 0.93061\n",
      "Epoch 945/3000\n",
      "18/18 - 1s - loss: 1.0599 - val_loss: 0.9696\n",
      "\n",
      "Epoch 00945: val_loss did not improve from 0.93061\n",
      "Epoch 946/3000\n",
      "18/18 - 1s - loss: 0.9709 - val_loss: 0.9379\n",
      "\n",
      "Epoch 00946: val_loss did not improve from 0.93061\n",
      "Epoch 947/3000\n",
      "18/18 - 1s - loss: 1.0964 - val_loss: 0.9684\n",
      "\n",
      "Epoch 00947: val_loss did not improve from 0.93061\n",
      "Epoch 948/3000\n",
      "18/18 - 1s - loss: 1.0364 - val_loss: 0.9685\n",
      "\n",
      "Epoch 00948: val_loss did not improve from 0.93061\n",
      "Epoch 949/3000\n",
      "18/18 - 1s - loss: 0.9637 - val_loss: 0.9543\n",
      "\n",
      "Epoch 00949: val_loss did not improve from 0.93061\n",
      "Epoch 950/3000\n",
      "18/18 - 1s - loss: 0.9632 - val_loss: 0.9610\n",
      "\n",
      "Epoch 00950: val_loss did not improve from 0.93061\n",
      "Epoch 951/3000\n",
      "18/18 - 1s - loss: 1.1315 - val_loss: 0.9589\n",
      "\n",
      "Epoch 00951: val_loss did not improve from 0.93061\n",
      "Epoch 952/3000\n",
      "18/18 - 1s - loss: 1.0344 - val_loss: 1.0106\n",
      "\n",
      "Epoch 00952: val_loss did not improve from 0.93061\n",
      "Epoch 953/3000\n",
      "18/18 - 1s - loss: 0.9658 - val_loss: 1.2355\n",
      "\n",
      "Epoch 00953: val_loss did not improve from 0.93061\n",
      "Epoch 954/3000\n",
      "18/18 - 1s - loss: 0.9808 - val_loss: 1.0147\n",
      "\n",
      "Epoch 00954: val_loss did not improve from 0.93061\n",
      "Epoch 955/3000\n",
      "18/18 - 1s - loss: 0.9500 - val_loss: 1.4799\n",
      "\n",
      "Epoch 00955: val_loss did not improve from 0.93061\n",
      "Epoch 956/3000\n",
      "18/18 - 1s - loss: 1.0935 - val_loss: 0.9868\n",
      "\n",
      "Epoch 00956: val_loss did not improve from 0.93061\n",
      "Epoch 957/3000\n",
      "18/18 - 1s - loss: 0.9680 - val_loss: 1.0408\n",
      "\n",
      "Epoch 00957: val_loss did not improve from 0.93061\n",
      "Epoch 958/3000\n",
      "18/18 - 1s - loss: 1.0380 - val_loss: 1.1645\n",
      "\n",
      "Epoch 00958: val_loss did not improve from 0.93061\n",
      "Epoch 959/3000\n",
      "18/18 - 1s - loss: 1.1002 - val_loss: 1.0281\n",
      "\n",
      "Epoch 00959: val_loss did not improve from 0.93061\n",
      "Epoch 960/3000\n",
      "18/18 - 1s - loss: 0.9447 - val_loss: 0.9532\n",
      "\n",
      "Epoch 00960: val_loss did not improve from 0.93061\n",
      "Epoch 961/3000\n",
      "18/18 - 1s - loss: 1.0226 - val_loss: 1.0760\n",
      "\n",
      "Epoch 00961: val_loss did not improve from 0.93061\n",
      "Epoch 962/3000\n",
      "18/18 - 1s - loss: 1.0998 - val_loss: 1.0544\n",
      "\n",
      "Epoch 00962: val_loss did not improve from 0.93061\n",
      "Epoch 963/3000\n",
      "18/18 - 1s - loss: 0.9815 - val_loss: 1.2907\n",
      "\n",
      "Epoch 00963: val_loss did not improve from 0.93061\n",
      "Epoch 964/3000\n",
      "18/18 - 1s - loss: 1.1044 - val_loss: 1.3182\n",
      "\n",
      "Epoch 00964: val_loss did not improve from 0.93061\n",
      "Epoch 965/3000\n",
      "18/18 - 1s - loss: 0.9557 - val_loss: 1.0575\n",
      "\n",
      "Epoch 00965: val_loss did not improve from 0.93061\n",
      "Epoch 966/3000\n",
      "18/18 - 1s - loss: 1.0074 - val_loss: 1.1895\n",
      "\n",
      "Epoch 00966: val_loss did not improve from 0.93061\n",
      "Epoch 967/3000\n",
      "18/18 - 1s - loss: 1.0933 - val_loss: 0.9746\n",
      "\n",
      "Epoch 00967: val_loss did not improve from 0.93061\n",
      "Epoch 968/3000\n",
      "18/18 - 1s - loss: 1.0682 - val_loss: 0.9657\n",
      "\n",
      "Epoch 00968: val_loss did not improve from 0.93061\n",
      "Epoch 969/3000\n",
      "18/18 - 1s - loss: 1.0291 - val_loss: 1.1079\n",
      "\n",
      "Epoch 00969: val_loss did not improve from 0.93061\n",
      "Epoch 970/3000\n",
      "18/18 - 1s - loss: 1.2052 - val_loss: 1.0598\n",
      "\n",
      "Epoch 00970: val_loss did not improve from 0.93061\n",
      "Epoch 971/3000\n",
      "18/18 - 1s - loss: 0.9594 - val_loss: 0.9580\n",
      "\n",
      "Epoch 00971: val_loss did not improve from 0.93061\n",
      "Epoch 972/3000\n",
      "18/18 - 1s - loss: 1.2588 - val_loss: 0.9845\n",
      "\n",
      "Epoch 00972: val_loss did not improve from 0.93061\n",
      "Epoch 973/3000\n",
      "18/18 - 1s - loss: 1.0500 - val_loss: 0.9905\n",
      "\n",
      "Epoch 00973: val_loss did not improve from 0.93061\n",
      "Epoch 974/3000\n",
      "18/18 - 1s - loss: 0.9587 - val_loss: 1.1721\n",
      "\n",
      "Epoch 00974: val_loss did not improve from 0.93061\n",
      "Epoch 975/3000\n",
      "18/18 - 1s - loss: 1.0354 - val_loss: 1.1349\n",
      "\n",
      "Epoch 00975: val_loss did not improve from 0.93061\n",
      "Epoch 976/3000\n",
      "18/18 - 1s - loss: 1.1766 - val_loss: 1.1782\n",
      "\n",
      "Epoch 00976: val_loss did not improve from 0.93061\n",
      "Epoch 977/3000\n",
      "18/18 - 1s - loss: 1.1333 - val_loss: 1.2171\n",
      "\n",
      "Epoch 00977: val_loss did not improve from 0.93061\n",
      "Epoch 978/3000\n",
      "18/18 - 1s - loss: 1.0107 - val_loss: 0.9805\n",
      "\n",
      "Epoch 00978: val_loss did not improve from 0.93061\n",
      "Epoch 979/3000\n",
      "18/18 - 1s - loss: 0.9578 - val_loss: 1.4377\n",
      "\n",
      "Epoch 00979: val_loss did not improve from 0.93061\n",
      "Epoch 980/3000\n",
      "18/18 - 1s - loss: 1.1038 - val_loss: 0.9628\n",
      "\n",
      "Epoch 00980: val_loss did not improve from 0.93061\n",
      "Epoch 981/3000\n",
      "18/18 - 1s - loss: 1.0690 - val_loss: 1.3342\n",
      "\n",
      "Epoch 00981: val_loss did not improve from 0.93061\n",
      "Epoch 982/3000\n",
      "18/18 - 1s - loss: 1.2594 - val_loss: 1.0100\n",
      "\n",
      "Epoch 00982: val_loss did not improve from 0.93061\n",
      "Epoch 983/3000\n",
      "18/18 - 1s - loss: 0.9955 - val_loss: 1.1622\n",
      "\n",
      "Epoch 00983: val_loss did not improve from 0.93061\n",
      "Epoch 984/3000\n",
      "18/18 - 1s - loss: 0.9707 - val_loss: 1.2590\n",
      "\n",
      "Epoch 00984: val_loss did not improve from 0.93061\n",
      "Epoch 985/3000\n",
      "18/18 - 1s - loss: 1.0579 - val_loss: 0.9680\n",
      "\n",
      "Epoch 00985: val_loss did not improve from 0.93061\n",
      "Epoch 986/3000\n",
      "18/18 - 1s - loss: 1.0432 - val_loss: 1.0459\n",
      "\n",
      "Epoch 00986: val_loss did not improve from 0.93061\n",
      "Epoch 987/3000\n",
      "18/18 - 1s - loss: 0.9734 - val_loss: 0.9474\n",
      "\n",
      "Epoch 00987: val_loss did not improve from 0.93061\n",
      "Epoch 988/3000\n",
      "18/18 - 1s - loss: 1.1423 - val_loss: 0.9536\n",
      "\n",
      "Epoch 00988: val_loss did not improve from 0.93061\n",
      "Epoch 989/3000\n",
      "18/18 - 1s - loss: 0.9500 - val_loss: 1.0072\n",
      "\n",
      "Epoch 00989: val_loss did not improve from 0.93061\n",
      "Epoch 990/3000\n",
      "18/18 - 1s - loss: 0.9726 - val_loss: 1.4904\n",
      "\n",
      "Epoch 00990: val_loss did not improve from 0.93061\n",
      "Epoch 991/3000\n",
      "18/18 - 1s - loss: 1.3399 - val_loss: 1.1415\n",
      "\n",
      "Epoch 00991: val_loss did not improve from 0.93061\n",
      "Epoch 992/3000\n",
      "18/18 - 1s - loss: 0.9544 - val_loss: 0.9472\n",
      "\n",
      "Epoch 00992: val_loss did not improve from 0.93061\n",
      "Epoch 993/3000\n",
      "18/18 - 1s - loss: 0.9465 - val_loss: 0.9414\n",
      "\n",
      "Epoch 00993: val_loss did not improve from 0.93061\n",
      "Epoch 994/3000\n",
      "18/18 - 1s - loss: 1.0016 - val_loss: 1.0975\n",
      "\n",
      "Epoch 00994: val_loss did not improve from 0.93061\n",
      "Epoch 995/3000\n",
      "18/18 - 1s - loss: 0.9432 - val_loss: 1.7057\n",
      "\n",
      "Epoch 00995: val_loss did not improve from 0.93061\n",
      "Epoch 996/3000\n",
      "18/18 - 1s - loss: 1.1243 - val_loss: 1.0382\n",
      "\n",
      "Epoch 00996: val_loss did not improve from 0.93061\n",
      "Epoch 997/3000\n",
      "18/18 - 1s - loss: 0.9977 - val_loss: 0.9671\n",
      "\n",
      "Epoch 00997: val_loss did not improve from 0.93061\n",
      "Epoch 998/3000\n",
      "18/18 - 1s - loss: 1.0114 - val_loss: 0.9656\n",
      "\n",
      "Epoch 00998: val_loss did not improve from 0.93061\n",
      "Epoch 999/3000\n",
      "18/18 - 1s - loss: 0.9563 - val_loss: 0.9560\n",
      "\n",
      "Epoch 00999: val_loss did not improve from 0.93061\n",
      "Epoch 1000/3000\n",
      "18/18 - 1s - loss: 1.0334 - val_loss: 0.9731\n",
      "\n",
      "Epoch 01000: val_loss did not improve from 0.93061\n",
      "Epoch 1001/3000\n",
      "18/18 - 1s - loss: 1.0739 - val_loss: 2.6478\n",
      "\n",
      "Epoch 01001: val_loss did not improve from 0.93061\n",
      "Epoch 1002/3000\n",
      "18/18 - 1s - loss: 1.0370 - val_loss: 1.4962\n",
      "\n",
      "Epoch 01002: val_loss did not improve from 0.93061\n",
      "Epoch 1003/3000\n",
      "18/18 - 1s - loss: 0.9994 - val_loss: 1.2395\n",
      "\n",
      "Epoch 01003: val_loss did not improve from 0.93061\n",
      "Epoch 1004/3000\n",
      "18/18 - 1s - loss: 1.0380 - val_loss: 0.9876\n",
      "\n",
      "Epoch 01004: val_loss did not improve from 0.93061\n",
      "Epoch 1005/3000\n",
      "18/18 - 1s - loss: 1.0815 - val_loss: 1.0106\n",
      "\n",
      "Epoch 01005: val_loss did not improve from 0.93061\n",
      "Epoch 1006/3000\n",
      "18/18 - 1s - loss: 1.0586 - val_loss: 0.9833\n",
      "\n",
      "Epoch 01006: val_loss did not improve from 0.93061\n",
      "Epoch 1007/3000\n",
      "18/18 - 1s - loss: 0.9433 - val_loss: 0.9481\n",
      "\n",
      "Epoch 01007: val_loss did not improve from 0.93061\n",
      "Epoch 1008/3000\n",
      "18/18 - 1s - loss: 0.9459 - val_loss: 0.9712\n",
      "\n",
      "Epoch 01008: val_loss did not improve from 0.93061\n",
      "Epoch 1009/3000\n",
      "18/18 - 1s - loss: 1.0528 - val_loss: 0.9677\n",
      "\n",
      "Epoch 01009: val_loss did not improve from 0.93061\n",
      "Epoch 1010/3000\n",
      "18/18 - 1s - loss: 1.0166 - val_loss: 1.2423\n",
      "\n",
      "Epoch 01010: val_loss did not improve from 0.93061\n",
      "Epoch 1011/3000\n",
      "18/18 - 1s - loss: 1.0487 - val_loss: 1.0084\n",
      "\n",
      "Epoch 01011: val_loss did not improve from 0.93061\n",
      "Epoch 1012/3000\n",
      "18/18 - 1s - loss: 0.9705 - val_loss: 0.9907\n",
      "\n",
      "Epoch 01012: val_loss did not improve from 0.93061\n",
      "Epoch 1013/3000\n",
      "18/18 - 1s - loss: 1.0809 - val_loss: 1.1194\n",
      "\n",
      "Epoch 01013: val_loss did not improve from 0.93061\n",
      "Epoch 1014/3000\n",
      "18/18 - 1s - loss: 0.9552 - val_loss: 0.9473\n",
      "\n",
      "Epoch 01014: val_loss did not improve from 0.93061\n",
      "Epoch 1015/3000\n",
      "18/18 - 1s - loss: 1.0777 - val_loss: 0.9670\n",
      "\n",
      "Epoch 01015: val_loss did not improve from 0.93061\n",
      "Epoch 1016/3000\n",
      "18/18 - 1s - loss: 1.1337 - val_loss: 0.9952\n",
      "\n",
      "Epoch 01016: val_loss did not improve from 0.93061\n",
      "Epoch 1017/3000\n",
      "18/18 - 1s - loss: 1.0630 - val_loss: 1.1000\n",
      "\n",
      "Epoch 01017: val_loss did not improve from 0.93061\n",
      "Epoch 1018/3000\n",
      "18/18 - 1s - loss: 1.0685 - val_loss: 1.0328\n",
      "\n",
      "Epoch 01018: val_loss did not improve from 0.93061\n",
      "Epoch 1019/3000\n",
      "18/18 - 1s - loss: 0.9595 - val_loss: 0.9843\n",
      "\n",
      "Epoch 01019: val_loss did not improve from 0.93061\n",
      "Epoch 1020/3000\n",
      "18/18 - 1s - loss: 1.0650 - val_loss: 0.9398\n",
      "\n",
      "Epoch 01020: val_loss did not improve from 0.93061\n",
      "Epoch 1021/3000\n",
      "18/18 - 1s - loss: 0.9551 - val_loss: 1.2387\n",
      "\n",
      "Epoch 01021: val_loss did not improve from 0.93061\n",
      "Epoch 1022/3000\n",
      "18/18 - 1s - loss: 1.0878 - val_loss: 1.2367\n",
      "\n",
      "Epoch 01022: val_loss did not improve from 0.93061\n",
      "Epoch 1023/3000\n",
      "18/18 - 1s - loss: 0.9652 - val_loss: 1.1600\n",
      "\n",
      "Epoch 01023: val_loss did not improve from 0.93061\n",
      "Epoch 1024/3000\n",
      "18/18 - 1s - loss: 0.9803 - val_loss: 1.8669\n",
      "\n",
      "Epoch 01024: val_loss did not improve from 0.93061\n",
      "Epoch 1025/3000\n",
      "18/18 - 1s - loss: 1.0189 - val_loss: 0.9456\n",
      "\n",
      "Epoch 01025: val_loss did not improve from 0.93061\n",
      "Epoch 1026/3000\n",
      "18/18 - 1s - loss: 1.1403 - val_loss: 1.1221\n",
      "\n",
      "Epoch 01026: val_loss did not improve from 0.93061\n",
      "Epoch 1027/3000\n",
      "18/18 - 1s - loss: 1.1638 - val_loss: 1.1910\n",
      "\n",
      "Epoch 01027: val_loss did not improve from 0.93061\n",
      "Epoch 1028/3000\n",
      "18/18 - 1s - loss: 1.3111 - val_loss: 2.7222\n",
      "\n",
      "Epoch 01028: val_loss did not improve from 0.93061\n",
      "Epoch 1029/3000\n",
      "18/18 - 1s - loss: 1.0866 - val_loss: 0.9666\n",
      "\n",
      "Epoch 01029: val_loss did not improve from 0.93061\n",
      "Epoch 1030/3000\n",
      "18/18 - 1s - loss: 0.9975 - val_loss: 0.9878\n",
      "\n",
      "Epoch 01030: val_loss did not improve from 0.93061\n",
      "Epoch 1031/3000\n",
      "18/18 - 1s - loss: 1.0114 - val_loss: 0.9273\n",
      "\n",
      "Epoch 01031: val_loss improved from 0.93061 to 0.92726, saving model to qkeras_weights.h5\n",
      "Epoch 1032/3000\n",
      "18/18 - 1s - loss: 1.0040 - val_loss: 1.0182\n",
      "\n",
      "Epoch 01032: val_loss did not improve from 0.92726\n",
      "Epoch 1033/3000\n",
      "18/18 - 1s - loss: 1.1438 - val_loss: 0.9727\n",
      "\n",
      "Epoch 01033: val_loss did not improve from 0.92726\n",
      "Epoch 1034/3000\n",
      "18/18 - 1s - loss: 0.9524 - val_loss: 0.9365\n",
      "\n",
      "Epoch 01034: val_loss did not improve from 0.92726\n",
      "Epoch 1035/3000\n",
      "18/18 - 1s - loss: 1.1225 - val_loss: 1.6068\n",
      "\n",
      "Epoch 01035: val_loss did not improve from 0.92726\n",
      "Epoch 1036/3000\n",
      "18/18 - 1s - loss: 0.9994 - val_loss: 1.3217\n",
      "\n",
      "Epoch 01036: val_loss did not improve from 0.92726\n",
      "Epoch 1037/3000\n",
      "18/18 - 1s - loss: 1.0238 - val_loss: 1.2291\n",
      "\n",
      "Epoch 01037: val_loss did not improve from 0.92726\n",
      "Epoch 1038/3000\n",
      "18/18 - 1s - loss: 1.2313 - val_loss: 3.1649\n",
      "\n",
      "Epoch 01038: val_loss did not improve from 0.92726\n",
      "Epoch 1039/3000\n",
      "18/18 - 1s - loss: 1.4425 - val_loss: 1.8554\n",
      "\n",
      "Epoch 01039: val_loss did not improve from 0.92726\n",
      "Epoch 1040/3000\n",
      "18/18 - 1s - loss: 1.1155 - val_loss: 1.5541\n",
      "\n",
      "Epoch 01040: val_loss did not improve from 0.92726\n",
      "Epoch 1041/3000\n",
      "18/18 - 1s - loss: 1.0964 - val_loss: 1.1295\n",
      "\n",
      "Epoch 01041: val_loss did not improve from 0.92726\n",
      "Epoch 1042/3000\n",
      "18/18 - 1s - loss: 1.0158 - val_loss: 1.0191\n",
      "\n",
      "Epoch 01042: val_loss did not improve from 0.92726\n",
      "Epoch 1043/3000\n",
      "18/18 - 1s - loss: 1.0423 - val_loss: 1.1236\n",
      "\n",
      "Epoch 01043: val_loss did not improve from 0.92726\n",
      "Epoch 1044/3000\n",
      "18/18 - 1s - loss: 1.0887 - val_loss: 1.0186\n",
      "\n",
      "Epoch 01044: val_loss did not improve from 0.92726\n",
      "Epoch 1045/3000\n",
      "18/18 - 1s - loss: 1.1166 - val_loss: 1.8573\n",
      "\n",
      "Epoch 01045: val_loss did not improve from 0.92726\n",
      "Epoch 1046/3000\n",
      "18/18 - 1s - loss: 1.0215 - val_loss: 1.0589\n",
      "\n",
      "Epoch 01046: val_loss did not improve from 0.92726\n",
      "Epoch 1047/3000\n",
      "18/18 - 1s - loss: 1.0147 - val_loss: 1.0624\n",
      "\n",
      "Epoch 01047: val_loss did not improve from 0.92726\n",
      "Epoch 1048/3000\n",
      "18/18 - 1s - loss: 1.0515 - val_loss: 1.0329\n",
      "\n",
      "Epoch 01048: val_loss did not improve from 0.92726\n",
      "Epoch 1049/3000\n",
      "18/18 - 1s - loss: 0.9927 - val_loss: 0.9988\n",
      "\n",
      "Epoch 01049: val_loss did not improve from 0.92726\n",
      "Epoch 1050/3000\n",
      "18/18 - 1s - loss: 0.9870 - val_loss: 1.3051\n",
      "\n",
      "Epoch 01050: val_loss did not improve from 0.92726\n",
      "Epoch 1051/3000\n",
      "18/18 - 1s - loss: 1.0709 - val_loss: 1.0323\n",
      "\n",
      "Epoch 01051: val_loss did not improve from 0.92726\n",
      "Epoch 1052/3000\n",
      "18/18 - 1s - loss: 0.9813 - val_loss: 1.7635\n",
      "\n",
      "Epoch 01052: val_loss did not improve from 0.92726\n",
      "Epoch 1053/3000\n",
      "18/18 - 1s - loss: 1.1544 - val_loss: 1.1990\n",
      "\n",
      "Epoch 01053: val_loss did not improve from 0.92726\n",
      "Epoch 1054/3000\n",
      "18/18 - 1s - loss: 1.0989 - val_loss: 0.9879\n",
      "\n",
      "Epoch 01054: val_loss did not improve from 0.92726\n",
      "Epoch 1055/3000\n",
      "18/18 - 1s - loss: 0.9963 - val_loss: 1.0161\n",
      "\n",
      "Epoch 01055: val_loss did not improve from 0.92726\n",
      "Epoch 1056/3000\n",
      "18/18 - 1s - loss: 1.0951 - val_loss: 1.0597\n",
      "\n",
      "Epoch 01056: val_loss did not improve from 0.92726\n",
      "Epoch 1057/3000\n",
      "18/18 - 1s - loss: 1.0672 - val_loss: 1.0176\n",
      "\n",
      "Epoch 01057: val_loss did not improve from 0.92726\n",
      "Epoch 1058/3000\n",
      "18/18 - 1s - loss: 0.9941 - val_loss: 0.9884\n",
      "\n",
      "Epoch 01058: val_loss did not improve from 0.92726\n",
      "Epoch 1059/3000\n",
      "18/18 - 1s - loss: 0.9840 - val_loss: 2.3792\n",
      "\n",
      "Epoch 01059: val_loss did not improve from 0.92726\n",
      "Epoch 1060/3000\n",
      "18/18 - 1s - loss: 1.1586 - val_loss: 0.9868\n",
      "\n",
      "Epoch 01060: val_loss did not improve from 0.92726\n",
      "Epoch 1061/3000\n",
      "18/18 - 1s - loss: 1.1333 - val_loss: 0.9956\n",
      "\n",
      "Epoch 01061: val_loss did not improve from 0.92726\n",
      "Epoch 1062/3000\n",
      "18/18 - 1s - loss: 1.1309 - val_loss: 1.0801\n",
      "\n",
      "Epoch 01062: val_loss did not improve from 0.92726\n",
      "Epoch 1063/3000\n",
      "18/18 - 1s - loss: 1.1562 - val_loss: 1.0348\n",
      "\n",
      "Epoch 01063: val_loss did not improve from 0.92726\n",
      "Epoch 1064/3000\n",
      "18/18 - 1s - loss: 1.2487 - val_loss: 1.2684\n",
      "\n",
      "Epoch 01064: val_loss did not improve from 0.92726\n",
      "Epoch 1065/3000\n",
      "18/18 - 1s - loss: 1.4954 - val_loss: 1.1516\n",
      "\n",
      "Epoch 01065: val_loss did not improve from 0.92726\n",
      "Epoch 1066/3000\n",
      "18/18 - 1s - loss: 1.0874 - val_loss: 1.0980\n",
      "\n",
      "Epoch 01066: val_loss did not improve from 0.92726\n",
      "Epoch 1067/3000\n",
      "18/18 - 1s - loss: 1.0264 - val_loss: 1.0888\n",
      "\n",
      "Epoch 01067: val_loss did not improve from 0.92726\n",
      "Epoch 1068/3000\n",
      "18/18 - 1s - loss: 1.0224 - val_loss: 2.0293\n",
      "\n",
      "Epoch 01068: val_loss did not improve from 0.92726\n",
      "Epoch 1069/3000\n",
      "18/18 - 1s - loss: 1.0393 - val_loss: 1.6643\n",
      "\n",
      "Epoch 01069: val_loss did not improve from 0.92726\n",
      "Epoch 1070/3000\n",
      "18/18 - 1s - loss: 1.1971 - val_loss: 1.1555\n",
      "\n",
      "Epoch 01070: val_loss did not improve from 0.92726\n",
      "Epoch 1071/3000\n",
      "18/18 - 1s - loss: 1.1192 - val_loss: 1.0141\n",
      "\n",
      "Epoch 01071: val_loss did not improve from 0.92726\n",
      "Epoch 1072/3000\n",
      "18/18 - 1s - loss: 1.0023 - val_loss: 1.0963\n",
      "\n",
      "Epoch 01072: val_loss did not improve from 0.92726\n",
      "Epoch 1073/3000\n",
      "18/18 - 1s - loss: 0.9978 - val_loss: 1.6809\n",
      "\n",
      "Epoch 01073: val_loss did not improve from 0.92726\n",
      "Epoch 1074/3000\n",
      "18/18 - 1s - loss: 1.0587 - val_loss: 1.0130\n",
      "\n",
      "Epoch 01074: val_loss did not improve from 0.92726\n",
      "Epoch 1075/3000\n",
      "18/18 - 1s - loss: 1.1091 - val_loss: 1.0030\n",
      "\n",
      "Epoch 01075: val_loss did not improve from 0.92726\n",
      "Epoch 1076/3000\n",
      "18/18 - 1s - loss: 1.0041 - val_loss: 1.0783\n",
      "\n",
      "Epoch 01076: val_loss did not improve from 0.92726\n",
      "Epoch 1077/3000\n",
      "18/18 - 1s - loss: 1.0033 - val_loss: 1.0398\n",
      "\n",
      "Epoch 01077: val_loss did not improve from 0.92726\n",
      "Epoch 1078/3000\n",
      "18/18 - 1s - loss: 1.0754 - val_loss: 1.4629\n",
      "\n",
      "Epoch 01078: val_loss did not improve from 0.92726\n",
      "Epoch 1079/3000\n",
      "18/18 - 1s - loss: 1.0168 - val_loss: 1.0018\n",
      "\n",
      "Epoch 01079: val_loss did not improve from 0.92726\n",
      "Epoch 1080/3000\n",
      "18/18 - 1s - loss: 1.0283 - val_loss: 1.0248\n",
      "\n",
      "Epoch 01080: val_loss did not improve from 0.92726\n",
      "Epoch 1081/3000\n",
      "18/18 - 1s - loss: 0.9635 - val_loss: 0.9785\n",
      "\n",
      "Epoch 01081: val_loss did not improve from 0.92726\n",
      "Epoch 1082/3000\n",
      "18/18 - 1s - loss: 0.9620 - val_loss: 1.5051\n",
      "\n",
      "Epoch 01082: val_loss did not improve from 0.92726\n",
      "Epoch 1083/3000\n",
      "18/18 - 1s - loss: 1.1718 - val_loss: 1.0735\n",
      "\n",
      "Epoch 01083: val_loss did not improve from 0.92726\n",
      "Epoch 1084/3000\n",
      "18/18 - 1s - loss: 1.0003 - val_loss: 1.0961\n",
      "\n",
      "Epoch 01084: val_loss did not improve from 0.92726\n",
      "Epoch 1085/3000\n",
      "18/18 - 1s - loss: 1.0620 - val_loss: 1.5677\n",
      "\n",
      "Epoch 01085: val_loss did not improve from 0.92726\n",
      "Epoch 1086/3000\n",
      "18/18 - 1s - loss: 0.9648 - val_loss: 0.9895\n",
      "\n",
      "Epoch 01086: val_loss did not improve from 0.92726\n",
      "Epoch 1087/3000\n",
      "18/18 - 1s - loss: 1.0553 - val_loss: 1.2872\n",
      "\n",
      "Epoch 01087: val_loss did not improve from 0.92726\n",
      "Epoch 1088/3000\n",
      "18/18 - 1s - loss: 1.0446 - val_loss: 1.0128\n",
      "\n",
      "Epoch 01088: val_loss did not improve from 0.92726\n",
      "Epoch 1089/3000\n",
      "18/18 - 1s - loss: 0.9746 - val_loss: 0.9801\n",
      "\n",
      "Epoch 01089: val_loss did not improve from 0.92726\n",
      "Epoch 1090/3000\n",
      "18/18 - 1s - loss: 0.9596 - val_loss: 0.9618\n",
      "\n",
      "Epoch 01090: val_loss did not improve from 0.92726\n",
      "Epoch 1091/3000\n",
      "18/18 - 1s - loss: 1.0843 - val_loss: 0.9708\n",
      "\n",
      "Epoch 01091: val_loss did not improve from 0.92726\n",
      "Epoch 1092/3000\n",
      "18/18 - 1s - loss: 0.9464 - val_loss: 0.9748\n",
      "\n",
      "Epoch 01092: val_loss did not improve from 0.92726\n",
      "Epoch 1093/3000\n",
      "18/18 - 1s - loss: 1.0224 - val_loss: 0.9726\n",
      "\n",
      "Epoch 01093: val_loss did not improve from 0.92726\n",
      "Epoch 1094/3000\n",
      "18/18 - 1s - loss: 0.9573 - val_loss: 0.9462\n",
      "\n",
      "Epoch 01094: val_loss did not improve from 0.92726\n",
      "Epoch 1095/3000\n",
      "18/18 - 1s - loss: 1.0197 - val_loss: 0.9470\n",
      "\n",
      "Epoch 01095: val_loss did not improve from 0.92726\n",
      "Epoch 1096/3000\n",
      "18/18 - 1s - loss: 0.9804 - val_loss: 1.1598\n",
      "\n",
      "Epoch 01096: val_loss did not improve from 0.92726\n",
      "Epoch 1097/3000\n",
      "18/18 - 1s - loss: 1.1049 - val_loss: 0.9874\n",
      "\n",
      "Epoch 01097: val_loss did not improve from 0.92726\n",
      "Epoch 1098/3000\n",
      "18/18 - 1s - loss: 1.0911 - val_loss: 2.6551\n",
      "\n",
      "Epoch 01098: val_loss did not improve from 0.92726\n",
      "Epoch 1099/3000\n",
      "18/18 - 1s - loss: 1.1916 - val_loss: 0.9934\n",
      "\n",
      "Epoch 01099: val_loss did not improve from 0.92726\n",
      "Epoch 1100/3000\n",
      "18/18 - 1s - loss: 0.9649 - val_loss: 0.9501\n",
      "\n",
      "Epoch 01100: val_loss did not improve from 0.92726\n",
      "Epoch 1101/3000\n",
      "18/18 - 1s - loss: 1.1435 - val_loss: 1.1391\n",
      "\n",
      "Epoch 01101: val_loss did not improve from 0.92726\n",
      "Epoch 1102/3000\n",
      "18/18 - 1s - loss: 1.0633 - val_loss: 1.3622\n",
      "\n",
      "Epoch 01102: val_loss did not improve from 0.92726\n",
      "Epoch 1103/3000\n",
      "18/18 - 1s - loss: 0.9767 - val_loss: 1.4806\n",
      "\n",
      "Epoch 01103: val_loss did not improve from 0.92726\n",
      "Epoch 1104/3000\n",
      "18/18 - 1s - loss: 1.1565 - val_loss: 1.0828\n",
      "\n",
      "Epoch 01104: val_loss did not improve from 0.92726\n",
      "Epoch 1105/3000\n",
      "18/18 - 1s - loss: 1.1075 - val_loss: 1.6583\n",
      "\n",
      "Epoch 01105: val_loss did not improve from 0.92726\n",
      "Epoch 1106/3000\n",
      "18/18 - 1s - loss: 1.2002 - val_loss: 1.0261\n",
      "\n",
      "Epoch 01106: val_loss did not improve from 0.92726\n",
      "Epoch 1107/3000\n",
      "18/18 - 1s - loss: 1.1250 - val_loss: 1.1155\n",
      "\n",
      "Epoch 01107: val_loss did not improve from 0.92726\n",
      "Epoch 1108/3000\n",
      "18/18 - 1s - loss: 0.9620 - val_loss: 1.0056\n",
      "\n",
      "Epoch 01108: val_loss did not improve from 0.92726\n",
      "Epoch 1109/3000\n",
      "18/18 - 1s - loss: 1.0356 - val_loss: 0.9997\n",
      "\n",
      "Epoch 01109: val_loss did not improve from 0.92726\n",
      "Epoch 1110/3000\n",
      "18/18 - 1s - loss: 0.9670 - val_loss: 0.9824\n",
      "\n",
      "Epoch 01110: val_loss did not improve from 0.92726\n",
      "Epoch 1111/3000\n",
      "18/18 - 1s - loss: 0.9801 - val_loss: 0.9705\n",
      "\n",
      "Epoch 01111: val_loss did not improve from 0.92726\n",
      "Epoch 1112/3000\n",
      "18/18 - 1s - loss: 1.2578 - val_loss: 1.1903\n",
      "\n",
      "Epoch 01112: val_loss did not improve from 0.92726\n",
      "Epoch 1113/3000\n",
      "18/18 - 1s - loss: 0.9771 - val_loss: 0.9779\n",
      "\n",
      "Epoch 01113: val_loss did not improve from 0.92726\n",
      "Epoch 1114/3000\n",
      "18/18 - 1s - loss: 0.9923 - val_loss: 0.9440\n",
      "\n",
      "Epoch 01114: val_loss did not improve from 0.92726\n",
      "Epoch 1115/3000\n",
      "18/18 - 1s - loss: 1.0014 - val_loss: 0.9521\n",
      "\n",
      "Epoch 01115: val_loss did not improve from 0.92726\n",
      "Epoch 1116/3000\n",
      "18/18 - 1s - loss: 1.2002 - val_loss: 0.9596\n",
      "\n",
      "Epoch 01116: val_loss did not improve from 0.92726\n",
      "Epoch 1117/3000\n",
      "18/18 - 1s - loss: 1.0441 - val_loss: 0.9443\n",
      "\n",
      "Epoch 01117: val_loss did not improve from 0.92726\n",
      "Epoch 1118/3000\n",
      "18/18 - 1s - loss: 0.9554 - val_loss: 1.0289\n",
      "\n",
      "Epoch 01118: val_loss did not improve from 0.92726\n",
      "Epoch 1119/3000\n",
      "18/18 - 1s - loss: 1.0889 - val_loss: 0.9590\n",
      "\n",
      "Epoch 01119: val_loss did not improve from 0.92726\n",
      "Epoch 1120/3000\n",
      "18/18 - 1s - loss: 1.0762 - val_loss: 1.9935\n",
      "\n",
      "Epoch 01120: val_loss did not improve from 0.92726\n",
      "Epoch 1121/3000\n",
      "18/18 - 1s - loss: 1.1454 - val_loss: 0.9485\n",
      "\n",
      "Epoch 01121: val_loss did not improve from 0.92726\n",
      "Epoch 1122/3000\n",
      "18/18 - 1s - loss: 1.0993 - val_loss: 0.9825\n",
      "\n",
      "Epoch 01122: val_loss did not improve from 0.92726\n",
      "Epoch 1123/3000\n",
      "18/18 - 1s - loss: 1.2282 - val_loss: 1.0434\n",
      "\n",
      "Epoch 01123: val_loss did not improve from 0.92726\n",
      "Epoch 1124/3000\n",
      "18/18 - 1s - loss: 1.2163 - val_loss: 2.6847\n",
      "\n",
      "Epoch 01124: val_loss did not improve from 0.92726\n",
      "Epoch 1125/3000\n",
      "18/18 - 1s - loss: 1.3875 - val_loss: 1.2360\n",
      "\n",
      "Epoch 01125: val_loss did not improve from 0.92726\n",
      "Epoch 1126/3000\n",
      "18/18 - 1s - loss: 1.3904 - val_loss: 1.2696\n",
      "\n",
      "Epoch 01126: val_loss did not improve from 0.92726\n",
      "Epoch 1127/3000\n",
      "18/18 - 1s - loss: 1.1509 - val_loss: 1.0071\n",
      "\n",
      "Epoch 01127: val_loss did not improve from 0.92726\n",
      "Epoch 1128/3000\n",
      "18/18 - 1s - loss: 1.1390 - val_loss: 0.9803\n",
      "\n",
      "Epoch 01128: val_loss did not improve from 0.92726\n",
      "Epoch 1129/3000\n",
      "18/18 - 1s - loss: 1.3165 - val_loss: 1.1427\n",
      "\n",
      "Epoch 01129: val_loss did not improve from 0.92726\n",
      "Epoch 1130/3000\n",
      "18/18 - 1s - loss: 0.9896 - val_loss: 1.0041\n",
      "\n",
      "Epoch 01130: val_loss did not improve from 0.92726\n",
      "Epoch 1131/3000\n",
      "18/18 - 1s - loss: 1.0029 - val_loss: 1.6478\n",
      "\n",
      "Epoch 01131: val_loss did not improve from 0.92726\n",
      "Epoch 1132/3000\n",
      "18/18 - 1s - loss: 1.0142 - val_loss: 1.0026\n",
      "\n",
      "Epoch 01132: val_loss did not improve from 0.92726\n",
      "Epoch 1133/3000\n",
      "18/18 - 1s - loss: 1.1364 - val_loss: 1.1156\n",
      "\n",
      "Epoch 01133: val_loss did not improve from 0.92726\n",
      "Epoch 1134/3000\n",
      "18/18 - 1s - loss: 1.1129 - val_loss: 0.9751\n",
      "\n",
      "Epoch 01134: val_loss did not improve from 0.92726\n",
      "Epoch 1135/3000\n",
      "18/18 - 1s - loss: 0.9716 - val_loss: 1.0485\n",
      "\n",
      "Epoch 01135: val_loss did not improve from 0.92726\n",
      "Epoch 1136/3000\n",
      "18/18 - 1s - loss: 1.0696 - val_loss: 1.0208\n",
      "\n",
      "Epoch 01136: val_loss did not improve from 0.92726\n",
      "Epoch 1137/3000\n",
      "18/18 - 1s - loss: 0.9870 - val_loss: 0.9771\n",
      "\n",
      "Epoch 01137: val_loss did not improve from 0.92726\n",
      "Epoch 1138/3000\n",
      "18/18 - 1s - loss: 1.0395 - val_loss: 0.9424\n",
      "\n",
      "Epoch 01138: val_loss did not improve from 0.92726\n",
      "Epoch 1139/3000\n",
      "18/18 - 1s - loss: 1.1267 - val_loss: 0.9530\n",
      "\n",
      "Epoch 01139: val_loss did not improve from 0.92726\n",
      "Epoch 1140/3000\n",
      "18/18 - 1s - loss: 0.9686 - val_loss: 0.9742\n",
      "\n",
      "Epoch 01140: val_loss did not improve from 0.92726\n",
      "Epoch 1141/3000\n",
      "18/18 - 1s - loss: 1.1000 - val_loss: 0.9679\n",
      "\n",
      "Epoch 01141: val_loss did not improve from 0.92726\n",
      "Epoch 1142/3000\n",
      "18/18 - 1s - loss: 1.1179 - val_loss: 0.9926\n",
      "\n",
      "Epoch 01142: val_loss did not improve from 0.92726\n",
      "Epoch 1143/3000\n",
      "18/18 - 1s - loss: 1.1591 - val_loss: 0.9961\n",
      "\n",
      "Epoch 01143: val_loss did not improve from 0.92726\n",
      "Epoch 1144/3000\n",
      "18/18 - 1s - loss: 0.9827 - val_loss: 0.9880\n",
      "\n",
      "Epoch 01144: val_loss did not improve from 0.92726\n",
      "Epoch 1145/3000\n",
      "18/18 - 1s - loss: 1.0809 - val_loss: 0.9892\n",
      "\n",
      "Epoch 01145: val_loss did not improve from 0.92726\n",
      "Epoch 1146/3000\n",
      "18/18 - 1s - loss: 1.0684 - val_loss: 0.9873\n",
      "\n",
      "Epoch 01146: val_loss did not improve from 0.92726\n",
      "Epoch 1147/3000\n",
      "18/18 - 1s - loss: 1.0842 - val_loss: 1.0250\n",
      "\n",
      "Epoch 01147: val_loss did not improve from 0.92726\n",
      "Epoch 1148/3000\n",
      "18/18 - 1s - loss: 0.9848 - val_loss: 1.0279\n",
      "\n",
      "Epoch 01148: val_loss did not improve from 0.92726\n",
      "Epoch 1149/3000\n",
      "18/18 - 1s - loss: 0.9610 - val_loss: 0.9619\n",
      "\n",
      "Epoch 01149: val_loss did not improve from 0.92726\n",
      "Epoch 1150/3000\n",
      "18/18 - 1s - loss: 1.0294 - val_loss: 1.3597\n",
      "\n",
      "Epoch 01150: val_loss did not improve from 0.92726\n",
      "Epoch 1151/3000\n",
      "18/18 - 1s - loss: 0.9784 - val_loss: 0.9585\n",
      "\n",
      "Epoch 01151: val_loss did not improve from 0.92726\n",
      "Epoch 1152/3000\n",
      "18/18 - 1s - loss: 1.0471 - val_loss: 0.9410\n",
      "\n",
      "Epoch 01152: val_loss did not improve from 0.92726\n",
      "Epoch 1153/3000\n",
      "18/18 - 1s - loss: 0.9313 - val_loss: 1.2245\n",
      "\n",
      "Epoch 01153: val_loss did not improve from 0.92726\n",
      "Epoch 1154/3000\n",
      "18/18 - 1s - loss: 0.9833 - val_loss: 1.4274\n",
      "\n",
      "Epoch 01154: val_loss did not improve from 0.92726\n",
      "Epoch 1155/3000\n",
      "18/18 - 1s - loss: 1.0235 - val_loss: 0.9693\n",
      "\n",
      "Epoch 01155: val_loss did not improve from 0.92726\n",
      "Epoch 1156/3000\n",
      "18/18 - 1s - loss: 0.9885 - val_loss: 0.9851\n",
      "\n",
      "Epoch 01156: val_loss did not improve from 0.92726\n",
      "Epoch 1157/3000\n",
      "18/18 - 1s - loss: 1.1478 - val_loss: 0.9684\n",
      "\n",
      "Epoch 01157: val_loss did not improve from 0.92726\n",
      "Epoch 1158/3000\n",
      "18/18 - 1s - loss: 0.9791 - val_loss: 0.9731\n",
      "\n",
      "Epoch 01158: val_loss did not improve from 0.92726\n",
      "Epoch 1159/3000\n",
      "18/18 - 1s - loss: 1.0086 - val_loss: 0.9808\n",
      "\n",
      "Epoch 01159: val_loss did not improve from 0.92726\n",
      "Epoch 1160/3000\n",
      "18/18 - 1s - loss: 1.0396 - val_loss: 0.9604\n",
      "\n",
      "Epoch 01160: val_loss did not improve from 0.92726\n",
      "Epoch 1161/3000\n",
      "18/18 - 1s - loss: 1.0284 - val_loss: 1.3341\n",
      "\n",
      "Epoch 01161: val_loss did not improve from 0.92726\n",
      "Epoch 1162/3000\n",
      "18/18 - 1s - loss: 0.9928 - val_loss: 0.9662\n",
      "\n",
      "Epoch 01162: val_loss did not improve from 0.92726\n",
      "Epoch 1163/3000\n",
      "18/18 - 1s - loss: 1.0657 - val_loss: 0.9950\n",
      "\n",
      "Epoch 01163: val_loss did not improve from 0.92726\n",
      "Epoch 1164/3000\n",
      "18/18 - 1s - loss: 0.9413 - val_loss: 0.9849\n",
      "\n",
      "Epoch 01164: val_loss did not improve from 0.92726\n",
      "Epoch 1165/3000\n",
      "18/18 - 1s - loss: 0.9498 - val_loss: 1.7122\n",
      "\n",
      "Epoch 01165: val_loss did not improve from 0.92726\n",
      "Epoch 1166/3000\n",
      "18/18 - 1s - loss: 0.9693 - val_loss: 1.4745\n",
      "\n",
      "Epoch 01166: val_loss did not improve from 0.92726\n",
      "Epoch 1167/3000\n",
      "18/18 - 1s - loss: 1.0334 - val_loss: 0.9601\n",
      "\n",
      "Epoch 01167: val_loss did not improve from 0.92726\n",
      "Epoch 1168/3000\n",
      "18/18 - 1s - loss: 1.0517 - val_loss: 1.0032\n",
      "\n",
      "Epoch 01168: val_loss did not improve from 0.92726\n",
      "Epoch 1169/3000\n",
      "18/18 - 1s - loss: 0.9505 - val_loss: 1.2041\n",
      "\n",
      "Epoch 01169: val_loss did not improve from 0.92726\n",
      "Epoch 1170/3000\n",
      "18/18 - 1s - loss: 1.0041 - val_loss: 1.6941\n",
      "\n",
      "Epoch 01170: val_loss did not improve from 0.92726\n",
      "Epoch 1171/3000\n",
      "18/18 - 1s - loss: 1.1062 - val_loss: 1.0131\n",
      "\n",
      "Epoch 01171: val_loss did not improve from 0.92726\n",
      "Epoch 1172/3000\n",
      "18/18 - 1s - loss: 1.1752 - val_loss: 0.9818\n",
      "\n",
      "Epoch 01172: val_loss did not improve from 0.92726\n",
      "Epoch 1173/3000\n",
      "18/18 - 1s - loss: 1.1179 - val_loss: 1.0071\n",
      "\n",
      "Epoch 01173: val_loss did not improve from 0.92726\n",
      "Epoch 1174/3000\n",
      "18/18 - 1s - loss: 1.0058 - val_loss: 1.0074\n",
      "\n",
      "Epoch 01174: val_loss did not improve from 0.92726\n",
      "Epoch 1175/3000\n",
      "18/18 - 1s - loss: 0.9506 - val_loss: 1.3746\n",
      "\n",
      "Epoch 01175: val_loss did not improve from 0.92726\n",
      "Epoch 1176/3000\n",
      "18/18 - 1s - loss: 1.1819 - val_loss: 0.9980\n",
      "\n",
      "Epoch 01176: val_loss did not improve from 0.92726\n",
      "Epoch 1177/3000\n",
      "18/18 - 1s - loss: 1.0200 - val_loss: 0.9841\n",
      "\n",
      "Epoch 01177: val_loss did not improve from 0.92726\n",
      "Epoch 1178/3000\n",
      "18/18 - 1s - loss: 1.3058 - val_loss: 1.0022\n",
      "\n",
      "Epoch 01178: val_loss did not improve from 0.92726\n",
      "Epoch 1179/3000\n",
      "18/18 - 1s - loss: 0.9755 - val_loss: 0.9594\n",
      "\n",
      "Epoch 01179: val_loss did not improve from 0.92726\n",
      "Epoch 1180/3000\n",
      "18/18 - 1s - loss: 0.9732 - val_loss: 0.9380\n",
      "\n",
      "Epoch 01180: val_loss did not improve from 0.92726\n",
      "Epoch 1181/3000\n",
      "18/18 - 1s - loss: 1.0425 - val_loss: 1.0400\n",
      "\n",
      "Epoch 01181: val_loss did not improve from 0.92726\n",
      "Epoch 1182/3000\n",
      "18/18 - 1s - loss: 0.9768 - val_loss: 1.0239\n",
      "\n",
      "Epoch 01182: val_loss did not improve from 0.92726\n",
      "Epoch 1183/3000\n",
      "18/18 - 1s - loss: 1.0200 - val_loss: 0.9678\n",
      "\n",
      "Epoch 01183: val_loss did not improve from 0.92726\n",
      "Epoch 1184/3000\n",
      "18/18 - 1s - loss: 1.0225 - val_loss: 1.0204\n",
      "\n",
      "Epoch 01184: val_loss did not improve from 0.92726\n",
      "Epoch 1185/3000\n",
      "18/18 - 1s - loss: 1.0171 - val_loss: 0.9597\n",
      "\n",
      "Epoch 01185: val_loss did not improve from 0.92726\n",
      "Epoch 1186/3000\n",
      "18/18 - 1s - loss: 0.9399 - val_loss: 0.9630\n",
      "\n",
      "Epoch 01186: val_loss did not improve from 0.92726\n",
      "Epoch 1187/3000\n",
      "18/18 - 1s - loss: 1.0421 - val_loss: 0.9889\n",
      "\n",
      "Epoch 01187: val_loss did not improve from 0.92726\n",
      "Epoch 1188/3000\n",
      "18/18 - 1s - loss: 0.9797 - val_loss: 0.9478\n",
      "\n",
      "Epoch 01188: val_loss did not improve from 0.92726\n",
      "Epoch 1189/3000\n",
      "18/18 - 1s - loss: 1.1074 - val_loss: 0.9700\n",
      "\n",
      "Epoch 01189: val_loss did not improve from 0.92726\n",
      "Epoch 1190/3000\n",
      "18/18 - 1s - loss: 0.9660 - val_loss: 0.9743\n",
      "\n",
      "Epoch 01190: val_loss did not improve from 0.92726\n",
      "Epoch 1191/3000\n",
      "18/18 - 1s - loss: 1.0800 - val_loss: 1.0339\n",
      "\n",
      "Epoch 01191: val_loss did not improve from 0.92726\n",
      "Epoch 1192/3000\n",
      "18/18 - 1s - loss: 1.0073 - val_loss: 0.9764\n",
      "\n",
      "Epoch 01192: val_loss did not improve from 0.92726\n",
      "Epoch 1193/3000\n",
      "18/18 - 1s - loss: 0.9775 - val_loss: 1.4324\n",
      "\n",
      "Epoch 01193: val_loss did not improve from 0.92726\n",
      "Epoch 1194/3000\n",
      "18/18 - 1s - loss: 1.1569 - val_loss: 0.9240\n",
      "\n",
      "Epoch 01194: val_loss improved from 0.92726 to 0.92396, saving model to qkeras_weights.h5\n",
      "Epoch 1195/3000\n",
      "18/18 - 1s - loss: 1.0273 - val_loss: 0.9447\n",
      "\n",
      "Epoch 01195: val_loss did not improve from 0.92396\n",
      "Epoch 1196/3000\n",
      "18/18 - 1s - loss: 1.0309 - val_loss: 0.9520\n",
      "\n",
      "Epoch 01196: val_loss did not improve from 0.92396\n",
      "Epoch 1197/3000\n",
      "18/18 - 1s - loss: 0.9931 - val_loss: 1.0000\n",
      "\n",
      "Epoch 01197: val_loss did not improve from 0.92396\n",
      "Epoch 1198/3000\n",
      "18/18 - 1s - loss: 0.9351 - val_loss: 0.9278\n",
      "\n",
      "Epoch 01198: val_loss did not improve from 0.92396\n",
      "Epoch 1199/3000\n",
      "18/18 - 1s - loss: 1.1339 - val_loss: 1.4254\n",
      "\n",
      "Epoch 01199: val_loss did not improve from 0.92396\n",
      "Epoch 1200/3000\n",
      "18/18 - 1s - loss: 1.0781 - val_loss: 1.1056\n",
      "\n",
      "Epoch 01200: val_loss did not improve from 0.92396\n",
      "Epoch 1201/3000\n",
      "18/18 - 1s - loss: 1.3617 - val_loss: 1.1683\n",
      "\n",
      "Epoch 01201: val_loss did not improve from 0.92396\n",
      "Epoch 1202/3000\n",
      "18/18 - 1s - loss: 1.0858 - val_loss: 0.9991\n",
      "\n",
      "Epoch 01202: val_loss did not improve from 0.92396\n",
      "Epoch 1203/3000\n",
      "18/18 - 1s - loss: 1.0481 - val_loss: 0.9881\n",
      "\n",
      "Epoch 01203: val_loss did not improve from 0.92396\n",
      "Epoch 1204/3000\n",
      "18/18 - 1s - loss: 0.9689 - val_loss: 1.1898\n",
      "\n",
      "Epoch 01204: val_loss did not improve from 0.92396\n",
      "Epoch 1205/3000\n",
      "18/18 - 1s - loss: 1.2057 - val_loss: 1.0256\n",
      "\n",
      "Epoch 01205: val_loss did not improve from 0.92396\n",
      "Epoch 1206/3000\n",
      "18/18 - 1s - loss: 1.0666 - val_loss: 0.9732\n",
      "\n",
      "Epoch 01206: val_loss did not improve from 0.92396\n",
      "Epoch 1207/3000\n",
      "18/18 - 1s - loss: 1.2127 - val_loss: 1.0339\n",
      "\n",
      "Epoch 01207: val_loss did not improve from 0.92396\n",
      "Epoch 1208/3000\n",
      "18/18 - 1s - loss: 1.1667 - val_loss: 0.9618\n",
      "\n",
      "Epoch 01208: val_loss did not improve from 0.92396\n",
      "Epoch 1209/3000\n",
      "18/18 - 1s - loss: 0.9993 - val_loss: 2.0216\n",
      "\n",
      "Epoch 01209: val_loss did not improve from 0.92396\n",
      "Epoch 1210/3000\n",
      "18/18 - 1s - loss: 1.4154 - val_loss: 2.6995\n",
      "\n",
      "Epoch 01210: val_loss did not improve from 0.92396\n",
      "Epoch 1211/3000\n",
      "18/18 - 1s - loss: 1.3341 - val_loss: 0.9880\n",
      "\n",
      "Epoch 01211: val_loss did not improve from 0.92396\n",
      "Epoch 1212/3000\n",
      "18/18 - 1s - loss: 0.9851 - val_loss: 1.2750\n",
      "\n",
      "Epoch 01212: val_loss did not improve from 0.92396\n",
      "Epoch 1213/3000\n",
      "18/18 - 1s - loss: 1.3029 - val_loss: 1.1572\n",
      "\n",
      "Epoch 01213: val_loss did not improve from 0.92396\n",
      "Epoch 1214/3000\n",
      "18/18 - 1s - loss: 1.0227 - val_loss: 1.5519\n",
      "\n",
      "Epoch 01214: val_loss did not improve from 0.92396\n",
      "Epoch 1215/3000\n",
      "18/18 - 1s - loss: 1.4462 - val_loss: 2.7453\n",
      "\n",
      "Epoch 01215: val_loss did not improve from 0.92396\n",
      "Epoch 1216/3000\n",
      "18/18 - 1s - loss: 1.0390 - val_loss: 1.2833\n",
      "\n",
      "Epoch 01216: val_loss did not improve from 0.92396\n",
      "Epoch 1217/3000\n",
      "18/18 - 1s - loss: 1.0669 - val_loss: 0.9500\n",
      "\n",
      "Epoch 01217: val_loss did not improve from 0.92396\n",
      "Epoch 1218/3000\n",
      "18/18 - 1s - loss: 1.0033 - val_loss: 0.9411\n",
      "\n",
      "Epoch 01218: val_loss did not improve from 0.92396\n",
      "Epoch 1219/3000\n",
      "18/18 - 1s - loss: 1.0034 - val_loss: 0.9363\n",
      "\n",
      "Epoch 01219: val_loss did not improve from 0.92396\n",
      "Epoch 1220/3000\n",
      "18/18 - 1s - loss: 1.3054 - val_loss: 1.0042\n",
      "\n",
      "Epoch 01220: val_loss did not improve from 0.92396\n",
      "Epoch 1221/3000\n",
      "18/18 - 1s - loss: 1.1006 - val_loss: 0.9701\n",
      "\n",
      "Epoch 01221: val_loss did not improve from 0.92396\n",
      "Epoch 1222/3000\n",
      "18/18 - 1s - loss: 0.9983 - val_loss: 0.9916\n",
      "\n",
      "Epoch 01222: val_loss did not improve from 0.92396\n",
      "Epoch 1223/3000\n",
      "18/18 - 1s - loss: 0.9598 - val_loss: 0.9887\n",
      "\n",
      "Epoch 01223: val_loss did not improve from 0.92396\n",
      "Epoch 1224/3000\n",
      "18/18 - 1s - loss: 0.9972 - val_loss: 1.0729\n",
      "\n",
      "Epoch 01224: val_loss did not improve from 0.92396\n",
      "Epoch 1225/3000\n",
      "18/18 - 1s - loss: 1.1676 - val_loss: 0.9912\n",
      "\n",
      "Epoch 01225: val_loss did not improve from 0.92396\n",
      "Epoch 1226/3000\n",
      "18/18 - 1s - loss: 1.0763 - val_loss: 0.9964\n",
      "\n",
      "Epoch 01226: val_loss did not improve from 0.92396\n",
      "Epoch 1227/3000\n",
      "18/18 - 1s - loss: 1.1201 - val_loss: 1.0005\n",
      "\n",
      "Epoch 01227: val_loss did not improve from 0.92396\n",
      "Epoch 1228/3000\n",
      "18/18 - 1s - loss: 1.0349 - val_loss: 0.9916\n",
      "\n",
      "Epoch 01228: val_loss did not improve from 0.92396\n",
      "Epoch 1229/3000\n",
      "18/18 - 1s - loss: 1.2338 - val_loss: 0.9586\n",
      "\n",
      "Epoch 01229: val_loss did not improve from 0.92396\n",
      "Epoch 1230/3000\n",
      "18/18 - 1s - loss: 0.9887 - val_loss: 0.9461\n",
      "\n",
      "Epoch 01230: val_loss did not improve from 0.92396\n",
      "Epoch 1231/3000\n",
      "18/18 - 1s - loss: 1.0543 - val_loss: 0.9417\n",
      "\n",
      "Epoch 01231: val_loss did not improve from 0.92396\n",
      "Epoch 1232/3000\n",
      "18/18 - 1s - loss: 0.9564 - val_loss: 0.9892\n",
      "\n",
      "Epoch 01232: val_loss did not improve from 0.92396\n",
      "Epoch 1233/3000\n",
      "18/18 - 1s - loss: 1.4551 - val_loss: 1.4588\n",
      "\n",
      "Epoch 01233: val_loss did not improve from 0.92396\n",
      "Epoch 1234/3000\n",
      "18/18 - 1s - loss: 1.2203 - val_loss: 1.0046\n",
      "\n",
      "Epoch 01234: val_loss did not improve from 0.92396\n",
      "Epoch 1235/3000\n",
      "18/18 - 1s - loss: 0.9744 - val_loss: 1.2836\n",
      "\n",
      "Epoch 01235: val_loss did not improve from 0.92396\n",
      "Epoch 1236/3000\n",
      "18/18 - 1s - loss: 0.9577 - val_loss: 0.9559\n",
      "\n",
      "Epoch 01236: val_loss did not improve from 0.92396\n",
      "Epoch 1237/3000\n",
      "18/18 - 1s - loss: 1.0515 - val_loss: 1.3970\n",
      "\n",
      "Epoch 01237: val_loss did not improve from 0.92396\n",
      "Epoch 1238/3000\n",
      "18/18 - 1s - loss: 1.2076 - val_loss: 1.0993\n",
      "\n",
      "Epoch 01238: val_loss did not improve from 0.92396\n",
      "Epoch 1239/3000\n",
      "18/18 - 1s - loss: 1.0579 - val_loss: 0.9638\n",
      "\n",
      "Epoch 01239: val_loss did not improve from 0.92396\n",
      "Epoch 1240/3000\n",
      "18/18 - 1s - loss: 1.0025 - val_loss: 1.1838\n",
      "\n",
      "Epoch 01240: val_loss did not improve from 0.92396\n",
      "Epoch 1241/3000\n",
      "18/18 - 1s - loss: 1.0346 - val_loss: 0.9548\n",
      "\n",
      "Epoch 01241: val_loss did not improve from 0.92396\n",
      "Epoch 1242/3000\n",
      "18/18 - 1s - loss: 0.9907 - val_loss: 0.9647\n",
      "\n",
      "Epoch 01242: val_loss did not improve from 0.92396\n",
      "Epoch 1243/3000\n",
      "18/18 - 1s - loss: 1.0461 - val_loss: 0.9825\n",
      "\n",
      "Epoch 01243: val_loss did not improve from 0.92396\n",
      "Epoch 1244/3000\n",
      "18/18 - 1s - loss: 0.9503 - val_loss: 0.9923\n",
      "\n",
      "Epoch 01244: val_loss did not improve from 0.92396\n",
      "Epoch 1245/3000\n",
      "18/18 - 1s - loss: 1.0929 - val_loss: 0.9570\n",
      "\n",
      "Epoch 01245: val_loss did not improve from 0.92396\n",
      "Epoch 1246/3000\n",
      "18/18 - 1s - loss: 1.0812 - val_loss: 0.9360\n",
      "\n",
      "Epoch 01246: val_loss did not improve from 0.92396\n",
      "Epoch 1247/3000\n",
      "18/18 - 1s - loss: 0.9476 - val_loss: 0.9775\n",
      "\n",
      "Epoch 01247: val_loss did not improve from 0.92396\n",
      "Epoch 1248/3000\n",
      "18/18 - 1s - loss: 1.0196 - val_loss: 0.9672\n",
      "\n",
      "Epoch 01248: val_loss did not improve from 0.92396\n",
      "Epoch 1249/3000\n",
      "18/18 - 1s - loss: 1.0164 - val_loss: 1.0604\n",
      "\n",
      "Epoch 01249: val_loss did not improve from 0.92396\n",
      "Epoch 1250/3000\n",
      "18/18 - 1s - loss: 1.0478 - val_loss: 0.9509\n",
      "\n",
      "Epoch 01250: val_loss did not improve from 0.92396\n",
      "Epoch 1251/3000\n",
      "18/18 - 1s - loss: 1.0227 - val_loss: 0.9693\n",
      "\n",
      "Epoch 01251: val_loss did not improve from 0.92396\n",
      "Epoch 1252/3000\n",
      "18/18 - 1s - loss: 1.0868 - val_loss: 0.9781\n",
      "\n",
      "Epoch 01252: val_loss did not improve from 0.92396\n",
      "Epoch 1253/3000\n",
      "18/18 - 1s - loss: 1.0762 - val_loss: 0.9406\n",
      "\n",
      "Epoch 01253: val_loss did not improve from 0.92396\n",
      "Epoch 1254/3000\n",
      "18/18 - 1s - loss: 1.0385 - val_loss: 0.9617\n",
      "\n",
      "Epoch 01254: val_loss did not improve from 0.92396\n",
      "Epoch 1255/3000\n",
      "18/18 - 1s - loss: 0.9530 - val_loss: 0.9937\n",
      "\n",
      "Epoch 01255: val_loss did not improve from 0.92396\n",
      "Epoch 1256/3000\n",
      "18/18 - 1s - loss: 1.0159 - val_loss: 0.9402\n",
      "\n",
      "Epoch 01256: val_loss did not improve from 0.92396\n",
      "Epoch 1257/3000\n",
      "18/18 - 1s - loss: 1.1867 - val_loss: 0.9366\n",
      "\n",
      "Epoch 01257: val_loss did not improve from 0.92396\n",
      "Epoch 1258/3000\n",
      "18/18 - 1s - loss: 0.9345 - val_loss: 1.1957\n",
      "\n",
      "Epoch 01258: val_loss did not improve from 0.92396\n",
      "Epoch 1259/3000\n",
      "18/18 - 1s - loss: 1.0228 - val_loss: 0.9526\n",
      "\n",
      "Epoch 01259: val_loss did not improve from 0.92396\n",
      "Epoch 1260/3000\n",
      "18/18 - 1s - loss: 1.0203 - val_loss: 1.1063\n",
      "\n",
      "Epoch 01260: val_loss did not improve from 0.92396\n",
      "Epoch 1261/3000\n",
      "18/18 - 1s - loss: 0.9750 - val_loss: 0.9602\n",
      "\n",
      "Epoch 01261: val_loss did not improve from 0.92396\n",
      "Epoch 1262/3000\n",
      "18/18 - 1s - loss: 1.0364 - val_loss: 1.0589\n",
      "\n",
      "Epoch 01262: val_loss did not improve from 0.92396\n",
      "Epoch 1263/3000\n",
      "18/18 - 1s - loss: 0.9463 - val_loss: 1.1241\n",
      "\n",
      "Epoch 01263: val_loss did not improve from 0.92396\n",
      "Epoch 1264/3000\n",
      "18/18 - 1s - loss: 1.2077 - val_loss: 1.8454\n",
      "\n",
      "Epoch 01264: val_loss did not improve from 0.92396\n",
      "Epoch 1265/3000\n",
      "18/18 - 1s - loss: 1.2938 - val_loss: 2.4338\n",
      "\n",
      "Epoch 01265: val_loss did not improve from 0.92396\n",
      "Epoch 1266/3000\n",
      "18/18 - 1s - loss: 1.0632 - val_loss: 1.2666\n",
      "\n",
      "Epoch 01266: val_loss did not improve from 0.92396\n",
      "Epoch 1267/3000\n",
      "18/18 - 1s - loss: 0.9907 - val_loss: 1.2805\n",
      "\n",
      "Epoch 01267: val_loss did not improve from 0.92396\n",
      "Epoch 1268/3000\n",
      "18/18 - 1s - loss: 1.1529 - val_loss: 1.1178\n",
      "\n",
      "Epoch 01268: val_loss did not improve from 0.92396\n",
      "Epoch 1269/3000\n",
      "18/18 - 1s - loss: 1.0282 - val_loss: 1.2529\n",
      "\n",
      "Epoch 01269: val_loss did not improve from 0.92396\n",
      "Epoch 1270/3000\n",
      "18/18 - 1s - loss: 1.0171 - val_loss: 1.5235\n",
      "\n",
      "Epoch 01270: val_loss did not improve from 0.92396\n",
      "Epoch 1271/3000\n",
      "18/18 - 1s - loss: 1.1215 - val_loss: 1.2522\n",
      "\n",
      "Epoch 01271: val_loss did not improve from 0.92396\n",
      "Epoch 1272/3000\n",
      "18/18 - 1s - loss: 0.9592 - val_loss: 1.3695\n",
      "\n",
      "Epoch 01272: val_loss did not improve from 0.92396\n",
      "Epoch 1273/3000\n",
      "18/18 - 1s - loss: 1.0664 - val_loss: 0.9837\n",
      "\n",
      "Epoch 01273: val_loss did not improve from 0.92396\n",
      "Epoch 1274/3000\n",
      "18/18 - 1s - loss: 0.9369 - val_loss: 0.9441\n",
      "\n",
      "Epoch 01274: val_loss did not improve from 0.92396\n",
      "Epoch 1275/3000\n",
      "18/18 - 1s - loss: 1.0040 - val_loss: 1.3911\n",
      "\n",
      "Epoch 01275: val_loss did not improve from 0.92396\n",
      "Epoch 1276/3000\n",
      "18/18 - 1s - loss: 0.9624 - val_loss: 1.0017\n",
      "\n",
      "Epoch 01276: val_loss did not improve from 0.92396\n",
      "Epoch 1277/3000\n",
      "18/18 - 1s - loss: 1.0443 - val_loss: 1.2824\n",
      "\n",
      "Epoch 01277: val_loss did not improve from 0.92396\n",
      "Epoch 1278/3000\n",
      "18/18 - 1s - loss: 1.1871 - val_loss: 0.9666\n",
      "\n",
      "Epoch 01278: val_loss did not improve from 0.92396\n",
      "Epoch 1279/3000\n",
      "18/18 - 1s - loss: 0.9502 - val_loss: 0.9884\n",
      "\n",
      "Epoch 01279: val_loss did not improve from 0.92396\n",
      "Epoch 1280/3000\n",
      "18/18 - 1s - loss: 1.0417 - val_loss: 0.9712\n",
      "\n",
      "Epoch 01280: val_loss did not improve from 0.92396\n",
      "Epoch 1281/3000\n",
      "18/18 - 1s - loss: 0.9853 - val_loss: 1.4176\n",
      "\n",
      "Epoch 01281: val_loss did not improve from 0.92396\n",
      "Epoch 1282/3000\n",
      "18/18 - 1s - loss: 0.9501 - val_loss: 0.9413\n",
      "\n",
      "Epoch 01282: val_loss did not improve from 0.92396\n",
      "Epoch 1283/3000\n",
      "18/18 - 1s - loss: 1.0551 - val_loss: 0.9579\n",
      "\n",
      "Epoch 01283: val_loss did not improve from 0.92396\n",
      "Epoch 1284/3000\n",
      "18/18 - 1s - loss: 1.1070 - val_loss: 1.0835\n",
      "\n",
      "Epoch 01284: val_loss did not improve from 0.92396\n",
      "Epoch 1285/3000\n",
      "18/18 - 1s - loss: 1.0503 - val_loss: 1.1990\n",
      "\n",
      "Epoch 01285: val_loss did not improve from 0.92396\n",
      "Epoch 1286/3000\n",
      "18/18 - 1s - loss: 1.0365 - val_loss: 1.1976\n",
      "\n",
      "Epoch 01286: val_loss did not improve from 0.92396\n",
      "Epoch 1287/3000\n",
      "18/18 - 1s - loss: 0.9434 - val_loss: 1.2295\n",
      "\n",
      "Epoch 01287: val_loss did not improve from 0.92396\n",
      "Epoch 1288/3000\n",
      "18/18 - 1s - loss: 1.0919 - val_loss: 0.9929\n",
      "\n",
      "Epoch 01288: val_loss did not improve from 0.92396\n",
      "Epoch 1289/3000\n",
      "18/18 - 1s - loss: 1.0433 - val_loss: 0.9334\n",
      "\n",
      "Epoch 01289: val_loss did not improve from 0.92396\n",
      "Epoch 1290/3000\n",
      "18/18 - 1s - loss: 1.0358 - val_loss: 1.3906\n",
      "\n",
      "Epoch 01290: val_loss did not improve from 0.92396\n",
      "Epoch 1291/3000\n",
      "18/18 - 1s - loss: 1.0264 - val_loss: 1.0244\n",
      "\n",
      "Epoch 01291: val_loss did not improve from 0.92396\n",
      "Epoch 1292/3000\n",
      "18/18 - 1s - loss: 1.0295 - val_loss: 0.9815\n",
      "\n",
      "Epoch 01292: val_loss did not improve from 0.92396\n",
      "Epoch 1293/3000\n",
      "18/18 - 1s - loss: 1.0718 - val_loss: 0.9693\n",
      "\n",
      "Epoch 01293: val_loss did not improve from 0.92396\n",
      "Epoch 1294/3000\n",
      "18/18 - 1s - loss: 1.2288 - val_loss: 1.1038\n",
      "\n",
      "Epoch 01294: val_loss did not improve from 0.92396\n",
      "Epoch 1295/3000\n",
      "18/18 - 1s - loss: 1.1615 - val_loss: 1.0282\n",
      "\n",
      "Epoch 01295: val_loss did not improve from 0.92396\n",
      "Epoch 1296/3000\n",
      "18/18 - 1s - loss: 1.0359 - val_loss: 1.0040\n",
      "\n",
      "Epoch 01296: val_loss did not improve from 0.92396\n",
      "Epoch 1297/3000\n",
      "18/18 - 1s - loss: 0.9618 - val_loss: 1.4581\n",
      "\n",
      "Epoch 01297: val_loss did not improve from 0.92396\n",
      "Epoch 1298/3000\n",
      "18/18 - 1s - loss: 1.1203 - val_loss: 1.1640\n",
      "\n",
      "Epoch 01298: val_loss did not improve from 0.92396\n",
      "Epoch 1299/3000\n",
      "18/18 - 1s - loss: 0.9630 - val_loss: 0.9541\n",
      "\n",
      "Epoch 01299: val_loss did not improve from 0.92396\n",
      "Epoch 1300/3000\n",
      "18/18 - 1s - loss: 1.0584 - val_loss: 0.9545\n",
      "\n",
      "Epoch 01300: val_loss did not improve from 0.92396\n",
      "Epoch 1301/3000\n",
      "18/18 - 1s - loss: 0.9503 - val_loss: 2.4652\n",
      "\n",
      "Epoch 01301: val_loss did not improve from 0.92396\n",
      "Epoch 1302/3000\n",
      "18/18 - 1s - loss: 1.1112 - val_loss: 1.2457\n",
      "\n",
      "Epoch 01302: val_loss did not improve from 0.92396\n",
      "Epoch 1303/3000\n",
      "18/18 - 1s - loss: 1.2574 - val_loss: 1.8659\n",
      "\n",
      "Epoch 01303: val_loss did not improve from 0.92396\n",
      "Epoch 1304/3000\n",
      "18/18 - 1s - loss: 1.0185 - val_loss: 1.0919\n",
      "\n",
      "Epoch 01304: val_loss did not improve from 0.92396\n",
      "Epoch 1305/3000\n",
      "18/18 - 1s - loss: 1.1113 - val_loss: 1.3532\n",
      "\n",
      "Epoch 01305: val_loss did not improve from 0.92396\n",
      "Epoch 1306/3000\n",
      "18/18 - 1s - loss: 0.9912 - val_loss: 0.9783\n",
      "\n",
      "Epoch 01306: val_loss did not improve from 0.92396\n",
      "Epoch 1307/3000\n",
      "18/18 - 1s - loss: 1.4475 - val_loss: 2.6880\n",
      "\n",
      "Epoch 01307: val_loss did not improve from 0.92396\n",
      "Epoch 1308/3000\n",
      "18/18 - 1s - loss: 1.5293 - val_loss: 2.5141\n",
      "\n",
      "Epoch 01308: val_loss did not improve from 0.92396\n",
      "Epoch 1309/3000\n",
      "18/18 - 1s - loss: 1.6426 - val_loss: 1.8648\n",
      "\n",
      "Epoch 01309: val_loss did not improve from 0.92396\n",
      "Epoch 1310/3000\n",
      "18/18 - 1s - loss: 1.5961 - val_loss: 2.3344\n",
      "\n",
      "Epoch 01310: val_loss did not improve from 0.92396\n",
      "Epoch 1311/3000\n",
      "18/18 - 1s - loss: 1.3707 - val_loss: 1.0638\n",
      "\n",
      "Epoch 01311: val_loss did not improve from 0.92396\n",
      "Epoch 1312/3000\n",
      "18/18 - 1s - loss: 1.1397 - val_loss: 1.0837\n",
      "\n",
      "Epoch 01312: val_loss did not improve from 0.92396\n",
      "Epoch 1313/3000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-44933215316d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = q_model.fit( X_train, Y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                         callbacks = [\n\u001b[1;32m      3\u001b[0m                                 \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                 ModelCheckpoint(f'qkeras_weights.h5', monitor='val_loss', verbose=True, save_best_only=True) ],\n\u001b[1;32m      5\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3039\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1963\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/rapids/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = q_model.fit( X_train, Y_train,\n",
    "                        callbacks = [\n",
    "                                EarlyStopping(monitor='val_loss', patience=500, verbose=1),\n",
    "                                ModelCheckpoint(f'qkeras_weights.h5', monitor='val_loss', verbose=True, save_best_only=True) ],\n",
    "                        epochs=3000,\n",
    "                        validation_split = 0.25,\n",
    "                        batch_size=2**14,\n",
    "                        #batch_size=1000000,\n",
    "                        verbose=2\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1578ba8-160f-499e-91f3-bcb6d50e4192",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_preds = q_model.predict(X, batch_size=1024)\n",
    "\n",
    "q_yhat = sigmoid(q_preds[:,0])\n",
    "\n",
    "q_x_reg = q_preds[:,1]*mult_fact\n",
    "q_a_reg = q_preds[:,2]*mult_facta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4a7f7454-1fee-4d6d-9a40-237c3372fa44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure()\n",
    "plt.hist( (q_yhat - keras_yhat)/keras_yhat, histtype='step', range=(-5000,5000), bins=50 )\n",
    "plt.hist( (q_x_reg - keras_x_reg)/keras_x_reg, histtype='step', range=(-5000,5000), bins=50 )\n",
    "plt.hist( (q_a_reg - keras_a_reg)/keras_a_reg, histtype='step', range=(-5000,5000), bins=50 )\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2688d5b9-4d24-47f6-b553-b5e730282c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2fb5661-de66-44b9-8702-9e821c2b1f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_cut=(Y_mu==1)\n",
    "keras_mod_msex = metrics.mean_squared_error(  data['ev_mu_x'][this_cut], keras_x_reg[this_cut] )\n",
    "keras_mod_msea = metrics.mean_squared_error( data['ev_mu_theta'][this_cut], keras_a_reg[this_cut] )\n",
    "keras_mod_auc = metrics.roc_auc_score( Y_mu, keras_yhat )\n",
    "keras_fpr, keras_tpr, keras_thresholds = metrics.roc_curve( Y_mu, keras_yhat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c23468e5-af73-44c1-ab56-15eb4f9654ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_cut=(Y_mu==1)\n",
    "q_mod_msex = metrics.mean_squared_error(  data['ev_mu_x'][this_cut], q_x_reg[this_cut] )\n",
    "q_mod_msea = metrics.mean_squared_error( data['ev_mu_theta'][this_cut], q_a_reg[this_cut] )\n",
    "q_mod_auc = metrics.roc_auc_score( Y_mu, q_yhat )\n",
    "q_fpr, q_tpr, q_thresholds = metrics.roc_curve( Y_mu, q_yhat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4a8453d8-b78f-47bd-856e-4c2f4567eb86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.70336808086051"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_mod_msex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee9336fa-edd9-48f9-8ece-a94bbbcee269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3026055154951957"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_mod_msex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "504cdde6-8fd5-4897-899d-a87d1b02ac75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999306730502137"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_mod_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf72b2da-a13f-43b9-b6dd-6662572788f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998063632144765"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_mod_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c55e72bf-5d2f-472a-a6bb-594644d36d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5535306131510264e-05"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_mod_msea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d78e4076-5fac-4a6d-9a82-bb7410b2843e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0014960198339481067"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_mod_msea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ccbf4ce-5f8e-4eb2-91fe-2eb0420dd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-0d1a34148515>:2: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.semilogy(keras_tpr, 1./keras_fpr)\n",
      "<ipython-input-89-0d1a34148515>:3: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  plt.semilogy(q_tpr, 1./q_fpr)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.Figure()\n",
    "plt.semilogy(keras_tpr, 1./keras_fpr)\n",
    "plt.semilogy(q_tpr, 1./q_fpr)\n",
    "plt.xlim(0.9,1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9528b626-7893-4f95-bec3-b9f17742b996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
