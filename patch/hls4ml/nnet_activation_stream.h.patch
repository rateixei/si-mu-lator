54a55,88
> // ************************************************* 
> //       RELU Activation with reshape capability    
> // *************************************************
> //   This implementation assumes that the size of the output is larger 
> // or equal to the size of the in put, which is generally true for a Relu.
> //   This relu can deal with reshape from one layer to another.
> 
> template<class data_T, class res_T, typename CONFIG_T>
> void relu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
>     #pragma HLS function_instantiate variable=res
>     res_T data_m;
>     ReLUActLoop: for (int i = 0; i < CONFIG_T::n_in / data_T::size; i++) {
>     	if (CONFIG_T::n_in / data_T::size > 1){
> 	    #pragma HLS PIPELINE
>     	}	
>     	data_T in_data = data.read();
>     	res_T out_data;
> 
>   	ReLuReadLoop: for (int k = 0; k < data_T::size; k++){
>             data_m[((i * data_T::size) + k) % res_T::size] = in_data[k];
>             if ((((i * data_T::size) + k + 1) % res_T::size) == 0 ){
>         	#pragma HLS DATA_PACK variable=out_data
>             	ReLUPackLoop: for (int j = 0; j <  res_T::size; j++) {
> 		    #pragma HLS UNROLL
>             	    if (data_m[j] > 0) out_data[j] = data_m[j];
>             	    else out_data[j] = 0;
>                 }
>             res.write(out_data);
>             }
>         }
>     }
> }
> 
> 
59c93
< void relu(hls::stream<data_T> &data, hls::stream<res_T> &res) {
---
> void relu_backup(hls::stream<data_T> &data, hls::stream<res_T> &res) {
643c677
< #endif
\ No newline at end of file
---
> #endif
